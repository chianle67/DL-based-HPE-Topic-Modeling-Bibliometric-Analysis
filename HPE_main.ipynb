{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chianle67/DL-based-HPE-Topic-Modeling-Bibliometric-Analysis/blob/main/HPE_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Topic Modeling using \"Llama-2-13b-chat-hf\" to label clusters**"
      ],
      "metadata": {
        "id": "PAGkdKo8-wfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install libraries and packages"
      ],
      "metadata": {
        "id": "IIVwRi0TIMci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes sentence_transformers bertopic torch"
      ],
      "metadata": {
        "id": "TMhEkLWhCido"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U git+https://github.com/huggingface/accelerate.git"
      ],
      "metadata": {
        "id": "Fh2Q1CCaCm0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "lsHJeRkHs1ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load libraries and packages for using LLM and Topic Modeling"
      ],
      "metadata": {
        "id": "-MnyVeZKIV6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from torch import cuda, bfloat16\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers"
      ],
      "metadata": {
        "id": "GREuMAq1Cp1A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing"
      ],
      "metadata": {
        "id": "pGoM_4ZgIvJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "papers = pd.read_csv('/content/main_dataset_678_abstracts.csv')\n",
        "db = papers['Abstract']\n",
        "title_data = papers['Title']\n",
        "db = pd.DataFrame(db)\n",
        "db = db.dropna()\n",
        "print(db.head())\n",
        "print(db.shape)\n",
        "\n",
        "db['Abstract'] = db['Abstract'].map(lambda x: x.lower())\n",
        "db_to_list = db['Abstract'].values.tolist()\n",
        "\n",
        "truncated_db_to_list = []\n",
        "for docs in db_to_list:\n",
        "    truncated_docs = docs.split(\" ?\")[0]\n",
        "    truncated_db_to_list.append(truncated_docs)\n",
        "\n",
        "remove_punc_db_to_list = []\n",
        "for docs in truncated_db_to_list:\n",
        "    cleaned_string = ''.join(character for character in docs if character not in string.punctuation)\n",
        "    remove_punc_db_to_list.append(cleaned_string)\n",
        "\n",
        "non_empty_punc_db_to_list = []\n",
        "for docs in remove_punc_db_to_list:\n",
        "    if docs:\n",
        "        non_empty_punc_db_to_list.append(docs)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "input = non_empty_punc_db_to_list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "stop_words.update(['propose', 'proposes', 'proposed', 'base', 'bases', 'based', 'include', 'contain', 'proceed', 'proceeding', 'proceedings', 'use', 'uses', 'using', 'datum', 'ieee', 'papers', 'stateoftheart', 'use', 'propose', 'approach', 'framework', 'frameworks', 'method', 'methods'])\n",
        "print(stop_words)\n",
        "\n",
        "output = []\n",
        "for docs in input:\n",
        "    word_tokenized = word_tokenize(docs)\n",
        "    filtered_docs = []\n",
        "    for w  in word_tokenized:\n",
        "        if w not in stop_words:\n",
        "            filtered_docs.append(w)\n",
        "            reconstructed_docs = ' '.join(filtered_docs)\n",
        "    output.append(reconstructed_docs)\n",
        "\n",
        "docs = output\n",
        "print(docs)\n",
        "\n",
        "# Using lemmatization if needed\n",
        "'''\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "import spacy\n",
        "\n",
        "from pprint import pprint\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "def tokenizer(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "docs_tokenized = list(tokenizer(docs))\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "def lemmatizer(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "docs_lemmatized = lemmatizer(docs_tokenized,\n",
        "                             allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "jvuhllEXAe3a",
        "outputId": "31671427-c194-4ab2-e54b-745af2955b77"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Abstract\n",
            "0  Traditional human pose estimation methods typi...\n",
            "1  Seniors who live alone at home are at risk of ...\n",
            "2  Human Pose Estimation (HPE) to assess human mo...\n",
            "3  Video-based 3D human pose estimation is an imp...\n",
            "4  Human pose estimation is an important Computer...\n",
            "(678, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'now', \"hasn't\", \"isn't\", 'my', 'o', 'aren', 'but', 'of', 'he', 'she', 'his', 'itself', 'before', 'up', 'further', \"aren't\", 'isn', 's', \"you're\", 'or', \"wouldn't\", 'your', 'that', 'with', \"needn't\", 'hers', 'shan', 'me', 'an', 'yourself', \"that'll\", 'both', 'all', 'this', 'during', 'them', 'it', 'same', 'so', 'if', 'd', \"don't\", 'herself', 'they', 'be', 'its', 'theirs', 'again', \"hadn't\", 'mightn', 'too', 'can', 'because', 'himself', \"it's\", 'most', 'will', 'a', 'yours', 'has', 'only', \"weren't\", \"you'd\", \"should've\", 'once', 'into', 've', \"wasn't\", 'shouldn', 'other', 'ma', 'needn', 'to', 'off', \"you'll\", 'and', 'down', \"didn't\", 'being', 'such', 'whom', 'some', 'those', 'not', 'very', 'don', 'there', 'against', 'weren', 'wasn', 'do', 'wouldn', 'no', \"mustn't\", 'll', 'haven', 'hadn', 'between', 'her', 'doing', 'here', 'in', 'where', 'why', 'own', 'by', 'what', \"she's\", 'been', 'over', \"shan't\", \"won't\", 'then', 'ourselves', 'couldn', 'at', 'myself', \"haven't\", 'their', 'out', 'does', 't', 'themselves', 'we', 'is', 'him', 'which', 'just', 'our', 'should', 'for', 're', 'having', 'ain', 'who', 'had', 'these', 'mustn', 'below', 'about', 'won', \"you've\", 'through', \"shouldn't\", 'any', \"doesn't\", 'few', 'how', 'i', 'are', 'am', 'until', 'after', 'm', 'were', 'from', 'did', 'have', \"mightn't\", 'above', 'the', 'nor', 'each', 'was', 'under', \"couldn't\", 'yourselves', 'y', 'on', 'didn', 'than', 'more', 'you', 'hasn', 'as', 'ours', 'doesn', 'when', 'while'}\n",
            "{'now', 'framework', \"hasn't\", \"isn't\", 'my', 'o', 'aren', 'proposes', 'but', 'of', 'he', 'she', 'his', 'itself', 'before', 'up', 'papers', 'further', \"aren't\", 'ieee', 'isn', 's', \"you're\", 'or', 'method', \"wouldn't\", 'your', 'that', 'with', \"needn't\", 'hers', 'shan', 'proceed', 'me', 'an', 'yourself', \"that'll\", 'both', 'all', 'methods', 'this', 'contain', 'during', 'using', 'them', 'it', 'same', 'so', 'if', 'use', 'd', \"don't\", 'herself', 'they', 'be', 'its', 'theirs', 'again', \"hadn't\", 'mightn', 'too', 'can', 'because', 'himself', \"it's\", 'most', 'will', 'a', 'base', 'yours', 'has', 'only', \"weren't\", \"you'd\", \"should've\", 'once', 'into', 'datum', 've', \"wasn't\", 'shouldn', 'other', 'ma', 'needn', 'to', 'off', \"you'll\", 'and', 'down', \"didn't\", 'include', 'being', 'such', 'bases', 'whom', 'some', 'those', 'not', 'very', 'don', 'there', 'against', 'weren', 'wasn', 'do', 'proceeding', 'wouldn', 'no', \"mustn't\", 'll', 'haven', 'hadn', 'between', 'her', 'doing', 'here', 'in', 'where', 'why', 'own', 'by', 'what', \"she's\", 'been', 'over', \"shan't\", \"won't\", 'then', 'ourselves', 'couldn', 'at', 'myself', \"haven't\", 'proceedings', 'their', 'out', 'does', 't', 'themselves', 'we', 'is', 'him', 'which', 'just', 'our', 'should', 'for', 're', 'having', 'ain', 'who', 'had', 'these', 'mustn', 'below', 'about', 'won', \"you've\", 'through', \"shouldn't\", 'any', \"doesn't\", 'few', 'how', 'i', 'are', 'am', 'until', 'after', 'stateoftheart', 'm', 'were', 'from', 'did', 'have', \"mightn't\", 'above', 'approach', 'the', 'nor', 'each', 'frameworks', 'was', 'under', \"couldn't\", 'yourselves', 'based', 'y', 'on', 'didn', 'propose', 'than', 'more', 'you', 'proposed', 'hasn', 'uses', 'as', 'ours', 'doesn', 'when', 'while'}\n",
            "['traditional human pose estimation typically rely complex models algorithms litehrnet achieve excellent performance reducing model complexity however feature extraction scale relatively single lead lower keypoints ’ localization accuracy crowded complex scenes address issue lightweight human pose estimation model joint channel coordinate attention mechanism model provides powerful information interaction channel enabling features different resolutions interact effectively interaction solve problem human pose estimation complex scenes improve robustness accuracy pose estimation model introduction joint channel coordinate attention mechanism enables model effectively retain key information thereby enhancing keypoints ’ localization accuracy also redesign lightweight basic module shuffle module joint channel coordinate attention mechanism replace spatial weight calculation module original litehrnet model introducing new module improve network calculation speed reduce number parameters entire model also ensure accuracy model thereby achieving balance performance efficiency compare model current mainstream coco mpii dataset experimental results show model effectively reduce number parameters computational complexity ensuring high model accuracy © 2023 authors', 'seniors live alone home risk falling injuring thus may need mobile robot monitors recognizes poses automatically even though deep learning actively evolving area limitations estimating poses absent rare training datasets lightweight offtheshelf 2d pose estimation sophisticated humanoid model fast optimization combined estimate joint angles 3d pose estimation novel idea depth ambiguity problem 3d pose estimation solved adding loss function deviation center mass center supporting feet penalty functions concerning appropriate joint angle rotation range verify pose estimation six daily poses estimated mean joint coordinate difference 0097 average angle difference per joint 10017 degrees addition confirm practicality videos exercise activities scene person falling filmed joint angle trajectories produced 3d estimation results optimized execution time per frame measured 0033 singleboard computer sbc without gpu showing feasibility realtime system © 2023 authors', 'human pose estimation hpe assess human motion sports rehabilitation work safety requires accurate sensing without compromising sensitive underlying personal data therefore local processing necessary limited energy budget systems addressed inertial measurement units imu instead common camera sensing central tradeoff accuracy efficient hardware resources rarely discussed research address tradeoff simulative design space exploration dse varying quantity positioning imu sensors first generate imudata publicly available body model dataset different sensor configurations train deep learning model data additionally combined metric assess accuracyresource tradeoff used dse tool evaluate sensor configurations identify beneficial ones specific case exemplary system equal importance accuracy resources identify optimal sensor configuration 4 sensors mesh error 603 cm increasing accuracy 327 reducing hardware effort two sensors compared state art work used design health applications wellsuited sensor positioning attention data privacy resourceawareness © 2023', 'videobased 3d human pose estimation important yet challenging task many humaninvolved pattern recognition systems existing deep learningbased 3d human pose estimation faced problems lacking largescale training data lacking effective solutions represent complicated human body structure end paper jointly learning entitled joypose simultaneously leverages human pose data augmentation human pose estimation particular joypose consists evolutionary data augmentation module anatomyaware global–local pose feature representation module 3d human pose estimation evolution data augmentation guided reinforcement learning strategy probabilistic way according pose estimation loss anatomyaware global–local pose feature representation module separately captures global features local features according anatomical kinematic patterns observed pose estimation errors across different human joints performance final human pose estimation leveraged data augmentation anatomyaware global–local feature representation extensive experiments three realworld datasets demonstrate superiority robustness © 2023 elsevier ltd', 'human pose estimation important computer vision problem whose goal estimate human body joints currently employ deep learning techniques excel task 2d human pose estimation however 3d poses bring accurate robust results since 3d pose labels acquired restricted scenarios fully convolutional tend perform poorly task one strategy solve problem 2d pose estimators estimate 3d poses two steps 2d pose inputs due database acquisition constraints performance improvement strategy observed controlled environments therefore domain adaptation techniques used increase generalization capability system inserting information synthetic domains work novel called domain unified aimed solving pose misalignment problems crossdataset scenario combination three modules top pose estimator pose converter uncertainty estimator domain classifier led 441mm 2924 error reduction training surreal synthetic dataset evaluating human36m noadaption scenario achieving performance © 2023 authors', 'modern deep learningbased 3d pose estimation approaches require plenty 3d pose annotations however existing 3d datasets lack diversity limits performance current generalization ability although existing utilize 2d pose annotations help 3d pose estimation mainly focus extracting 2d structural constraints 2d poses ignoring 3d information hidden images paper novel extract weak 3d information directly 2d images without 3d pose supervision firstly utilize 2d pose annotations perspective prior knowledge generate relative depth human joints collect 2d pose dataset mcpc generate relative depth labels mcpc weaklysupervised pretraining wsp strategy distinguish depth relationship two points image wsp enables learning relative depth two keypoints lots inthewild images capable predicting depth generalization ability 3d human pose estimation finetuning pose model 3d pose datasets wsp achieves results two widelyused benchmarks © 2023 elsevier ltd', 'paper presents virteach exploitation virtual point cloud vpc assisted teacher learning process human pose estimation incorporated millimeter wave mmwave radar point cloud rpc due observations involvement different body parts varies moving ranges directions performing postures mmwave signals possess inherent characteristics ie specular reflection radical sensitiveness perception rpc data suffer issues blind spots data imbalance response points induced specific joints leading insufficient biased learning thus large estimation errors address issues introduce vpc data driven real human motions assist learning process indispensable explicitly imposing taskspecific constraints distorted rpc data fashion learning teaching specifically first design generation module produce desired vpc data considering global structure local motions human skeleton serving x201cteacherx201d augment corrupted rpc data secondly incorporate global local guidance vpc data within coarsetofine pose estimation former addresses blind spots issue completing rpc data facilitate global skeleton reconstruction latter targeted strengthening contribution specific joints constructing local spatialtemporal neighborhood refine positions extensive experiments conducted validate effectiveness', 'teachers ’ classroom behavior testing help understand learning situation students feedback teaching effect formulate corresponding measures help traditional evaluation rely manual testing timeconsuming lacks objectivity article suggests new analyze behavior teachers human skeleton posture first popular human posture gesture estimation technology obtain skeleton joint node teacher teaching process remove several joint nodes weaker effect classification teachers class finally convolutional neural network designed classify teacher ’ behavior skeleton joint nodes confidence obtained processing input experimental results show effectiveness helps dynamic management evaluation classroom teachers ’ behavior © authors exclusive license springer nature singapore pte ltd 2024', 'prevalence wearable devices inertial measurement unit imu data utilized monitoring assessing human mobility human activity recognition har human pose estimation hpe training deep neural network dnn models tasks require large amount labelled data hard acquire uncontrolled environments mitigate data scarcity problem design cromosim crossmodality sensor simulator simulates high fidelity virtual imu sensor data motion capture systems monocular rgb cameras utilizes skinned multiperson linear model smpl 3d body pose shape representations enable simulation arbitrary onbody positions dnn model trained learn functional mapping imperfect trajectory estimations 3d smpl body trimesh due measurement noise calibration errors occlusion modelling artifacts imu data evaluate fidelity cromosim simulated data utility data augmentation various har hpe datasets extensive empirical results show model achieves 67 improvement baseline har task © 2024 institute electrical electronics engineers inc rights reserved', 'human pose estimation popular research topic field computer vision development deep learning human pose estimation models accurately predict human key points aiming problems occlusion key points overlapping key points complex background paper cascade pyramid model combined attention mechanism integrating attention mechanism feature extraction network model obtain richer feature information help globalnet refinenet accurately locate occluded key points shown via results public dataset mpii ms coco2017 3doh50k model attain higher accuracy human pose estimation standard occluded situations previous models better robustness © 2023 east china university science technology rights reserved', 'camera inertial measurement unit imu threedimensional 3d human pose estimation flourished last decade however existing difficult apply clinical environments many required devices study attempt reduce devices maintain robustness occcorrector semantic convolutionbased neural network estimate 3d human poses robustly occlusion cases involving single camera help imus includes simple sensorreshape helps fuse imu information camera effectively new strategy alternating loss function allowing model improve accuracy predicting challenging poses conducting inverse analysis weight matrix importance imu investigated device simplification purposes verified total capture dataset two hypothetical occlusion conditions result showed five imus make accuracy stable different occlusion situations © 2023 authors', 'threedimensional human pose shape estimation compute full human 3d mesh given single image contamination features caused occlusion usually degrades performance significantly recent progress field typically addressed occlusion problem implicitly contrast paper address explicitly simple yet effective deocclusion multitask learning network key insight feature mesh parameter regression noiseless thus feature space disentangles occludee represents noiseless human feature occluder specifically spatial regularization attention mechanism imposed backbone network disentangle features different channels furthermore two segmentation tasks supervise deocclusion process final mesh model regressed disentangled occlusionaware features experiments occlusion nonocclusion datasets conducted results prove superior two occlusion datasets achieving competitive performance nonocclusion dataset also demonstrate deocclusion strategy main factor improve robustness occlusion code available httpsgithubcomqihangrandeocclusionmtlhmr © 2023', 'svmbased realtime pose detection correction system refer computer system machine learning techniques detect correct persons yoga pose realtime system act virtual yoga assistant helping people improve yoga practice providing immediate feedback form helping prevent injury paper presents yoga tracker correction system computer vision machine learning algorithms track correct yoga poses system comprises camera computer vision module captures images yoga practitioner identifies poses performed machine learning module analyzes images provide feedback quality poses recommends corrections improve form prevent injuries paper customized support vector machine svm realtime pose detection correction system suggests yoga practices specific health conditions diseases paper aims provide reliable accessible resource individuals seeking yoga complementary managing health conditions system also includes practitioner ’ interface enables practitioners receive personalized recommendations yoga practice system developed python several opensource libraries tested dataset yoga poses hyper parameter gamma tuned optimize classification accuracy dataset produced 87 better approaches experiment results demonstrate effectiveness system tracking correcting yoga poses potential enhance quality yoga practice © 2023 authors rights reserved', 'recent years human pose estimation task typical computer vision task still remains room improvement overcoming complex scenarios eg occlusion data flaws eg motion blur etc recent researches often equip deep learning models human prior knowledge obtain enhanced accuracy study present novel model simultaneously maintains joint graph embeddings feature maps two separated branches communicates information aiming enable model leverage longrange semantic relationships local textile information effectively evaluate experiment posetrack18 dataset compare performance existing model achieves mean average precision map 802 outperformed competitive existing previous solely either feature maps joint embeddings generate prediction research conducted explore parallelly combining feature maps joint embeddings enhance result © 2023 copyright held ownerauthors', 'pointwise supervision widely adopted computer vision tasks crowd counting human pose estimation practice noise point annotations may affect performance robustness algorithm significantly paper investigate effect annotation noise pointwise supervision series robust loss functions different tasks particular point annotation noise includes spatialshift noise missingpoint noise duplicatepoint noise spatialshift noise common one exists crowd counting pose estimation visual tracking etc missingpoint duplicatepoint noises usually appear dense annotations crowd counting paper first consider shift noise modeling real locations random variables annotated points noisy observations probability density function intermediate representation smooth heat map generated dot annotations derived negative log likelihood used loss function naturally model shift uncertainty intermediate representation missing duplicate noise modeled empirical way assumption noise appears high density region high probability apply crowd counting human pose estimation visual tracking robust loss functions tasks achieve superior performance robustness widely used datasets © 19792012', 'paper introduces novel ie rfposeot enable threedimensional 3d human pose estimation radio frequency rf signals different existing predict human poses rf signals signal level directly consider structure difference rf signals human poses transformation rf signals pose domain feature level optimal transport ot theory generate human poses transformed features evaluate rfposeot build radio system multiview camera system acquire rf signal data groundtruth human poses experimental results basic indoor environment occlusion indoor environment outdoor environment demonstrate rfposeot predict 3d human poses higher precision © 2023 zhejiang university press', 'human pose estimation aims localizing human anatomical keypoints body parts input data eg images videos signals forms crucial component enabling machines insightful understanding behaviors humans become salient problem computer vision related fields deep learning techniques allow learning feature representations directly data significantly pushing performance boundary human pose estimation paper reap recent achievements 2d human pose estimation present comprehensive survey briefly existing approaches put efforts three directions namely network architecture design network training refinement post processing network architecture design looks architecture human pose estimation models extracting robust features keypoint recognition localization network training refinement tap training neural networks aims improve representational ability models post processing incorporates modelagnostic polishing strategies improve performance keypoint detection 200 research contributions involved survey covering methodological common benchmark datasets evaluation metrics performance comparisons seek provide researchers comprehensive systematic review human pose estimation allowing acquire grand panorama better identify future directions © 2022 authors exclusive licence springerverlag gmbh germany part springer nature', 'sensorbased human activity recognition facilitates unobtrusive monitoring human movements however determining effective sensor placement optimal classification performance remains challenging paper introduces novel methodology resolve issue realtime 2d pose estimations derived video recordings target activities derived skeleton data provides unique strategy identifying optimal sensor location validate feasibility study applying inertial sensors monitor 13 different activities across ten subjects findings indicate visionbased sensor placement offers comparable results conventional deep learning demonstrating efficacy research significantly advances field human activity recognition providing lightweight ondevice solution determining optimal sensor placement thereby enhancing data anonymization supporting multimodal classification © 2023 ownerauthor', 'monitoring fatigue resistance training essential avoid injuries caused overtraining fatigue comprehensively quantified external internal load external load work done athlete internal load psychological physiological response external load paper computer vision continuously monitor fatigue resistance training predicting external internal parameters namely generated power rating perceived exertion utilize human pose estimation two microsoft azure kinect cameras capture movement athletes performing stationary exercises processes obtained kinematic data computes skeleton features train traditional machine learning algorithms constructs feature maps train convolutional neural networkbased models predict load parameters evaluation recorded dataset 16 subjects performed squat exercises flywheel rated perceived exertion set measuring unit integrated flywheel provided power readings repetition results show achieves good results predicting parameters gradient boosting regression trees best predicted perceived exertion mean absolute percentage error 808 spearmans ρ074 multilayer perceptron performed best predicting power mean absolute error 2313 watts ρ079 findings show delivers promising external internal load quantifications fatigue great potential provide external feedback coaches athletes © 2023 authors', 'automatically determinant threedimensional human pose monocular rgb image information may difficult problem twodimensional nature input leads intrinsic ambiguities create inferring depth notably tough recently researchers incontestable versatile applied mathematics modelling capabilities deep neural networks area unit spare create inferences cheap accuracy however several models coordinate output techniques area unit memoryintensive differentiable andor dont spatially generalize well enhancements 3d coordinate prediction avoid undesirable traits predicting 2d marginal heat maps beneath associate nursing increased soft argmax scheme ensuing model margi pose produces visually coherent heat maps maintaining differentiability also ready deliver goods progressive accuracy publically offered 3d human create estimation information task human cause estimation properly find estimate body cause individuals within scene human posture provides powerful clues welltried effectively represent varied tasks like activity recognition motion capture content retrieval social interaction signal process © 2024 authors', 'human detection task locating instances human beings present image wide range applications across various fields including search rescue surveillance autonomous driving rapid advancement computer vision deep learning technologies brought significant improvements human detection however advanced applications like healthcare human–computer interaction scene understanding crucial obtain information beyond localization humans applications require deeper understanding human behavior state enable effective safe interactions humans environment study presents comprehensive benchmark common human postures chp dataset aimed promoting informative encouraging task beyond mere human detection benchmark dataset comprises diverse collection images featuring individuals different environments clothing occlusions performing wide range postures activities benchmark aims enhance research challenging task designing novel precise specifically chp dataset consists 5250 human images collected different scenes annotated bounding boxes seven common human poses wellannotated dataset developed two baseline detectors namely chpyolof chpyolox building upon two identitypreserved human posture detectors iphyolof iphyolox evaluate performance baseline detectors extensive experiments results demonstrate baseline detectors effectively detect human postures chp dataset releasing chp dataset aim facilitate research human pose estimation attract researchers focus challenging task © 2023 authors', 'necessary analyze wholebody kinematics including joint locations joint angles assess risks fatal musculoskeletal injuries occupational tasks human pose estimation gotten attention recent years minimize errors determining joint locations however joint angles often estimated quality joint angle estimation assessed paper presented endtoend direct joint angle estimation multiview images leveraged volumetric pose representation mapped rotation representation continuous space rotation uniquely represented also presented new kinematic dataset domain residential roofing data processing pipeline generate necessary annotations supervised training procedure direct joint angle estimation achieved mean angle error 719° new roofing dataset 841° human36m dataset paving way employment onsite kinematic analysis multiview images © 2023 elsevier inc', 'human posture detection plays important role areas human behavior analysis humancomputer interaction deep learning one mainstream approaches practice detection models often deployed resourceconstrained edge devices privacy protection increased realtime etc however pose detection dynamic scenes requires high detection speed accuracy thus models mostly complex structure decentralized computation aiming difficulties paper multiperson pose detection implements lightweight detection networkreppose deploys embedded device specifically paper branch pruning scheme reparameterized structure designs highperformance postprocessing operator postprocessing part addition also multithreaded parallel accelerated inference scheme finally deployed huawei atlas200dk result reaches 26fps multitarget detection experiment achieves 90 classification accuracy pedestrian state detection experiment simple traffic scenarios © 2023', 'present fullbapose novel bottomup full body pose estimation achieves results without relying external people detectors fullbapose addresses broader task full body pose estimation including hands feet facial landmarks deep learning architecture endtoend trainable encoderdecoder configuration hrnet backbone multiscale representations disentangled waterfall atrous spatial pooling module disentangled waterfall module leverages efficiency progressive filtering maintaining multiscale fieldsofview comparable spatial pyramid configurations additionally combines multiscale features obtained waterfall flow persondetection capability disentangled adaptive regression incorporates adaptive convolutions infer keypoints precisely crowded scenes fullbapose achieves stateofthe art performance challenging crowdpose cocowholebody datasets ap 722 684 respectively 133 keypoints results demonstrate fullbapose efficient robust operating variety conditions including multiple people changes scale occlusions © 2023 authors', 'metaverse digital avatar plays important role representing human beings various interaction virtual objects environments puts high demand effective pose estimation though camerabased solutions yield remarkable performance encounter privacy issues degraded performance caused varying illumination especially smart home article wifibased internet thingsenabled human pose estimation scheme metaverse avatar simulation namely metafi specifically wpformer designed shared convolutional module transformer block map channel state information wifi signals human pose landmarks effectively exploring spatial information human pose selfattention enforced learn annotations accurate computer vision model thus achieving crossmodal supervision due ubiquitous existence wifi robustness various illumination conditions wifibased human poses suitable instruct movement digital avatars metaverse promoting avatar applications smart homes experiments conducted real world results show metafi achieves high performance pck50 9730 codes available httpsgithubcompridy999metafiposeestimation © 2014', 'human segmentation tracking hst video often utilize person detection results addition 3d human pose estimation 3dhpe human activity recognition har often human segmentation results reduce data storage computational time recent advantages deep learning especially convolutional neural networks cnns excellent results relevant tasks consequently applied building many practical applications sports analysis sports scoring health protection teaching preserving traditional martial arts paper performed survey relevant studies datasets results hst 3dhpe har also deeply analyze results detecting persons affects results human segmentation human tracking survey performed great detail source code paths mads martial arts dancing sports dataset comprises fast complex activities published task estimating human pose however determining human pose person needs detected segment video especially 3d human pose annotation data different point cloud data generated rgbd images therefore also prepared 2d human pose annotation data 28k images creating 3d human pose annotation action labeling data moreover also evaluated mads dataset many recently published deep learning human segmentation mask rcnn pointrend tridentnet tensormask centermask tracking 3dhpe repnet mediapipe pose lifting deep v2vposenet har stgcn ddnet pagesgcn video data published results available © 2022 authors exclusive licence springer sciencebusiness media llc part springer nature', 'technologymediated dance experiences medium entertainment key element traditional virtual realitybased gaming platforms platforms predominantly depend unobtrusive continuous human pose estimation means capturing input current solutions primarily employ rgb rgbdepth cameras dance gaming applications however former hindered lowlight conditions due motion blur reduced sensitivity latter exhibits excessive power consumption diminished frame rates restricted operational distance boasting ultralow latency energy efficiency wide dynamic range neuromorphic cameras present viable solution surmount limitations introduce yelan neuromorphic cameradriven threedimensional highfrequency human pose estimation hpe system capable withstanding lowlight environments dynamic backgrounds compiled firstever neuromorphic camera dance hpe dataset devised fully adaptable motiontoevent physicsconscious simulator yelan surpasses baseline models strenuous conditions exhibits resilience varying clothing types background motion viewing angles occlusions lighting fluctuations © 2023 authors', 'lightweight efficient human pose estimation enhanced priori skeleton structure better utilize unique distribution properties human pose keypoints highresolution network used preserve spatial location information better lightweight inverse residual module employed reduce number model parameters postural enhancement module designed strengthen priori information human pose connection human pose keypoints global spatial feature information context information directionenhanced convolution module address problem missing spatial feature information keypoints caused blurred pixel positions directional shifts convolution kernel optimization fusing multiresolution feature images prior distribution keypoints combined utilizing properties horizontal vertical directions keypoints torso experimental results demonstrate network efficiently estimate human pose model achieves average precision score 784 coco testdev set reduces number parameters 174×106 compared benchmark network balancing accuracy efficiency © 2024 zhejiang university rights reserved', 'paper presents monocular 3d human pose estimation virtual character skeleton retargeting monocular visual equipment first 2d human pose achieved openpose continuous video frames collected monocular camera corresponding 3d human pose estimated fusing constructing depthchannel pose estimation network pose filter network next designed smooth optimize 3d human pose estimation sliding window strategy human pose skeleton retargeting optimizer support video motion capture applications virtual character skeleton retargeting animation bone direction vectors reprojection error joint points finally performance verified human36 dataset results show mean perjoint position error public dataset 405 mm lower multiple benchmark © 2023 authors exclusive licence springerverlag gmbh germany part springer nature', 'article introduce new human movement defining movement static super object represented single twodimensional image described applicable remote healthcare applications physiotherapeutic exercises allows researchers label describe entire exercise standalone object isolated reference video allows us perform various tasks including detecting similar movements video measuring comparing movements generating new similar movements defining choreography controlling specific parameters human body skeleton result presented eliminate need label images manually disregard problem finding start end exercise overcome synchronization issues movements perform deep learning networkbased operation processes super objects images general part article demonstrate two application cases one illustrates verify score fitness exercise contrast illustrates generate similar movements human skeleton space addressing challenge supplying sufficient training data deep learning applications dl variational auto encoder vae simulator efficientnetb7 classifier architecture embedded within siamese twin neural network presented paper order demonstrate two cases cases demonstrate versatility innovative concept measuring categorizing inferring human behavior generating gestures researchers © 2023 authors', 'advanced human sensing technologies radio frequency rf signals gained widespread attention recent years however due sparsity incompleteness rf signals finegrained rfbased multiperson 3d pose estimation progressed slowly paper present rfbased pose machine rpm 20 multiperson 3d pose estimation rf signals specifically first develop lightweight anchorfree detector module locate crop regions interest horizontal vertical rf signals afterward treat horizontal vertical millimeterwave radars rf cameras different viewing angles multiview fusion network unproject rf signals unified latent feature space calculate correlation weighted fusion finally spatiotemporal attention network designed reconstruct multiperson 3d skeleton sequences spatial attention module recover invisible body parts nonlocal correlations among joints temporal attention module refines 3d pose sequences temporal coherency learned frame queries evaluate performance rpm 20 largescale dataset multiperson 3d pose labels corresponding radar signals experimental results show rpm 20 outperforms baseline locates multiperson 3d key points average error 73 mm generalizes well new data occlusion low illumination © 19912012', 'radiofrequency rf human sensing technologies due great practical value various applications privacypreserving nature gained tremendous attention recent years however without fully exploiting characteristics radio signals performance existing still limited first rf features moving human body different representations dimensions channel scale challenging performing feature fusion besides human body specularly reflective respect radar means human body fully captured single rf snapshot therefore radar signal reflected human body sparse incomplete difficult extract highquality features 3d human pose estimation paper present rfbased pose machines rpm novel generate 3d skeletons rf signals considering characteristics rf signals rpm includes several modules overcome challenges firstly feature fusion network ffn designed effectively fuse radio signals horizontal vertical planes channels correlation maintain highquality feature via multiscale fusion block spatiotemporal attention network designed reconstruct 3d skeletons sparse incomplete rf signals specifically spatial attention module designed model nonlocal relationships among joints reconstruct body parts single rf snapshot capture afterwards temporal attention module refine 3d pose temporal coherency learned frame queries evaluate performance rpm construct largescale dataset synchronized 3 skeletons rf signals rfskeleton3d experimental results show rpm locates 3d key points human body average error 571cm maintains performance new environments occlusion bad illumination dataset codes made public © 19992012', 'single bicycle crashes ie falls impacts involving collision another road user significantly underestimated road safety problem motions behaviours falling people fall kinematics often investigated injury biomechanics research field understanding mechanics fall help researchers develop better protective gear safety measures reduce risk injury however little known cyclist fall kinematics dynamics therefore study video analysis cyclist falls performed investigate common kinematic forms impact patterns furthermore pipeline involving deep learningbased human pose estimation inverse kinematics optimisation created extracting human motion realworld footage falls initialise forward dynamics computational human body models bracing active response optimised genetic algorithm applied case study cyclist fall kinematic forms characterised study used inform initial conditions computational modelling injury estimation cyclist falls findings indicate protective response important consideration fall kinematics dynamics included computational modelling furthermore novel reconstruction pipeline applied broadly traumatic injury biomechanics tasks tool developed study available httpskevgildeagithubiokineposekevgildeagithubiokinepose © 2024 authors', 'chapter helps researchers explain roles artificial intelligence ai metaverse ai improve users immersive experience describe ai intensively contributes many technical aspects developing virtual worlds metaverse nowadays ai technology machine learning ml algorithms deep learning dl architectures seen many diversified aspects metaverse along computer vision technologies shine appearance virtual worlds chapter provides wide spectrum ai including conventional ml algorithms recently modern dl architectures embrace different learning mechanisms various learning tasks popular computer vision tasks associated ai image classification object detection semantic segmentation facial recognition human pose estimation scene reconstruction many aiassisted services applications decisions made ai agentswhich driven weakinterpretability poorexplainability ml models © 2024 institute electrical electronics engineers inc rights reserved', 'existing 3d human pose estimation often suffer inferior generalization performance new datasets largely due limited diversity 2d3d pose pairs training data address problem present poseaug novel autoaugmentation learns augment available training poses towards greater diversity thus enhances generalization power trained 2dto3d pose estimator specifically poseaug introduces novel pose augmentor learns adjust various geometry factors pose differentiable operations differentiable capacity augmentor jointly optimized 3d pose estimator take estimation error feedback generate diverse harder poses online manner poseaug generic handy applied various 3d pose estimation models also extendable aid pose estimation video frames demonstrate introduce poseaugv simple yet effective decomposes video pose augmentation end pose augmentation conditioned intermediate pose generation extensive experiments demonstrate poseaug extension poseaugv bring clear improvements framebased videobased 3d pose estimation several outofdomain 3d human pose benchmarks © 19792012', 'human wholebody pose estimation challenging task since model needs learn keypoints bodyonly case meet needs realtime performance maintaining accuracy also hard issue wholebody pose estimation due learning capability lightweight networks order solve problems large extent light wholebody pose estimation optimized training strategy model designed bottomup architecture network followed refinement network twostage training process learns rough features first stage improves estimation precision second stage online data augmentation procedure second stage improve refinement performance also introduce separate learning refinement structure finetunes body foot hand part independently experimental results show improves 8–10 average precision compared lightweight approaches wholebody pose estimation task nearly quarter 25 size model parameters saved © 2024 institute electrical electronics engineers inc rights reserved', 'human pose estimation challenging research task field computer vision current mainstream works made great progress pose estimation works still weakness two aspects first feature extraction module competent representation learning second training process take fully advantage projection model 3d space 2d plane work human pose estimation exploits 3d root coordinates subordinate input 2d joint coordinates provide positional reference recovered 3d joint coordinates employs inner camera parameters construct additional projection constraints recovering 3d joint coordinates moreover enhance feature learning residual branch tested two benchmark datasets human pose estimation experimental results show model superior current mainstream algorithms © 2021 authors exclusive licence springerverlag gmbh germany part springer nature', 'recent years significant progress human pose estimation fueled widespread adoption deep convolutional neural networks however despite advancements multiperson 2d pose estimation still remains highly challenging due factors occlusion noise nonrigid body movements currently multiperson pose estimation approaches handle joint localization association separately study direct regressionbased estimate 2d human pose single image authors name network yolorlepose compared traditional yolorlepose leverages transformer models better capture global dependencies image feature blocks preserves sufficient spatial information keypoint detection multihead selfattention mechanism improve accuracy yolorlepose model paper following enhancements firstly study introduces c3 module swin transformer c3str module builds upon c3 module look yolo incorporating swin transformer branch enhancing yolorlepose model ’ ability capture global information rich contextual information next novel loss function named rleoks loss loss function facilitates training process learning distributional changes residual loglikelihood estimation assign different weights importance different keypoints human body study introduces weight coefficient loss function experiments proved efficiency yolorlepose model coco dataset model outperforms previous sota 211 ap © 2024 authors', 'study human pose estimation widely used safety sports scenes performance deep learning greatly reduced high overlap rate crowded scenes therefore bottomup model called balancehrnet balanced highresolution module new branch attention module balancehrnet draws multibranch structure fusion popular model higherhrnet model overcomes shortcoming higherhrnet obtain large enough receptive field specifically connecting structure balanced highresolution module connect almost convolutional layers obtain sufficiently large receptive field time multiresolution representation maintained due balanced highresolution module enable model recognize objects richer scales obtain complex semantics information branch fusion design branch attention obtain importance different branches different stages finally model improves accuracy ensuring smaller amount computation higherhrnet crowdpose dataset used test dataset higherhrnet alphapose openpose taken comparison models ap measured balancehrnet 630 increased 31 compared best model — higherhrnet also demonstrate effectiveness network coco2017 keypoint detection dataset compared higherhrnetw32 ap model improved 16 © 2023 elsevier ltd', 'human pose estimation refers accurately estimating position human body single rgb image detecting location body serves basis several computer vision tasks human tracking 3d reconstruction autonomous driving improving accuracy pose estimation significant implications advancement computer vision paper addresses limitations singlebranch networks pose estimation presents topdown singletarget pose estimation multibranch selfcalibrating networks combined graph convolutional neural networks study focuses two aspects human body detection human body pose estimation human body detection athletes appearing sports competitions followed human body pose estimation divided two coordinate regressionbased heatmap testbased improve accuracy heatmap test highresolution feature map output hrnet used deconvolution improve accuracy singletarget pose estimation recognition © 2023 authors', 'number traffic accidents increased steadily recent years world according us national highway traffic safety administration 45 percent traffic accidents caused distracted driver work deals problem automating detection classification driver distraction monitoring driver case unsafe driving lightweight model driver distraction classification combines human pose estimation followed common classifier like randomforest movenet model uded human pose estimation ultrafast accurate model detects 17 keypoints human body keypoints prepocessed feeded classifier model trained publicly available dataset namely statefarm distracted driver detection dataset experimental results show keypoints human pose used good features classification give good performance compared deep neural networks © 2023 acm', 'featured application study could used sitting posture monitoring workfromhome setup could also used rehabilitation purposes patients posturerelated problems human posture recognition one challenging tasks due variation human appearance changes background illumination additional noise frame diverse characteristics amount data generated aside generating high configuration recognition human body parts occlusion nearly identical parts body variations colors due clothing various factors make task one hardest computer vision therefore studies require highcomputing devices machines could handle computational load task study used smallscale convolutional neural network smartphone builtin camera recognize proper improper sitting posture workfromhome setup aside recognition body points study also utilized points ’ distances angles help recognition overall study able develop two objective datasets capturing left right side participants supervision guidance licensed physical therapists study shows accuracies 8518 9207 kappas 0691 0838 respectively system developed implemented tested workfromhome environment © 2023 authors', 'one option teaching robot new skills learning demonstration techniques traditional techniques often involve expensive sensorsequipment advancements computer vision made possible achieve similar outcomes lower cost best knowledge previous research robot learning produce 3d motions 2d data knowledge interact people end designed study nao robot imitate human behavior reproducing motions 3d space viewing small number 2d rgb videos motion goal robot learn certain social interactive skills learning video observation apply humanrobot interaction five steps taken achieve objective 1 collecting dataset 2 human pose estimation 3 transferring data human space robot space 4 robot control 5 humanrobot interaction steps separated two phases robot imitation learning humanrobot social interaction majority algorithms employed deep learningbased achieving 96 accuracy action recognition dataset results also promising implemented robot overall preliminary exploratory study successfully showed proof concept producing 3d motions 2d data noteworthy amount online training data robot trained quickly require expert © 2023 authors exclusive licence springer nature bv', 'numerous applications tracking physical activity decoding sign language controlling fullbody gestures human position estimation video crucial human pose classification challenging yet wellresearched area particular way human joints arranged referred pose localization human joints predetermined markers images videos therefore defined topic human pose estimation pose estimate required applications like human activity detection sports yoga positions sign language applications motion mediapipe api utilized study human classification model categorizes human poses identifying tracking actions individual body parts paper demonstrates adaptability deep learning techniques highlights efficiency accuracy blazepose api providing reliable pose landmarks © 2023', 'present study modelling american sign language asl encoderonly transformers human pose estimation keypoint data enhanced version publicly available wordlevel asl wlasl dataset novel normalisation technique signer body size show impact model architecture accurately classifying sets 10 50 100 300 isolated dynamic signs twodimensional keypoint coordinates demonstrate importance running reporting results repeated experiments describe evaluate model performance descriptions algorithms used normalise data generate train validation test data splits report top1 top5 top10 accuracy results evaluated two separate model checkpoint metrics validation accuracy loss find models fewer 100k learnable parameters achieve high accuracy reduced vocabulary datasets paving way lightweight consumer hardware perform tasks traditionally resourceintensive requiring expensive highend equipment achieve top1 top5 top10 accuracies formula presented formula presented formula presented respectively vocabulary size 10 signs formula presented formula presented formula presented 50 signs formula presented formula presented formula presented 100 signs formula presented formula presented formula presented 300 signs thereby setting new benchmark task © 2023 authors', 'paper interested human pose estimation problem focus leveraging discriminative pose features recent pose estimation works concentrate extracting highlevel features ignore lowlevel details thus reducing prediction accuracy mitigate issues endtoend called multiscale representation transformer network msrt network consists two key components feature aggregation module fam transformers fam splits stacks feature maps different scales fuses achieve multiscale representation learning module makes lack detailed information highlevel features furthermore utilize transformers identify longrange interactions among feature maps capture implicit body structure information allows network refine locations terminal occluded joints compared existing regressionbased msrt achieves superior results coco2017 mpii datasets © 2023 authors exclusive licence springerverlag london ltd part springer nature', 'human pose estimation significant issue taken consideration computer vision network recent decades vital advance toward understanding individuals videos still images simple terms human pose estimation model takes image video estimates position person ’ skeletal joints either 2d 3d space several studies human posture estimation found literature however center around specific class instance modelbased methodologies human movement investigation later various deep learning dl algorithms came existence overcome difficulties earlier approaches study exhaustive review human pose estimation hpe including milestone work recent advancements carried survey discusses different twodimensional 2d threedimensional human 3d pose estimation techniques along classical deep learning approaches provide solution various computer vision problems moreover paper also considers different deep learning models used pose estimation analysis 2d 3d datasets done evaluation metrics used estimating human poses also discussed knowing direction individuals hpe opens road reallife applications talked study © 2022 authors exclusive licence springerverlag gmbh germany part springer nature', 'nowadays smart environments ses enable monitoring people physical disabilities incorporating activity recognition thermal cameras incorporated preserve privacy deep learning dl solutions pose users removes external noise although robust dl solutions visible spectrum vs fail thermal domain thus thermal human pose lite thposelite convolutional neural network cnn mobilenetv2 extracts pose thermal images tis novel way autolabeling developed includes background removal optical flow estimator also integrates blazepose pose estimator vs images vsis obtain poses preprocessed tis results show preprocessing increases percentage detected poses blazepose 1955 7685 allows recording human pose estimation hpe data sets vs without requiring vs cameras manually annotating data sets furthermore thposelite embedded internet things iot device incorporating edge tensor processing unit tpu accelerator process tis recorded 9 frames per second fps real time 1228 fps requires fewer 6w energy run achieved model quantization decreasing accuracy estimating poses 1 meansquared error mobilenetv2 test images 3548 obtaining accurate poses 21 images blazepose able detect pose © 2014', 'aiming problems complex background textures onsite weld images difficulty locating profile feature points realtime detection endtoend weld features improve welding efficiency quality building steel structures adopting idea human pose estimation extraction weld feature points equivalent key point detection task human skeleton following topdown paradigm pose estimation extraction weld feature building steel structure construction site established rtmdet object detector first introduced quickly locate welding profile area rtmpose pose estimation model applied detect profile feature points target region converting feature point coordinate regression localization classification problem horizontal vertical coordinates localization accuracy efficiency effectively improved experimental results demonstrate compared welding identification digital image processing regression fully connected layers rapidly precisely extract welding feature points images containing complex information localization error feature point single image less 2 pixels average processing time 38 2 ms meeting requirements automated welding construction sites © 2023 southeast university rights reserved', 'multiscale feature fusion commonlyused module existing deeplearning models feature misalignment occurs process feature fusion spatial misalignment hinders learning semantic representation multiscale levels received much attention misalignment problem caused feature position shift convolution interpolation operation feature fusion solve misalignment problem paper formulates shift error mathematically plugandplay unbiased feature position alignment strategy align convolution interpolation modelagnostic unbiased feature position alignment boost performance different models without introducing extra parameters furthermore unbiased feature position alignment applied build unbiased human pose estimation experimental results demonstrated effectiveness unbiased pose model comparison stateofthearts especially lowresolution field codes shared httpsgithubcomwangchen100unbiasedfeaturepositionalignmentforhumanposeestimation © 2023', 'transformerbased networks almost thoroughly outperformed convolutional neural network convnet predominate field pose estimation get hook resuscitate convnets convpose pure convnet utilize conventional improvement strategies like attention mechanisms lightweight approaches instead pioneeringly modernizes network structures modernization process includes deepening stem cell transition layers separate pointwise convolution layer adopting batch normalization bn layer resizing feature maps employing largekernel depthwise separable convolutions designing reparameterizedstyle structures constructing two consecutive modules mixer inverted bottleneck etc designs similar corresponding transformer architectures means translating transformerspecific components convolutional variations incorporating convnet modern convnet maintains simplicity convolutional also takes advantage transformers experiments show convposebl achieves 760 average precision ap score coco val2017 dataset convpose performs par better existing representative networks transformer convnet represents slight superiority terms speed © 2023 elsevier bv', 'mainstream multiperson pose estimation endtoend recently build endtoend detr aiming eliminate need handcrafted modules like heuristic grouping nms postprocessing however detrbased suffer heavy memory burden processing highresolution backbone feature maps transformers paper endtoend multiperson pose estimation fully convolutional network termed efcpose different detrbased directly predicts instanceaware poses pixelwise manner lightweight convolutional heads avoiding heavy memory burden overall adopts centeroffset formulation onetoone label assignment strategy achieve multiperson pose estimation endtoend manner main contribution fully convolutional heads includes two aspects one hand unaligned centeroffset representation learn reliable semantic centers replace inconsistent geometric centers improving performance instance detection hand novel regression strategy named limbaware adaptive regression leverages separate adaptive points convert challenging longrange offsets simplified shortrange offsets incorporates limb constraints elevate regression quality joint offsets compared current detrbased endtoend efcpose avoids high computational complexity achieves higher accuracy extensive experiments coco keypoint crowdpose benchmarks show efcpose outperforms bottomup singlestage without flipping augmentation', 'background work aims develop deep learning model assessing atlantoaxial subluxation aas rheumatoid arthritis ra often ambiguous clinical practice collected 4691 xray images cervical spine 906 patients ra among images 3480 used training deep learning model 803 used validating model training process remaining 408 used testing performance trained model twodimensional key points ’ detection model deep highresolution representation learning human pose estimation adopted convolutional neural network model model inferred four coordinates calculate atlantodental interval adi space available spinal cord sac finally values compared clinicians evaluate performance model results among 408 cervical images testing performance trained model correctly identified four coordinates 995 dataset values adi sac positively correlated among model two clinicians sensitivity aas diagnosis adi sac model 086 097 respectively specificity 057 05 respectively conclusions present development deep learning model evaluation cervical lesions patients ra model demonstrably shown useful quantitative evaluation © 2023 biomed central ltd part springer nature', 'digital tools offer extensive solutions explore novel interactiveart paradigms relying various sensors create installations performances human activity captured analysed used generate visual sound universes realtime deep learning approaches including human detection human pose estimation constitute ideal humanart interaction mediums allow automatic human gesture analysis directly used produce interactive piece art context paper presents interactive work art explores relationship thought movement combining dance philosophy numerical arts deep learning present novel system combines multicamera setup capture human movement human pose estimation models automatically analyze movement immersive 180° projection system projects dynamic textual content intuitively responds users behaviors demonstration consists two parts firstly professional dancer utilize setup deliver conferenceshow secondly audience given opportunity experiment discover potential setup transformed interactive installation allows multiple spectators engage simultaneously clusters words letters extracted conference text © 2023 ownerauthor', 'paper introduce posesonic intelligent acoustic sensing solution smartglasses estimates upper body poses system requires two pairs microphones speakers hinges eyeglasses emit fmcwencoded inaudible acoustic signals receive reflected signals body pose estimation customized deep learning model posesonic estimates 3d positions 9 body joints including shoulders elbows wrists hips nose adopt crossmodal supervision strategy train model synchronized rgb video frames ground truth conducted inlab semiinthewild user studies 22 participants evaluate posesonic userindependent model achieved mean per joint position error 617 cm lab setting 1412 cm semiinthewild setting predicting 9 body joint positions 3d studies show performance significantly impacted different surroundings devices remounted realworld environmental noise finally discuss opportunities challenges limitations deploying posesonic realworld applications © 2023 acm', '31 special focus conference intelligent 3d technologies topics 3d animation design production intelligent algorithm virtual reality 4dfa fourdimensional fullanatomy reconstruction individualized digital human models motion videos research fault prediction technology air compressor wavelet neural network development intelligent vr systems deep learning research implementation multi fusion data model construction technology distribution network digital twins research implementation rules extraction technology digital twin objects distribution network semantic understanding analysis urban landscape plants ’ configuration virtual reality 3d animation simulation computer virtual simulation technology lightweight human pose estimation selfattention mechanism new soil moisture prediction multiple influencing factors facial photoguided head anatomy modeling deep learning 2d3d shape prior model registration design water ecological cleaning robot raspberry pi opencv visual recognition fragrant pear target identification positioning deep learning online minigame popularity feedback data prediction time series bp neural network prediction mileage pile detection vehicleborne video exploration future temperature analysis arima time series model gabp neural network prediction model research feature extraction time series images research human eyesight tracking algorithm monocular vision automatic calibration high resolution lidar fisheye camera', 'paper efficient extracting feature points weld images noisy construction environments inspired human pose estimation reformulates weld feature point extraction skeletal keypoint detection task quick object detector locates weld region amidst complex backgrounds followed efficient feature point extraction via two coordinate classification tasks achieves subpixel accuracy low computational cost confines annotation within one bounding box four keypoints per image eliminating pixellevel labeling test results demonstrate realtime accurate feature point extraction superior efficiency robustness compared traditional thus facilitates quality control automated welding realworld construction scenarios © 2024', 'human body pose estimation represented joint rotations essential driving virtual characters present paper developed novel endtoend pointtopose mesh fitting network p2pmeshnet directly estimate body joint rotations p2pmeshnet provided strong collaboration deep learning network inverse kinematics network body pose estimation iknetbody selfcorrecting network iterative error feedback network ief introduced p2pmeshnet applied free mocap freemocap dataset covering openpose 3d joint locations reconstructed multiview openpose 2d joint locations generated joint rotations tested mean per joint position error mpjpe well percentage correct keypoints pck along area pck curve auc threshold range 0–60 mm procrustes aligned compared metrics p2pmeshnet 1131 mm 997 estimate error success rate well auc 809 demonstrated consistent tool future human body pose estimation runtime performance 100 frames per second implied potential application prospects © 2023 elsevier ltd', 'recent era 2d human pose estimation hpe become integral part advanced computer vision cv applications particularly understanding human behaviors despite challenges occlusion unfavorable lighting motion blur advancements deep learning significantly enhanced performance 2d hpe enabling automatic feature learning data improving model generalization given crucial role 2d hpe accurately identifying classifying human body joints optimization imperative response introduce spatially oriented attentioninfused structuredfeatureenabled poseresnet socaprnet enhanced 2d hpe model incorporates novel element spatially oriented attention soca designed enhance accuracy without significantly increasing parameter count leveraging strength resnet34 integrating global context blocks gcbs socaprnet precisely captures detailed human poses empirical evaluations demonstrate model outperforms existing approaches achieving percentage correct keypoints 05 pckh05 90877 50 threshold mean precision mean01 score 41137 results underscore potential socaprnet realworld applications robotics gaming human–computer interaction precise efficient 2d hpe paramount © 2023 authors', 'present umvpose address problem 3d pose estimation multiple persons multiview scenario different recent supervised techniques work need labeled data perform 3d pose estimation furthermore generating 3d annotations costly high probability containing errors plane sweep generate 3d pose estimation define one view target remainder reference views estimate depth 2d skeleton target view obtain 3d poses instead comparing ground truth poses project estimated 3d poses onto reference views compare 2d projections 2d poses obtained offtheshelf 2d poses pedestrian obtained target reference views must matched allow comparison performing matching process ground points identify corresponding 2d poses compare respective projections furthermore new reprojection loss smooth l1 norm evaluated publicly available campus dataset result obtained better accuracy unsupervised achieving 05 points best geometric furthermore outperform supervised results comparable bestsupervised achieving 02 points © 2023 scitepress science technology publications lda', 'performance multiperson pose estimation greatly improved due rapid development deep learning however problems selfocclusion mutual occlusion complex background effectively solved order effectively solve problems paper design novel global local contentaware feature boosting network glcfbnet includes intralayer feature residuallike module ifrm input feature aggregation module ifam spatial channel feature hourglass attention module scfham novel ifrm expand receptive field convolution layer aggregation feature iram fully extract edge information input imageand effectively solve problem negative background impact scfham accurately determine location occluded keypoints judge global information reasonable keypoints extract effective features joint node positioning redundant feature information evaluate effectiveness mscoco keypoint detection dataset mpii human pose dataset crowdpose dataset © 2023 authors exclusive licence springer sciencebusiness media llc part springer nature', 'cricket massive global following ranked second popular sport globally estimated 25 billion fans batting requires quick decisions ball speed trajectory fielder positions etc recently computer vision machine learning techniques gained attention potential tools predict cricket strokes played batters study presents cuttingedge predicting batsman strokes computer vision machine learning study analyzes eight strokes pull cut cover drive straight drive backfoot punch drive flick sweep study mediapipe library extract features videos several machine learning deep learning algorithms including random forest rf support vector machine knearest neighbors decision tree linear regression long shortterm memory predict strokes study achieves outstanding accuracy 9977 rf algorithm outperforming algorithms used study kfold validation rf model 950 standard deviation 007 highlighting potential computer vision machine learning techniques predicting batsman strokes cricket study ’ results could help improve coaching techniques enhance batsmen ’ performance cricket ultimately improving game ’ overall quality © 2023 authors', 'recent years human pose estimation achieved impressive results rgb images advent deep learning architectures large annotated datasets contributed achievements however little done towards estimating human pose depth maps especially towards obtaining precise 3d body joint localization fill gap paper presents refinet depthbased 3d human pose refinement given depth map initial coarse 2d human pose refinet regresses fine 3d pose composed three modules different data representations ie 2d depth patches 3d human skeletons point clouds extensive experimental evaluation carried investigate impact model hyperparameters compare refinet offtheshelf 2d literature approaches results confirm effectiveness limited computational requirements © 2023 elsevier bv', 'current human pose estimation networks difficult widely used mobile devices embedded platforms due arithmetic power memory limitations address problem，this paper lightweight human pose estimation network xhrnet hrnet basic resnext module replace common basic module reduce parameters computational complexity network model achieves 782 accuracy coco validation set，which 19 higher hrnet，the number parameters decreases 222m，and computational effort decreases 273 gflops xhrnet combination accuracy lightweight，which new lightweight human pose estimation network embedded platforms reducing computation number parameters effectively maintaining accuracy © 2023 hunan university rights reserved', 'many research efforts spent developing robust videobased algorithms human pose estimation goal compare videobased algorithms pose estimation gait analysis conducted experiment healthy subject performing walking sessions treadmill three different speeds slow 36 kmh medium 5 kmh high 7 kmh rgb 4k camera placed laterally sagittal plane four algorithms compared colour threshold filtering blobanalysis three deep learningbased markerless algorithms ii tcformer iii fastpose iv blazepose colour threshold filtering blobanalysis algorithm six magenta passive markers placed joint centres subjects lower limb selected deep learningbased markerless algorithms supported various opensource pose estimation toolboxes pretrained several wholebody keypoint datasets 2d trajectories joint centres compared considering root mean square error pearsons coefficient preliminary results showed high correlations marker markerless algorithms walking speeds tcformer generally performed better root mean square error trajectories 35 mm suffer selfocclusion issues © 2023 institute physics publishing rights reserved', 'recent years 2d human pose estimation hpe become increasingly important complex computer vision tasks including understanding human behavior interaction despite challenges like occlusion unfavorable lighting motion blur deep learning techniques revolutionized 2d hpe allowing automatic feature learning data improving generalization new model called spatiallyaware attentionbased hierarchical features enabled lightweight poseresnet sahflightposeresnet 2d hpe model extends simple baseline network spatiallyaware attentionbased hierarchical features enhance accuracy minimizing parameters model efficiently captures finer details incorporating resnet18 global context blocks novel sahf module sahflightposeresnet demonstrates superior performance compared existing achieving pckh05 908 mean01 metric 411 highlighting enhanced accuracy efficiency model important practical applications robotics gaming humancomputer interaction accurate efficient 2d hpe essential © 2024 authors exclusive license springer nature singapore pte ltd', 'person reidentification reid crucial task computer vision aims match persons captured nonoverlapping camera views paper alleviate impacts background occlusion instance segmentation pose estimation create masks global feature extraction furthermore divide pedestrian images three regions according pedestrian keypoints trying eliminate alignment errors partbased matching strategy also helps address occlusion issue overall construct deep learning network three branches including two global branches partbased branch two global branches extract global features segmentationbased mask mask derived pedestrian keypoint heatmaps respectively end weighted fusion strategy used combine global scores partbased scores final classification network enables us acquire robust global feature pedestrians excluding background occlusion simultaneously address alignment errors extent experimental results three widely used datasets demonstrate effectiveness specifically achieves 645 rank1 accuracy 543 map occludedduke 944 871 rank1 accuracies market1501 dukemtmcreid respectively © 2023', 'human pose estimation hpe attracted significant amount attention computer vision community past decades moreover hpe applied various domains humancomputer interaction sports analysis human tracking via images videos recently deep learningbased approaches shown performance hpebased applications although deep learningbased approaches achieved remarkable performance hpe comprehensive review deep learningbased hpe remains lacking literature article provide uptodate indepth overview deep learning approaches visionbased hpe summarize 2d 3d hpe applications discuss challenges research trends bibliometrics provide insightful recommendations future research article provides meaningful overview introductory material beginners deep learningbased hpe well supplementary material advanced researchers © 2022', 'camerabased human pose estimation become popular due wide applications easy implementation however applicable several circumstances poor illumination occlusion private protection work utilize wifi signals estimate 2d human poses challenging wifi signals abstract limited information address challenges develop evolving attentive spatialfrequency network discover relationship signal variation body movement wifibased 2d human pose estimation first taking dilated csi sequences inputs spatialfrequency encoder introduced effectively integrate static spatial information dynamic frequency information csi signals finally design evolving attention module enable model attend certain channels features due lack benchmarks two wifibased human pose estimation datasets general pose estimation dataset gpe specific pose estimation dataset spe released public download project page extensive experiments datasets show model outperforms least 16 terms pck20 percentage correct keypoints © 2023', 'gait analysis plays important role fields healthcare sports sciences conventional gait analysis relies costly equipment optical motion capture cameras wearable sensors require trained assessors data collection processing recent developments computer vision deep neural networks monocular rgb cameras 3d human pose estimation shown tremendous promise costeffective efficient solution clinical gait analysis paper markerless human pose technique developed motion captured consumer monocular camera 800 × 600 pixels 30 fps clinical gait analysis experimental results shown postprocessing algorithm significantly improved original human pose detection model blazepose ’ prediction performance compared goldstandard gait signals 107 movi dataset addition predicted t2 score excellent correlation ground truth r 099 094x 001 regression line supports potential alternative conventional markerbased solution assist clinical gait assessment © 2023 authors', 'number approaches address occlusion truncation issues pose estimation typically require additional devices specific recording environments paper novel pose estimation mechanism utilizes ganbased viewpoint transformation mechanism complements missing pose information without requiring additional devices prealigned camera setups achieves transforming supplementary viewpoint video target viewpoint video utilizing gan complement missing keypoints target viewpoint video evaluation results confirm mechanism effectively complements missing pose information provides reliable viewpoint transformation performance © 2023', 'human posture estimation one foremost essential errands within field computer vision empowers localization discovery key body points advantage areas like gaming excitement sports investigation numerous capturing human developments analysing giving feedback execution profoundly advantage human posture estimation dependable location later progressions within field profound learning presently made conceivable create realtime capability precisely recognize analyse human postures realtime © 2023', 'computer visionoriented human pose estimation focused location human skeleton image video，in pose information used pose estimation specific pose actionobjective location analysis terms position relationship key areas human body nowadays，human pose estimationoriented action recogni⁃ tion pose tracking developing intensively conventional pose estimation segmented two categories object detection pose estimation object detection analysis segmentation，matching，or statistical learning，which challenged targets backgrounds clarification complex scenarios still vulner⁃ able prior information additionally，it timeconsuming laborintensive construct training sample libraries classifiers pose estimation analysis relevance modelbased nonmodelbased methods，which challenged object detectionderived error extension much artificial constraint information nevertheless，its efficiency still optimized farther emerging artificial intelligence（ai）based deep learning technique potentials recognition precision speed deep learningbased human pose estimation certain extent gener⁃ ally，human pose estimation divided twodimensional threedimensional human pose estimation threedimensional human pose estimation，twodimensional human pose estimation model beneficial dealing crowd⁃ ing occlusion situations however，most network models originated convolutional neural network（cnn）mod⁃ els challenged depthloaded network speed lightweight twodimensional human pose estimation networks concerned edge measurement deployment review development process optimization trend twodimensional human pose estimation model deep learning literately divided three categories：singleperson pose estimation，multiperson pose estimation，and lightweight human pose estimation singleperson pose estimation basis multiperson pose estimation，which divided keypoints regression heatmap detection，and trend combine two achieve singleperson pose estimation over⁃ all，multiperson pose estimation network model divided topdown，bottomup，and others precision topdown network model higher，but time efficiency satisfactory，especially crowded problemrelated input data number human bodies larger input data，the estimation time much longer network model precision bottomup network model shrunk small range，but efficiency greatly improved more⁃ over，time consumption network model used human poseestimated independent number human bodies input data two actually dual initially，to locate position human body input data，topdown pose estimation focused body detector，and pose estimation per⁃ formed sample specifically，some topdown need crop singleperson body accurately adjust central position input data estimation bottomup oriented get body keypoints input data keypoints assigned objects time，the appearance singlestage network also means researchers need pay attention computational cost network model small number net⁃ works combined topdown bottomup together，and achieved good results summarize mul⁃ tiple cnn models used various human pose estimations，analyze characteristics various neural network models，and compare performance various pose estimation seen structural design deep convolu⁃ tional neural network models becoming diverse，but various deep learning network models still cer⁃ tain limitations dealing human pose estimation tasks technical adopted twodimensional human pose estimation models existing problems discussed，and possible future research directions pre⁃ dicted recommendation aware improve existing twodimensional pose estimation network model preprocessing input data aspects mentioned below：the clarity input data directly affects pose estimation results，and effective image video preprocessing may become new idea improve precision efficiency pose estimation existing pose estimation mostly via video datacut static video frames essence，it still restricted image data pose estimation current realtime pose estimation video data essential applica⁃ tion pose tracking action recognition nowadays，a combine deep learning pose estimation related time domain information，such optical flow，pose flow long shortterm memory images involved actual application still developed aspects crowded serious occlusion，so still resolved optimized recent pose estimation network models improved lightweight lightweight potentials one key directions pose estimation ©2023 chinsocfor eleceng', 'musculoskeletal conditions affect millions people globally however conventional treatments pose challenges concerning price accessibility convenience many telerehabilitation solutions offer engaging alternative rely complex hardware body tracking work explores feasibility model 3d human pose estimation hpe monocular 2d videos mediapipe pose physiotherapy context comparing performance ground truth measurements mediapipe pose investigated eight exercises typically performed musculoskeletal physiotherapy sessions range motion rom human joints evaluated parameter model showed best performance shoulder abduction shoulder press elbow flexion squat exercises results shown mape ranging 149 250 pearson ’ coefficient ranging 0963 0996 cosine similarity ranging 0987 0999 exercises eg seated knee extension shoulder flexion posed challenges due unusual poses occlusions depth ambiguities possibly related lack training data study demonstrates potential hpe monocular 2d videos markerless affordable accessible solution musculoskeletal telerehabilitation approaches future work focus exploring variations 3d hpe models trained physiotherapyrelated datasets fit3d dataset postpreprocessing techniques enhance model ’ performance © 2023 authors', 'human pose estimation aims detecting human joint points input data images videos building human body model motion analysis however due ambiguities human occlusion depth blurring lack training data accuracy motion capture still far satisfactory paper reviews advances human pose estimation markerless motion capture since 2019 three types representations human body detect unified volumetric model provides detailed motion representation introduce datasets evaluation metrics widely used 2d 3d pose estimation comparisons discussions conducted different model human pose estimation accuracy robustness speed summarizing strengths weaknesses various discover pose estimation transformer exhibit better accuracy robustness kinematic physical knowledge greatly assist solving 3d pose estimation additionally lightweight often overlooked research conclusion paper serves guide researchers interested field assists newcomers selecting developing human pose estimation © 2024 uturn press llc httpwwwcadjournalnet', 'article describes pictonaut novel automatically synthetise animated shots motion picture footage results editable backgrounds characters lighting etc conventional 3d software finish professional 2d animation rather addressing challenge solely image translation problem hybrid combining multiperson 3d human pose estimation gans taken subsampled video frames processed openpose smplifyx obtain 3d parameters pose body hands face expression depicted characters captured parameters retargeted manually selected 3d models cel shaded mimic style 2d cartoon results subsampled frames interpolated generate complete smooth motion characters background cartoonized gan qualitative evaluation shows feasible small dataset synthetised shots obtained real movie scenes provided © 2023 authors', 'paper mmposefk novel millimeter wave mmwave radarbased pose estimation employs dynamic forward kinematics fk address challenges posed low resolution specularity noise artifacts commonly associated mmwave radars issues often result unstable joint poses vibrate time reducing effectiveness traditional pose estimation techniques overcome limitations integrate fk mechanism deep learning model develop endtoend solution driven data comprehensive experiments various matrices benchmarks highlight superior performance mmposefk especially compared previous research provides accurate pose estimation ensures increased stability consistency underscores continuous improvement methodology showcasing superior capabilities antecedents moreover model output joint rotations human bone lengths could utilized various applications gait parameter analysis height estimation makes mmposefk highly promising solution wide range applications field human pose estimation beyond', 'human pose estimation hpe task aims predict location human joints images videos task used many applications sports analysis surveillance systems recently several studies embraced deep learning enhance performance hpe tasks however building efficient hpe model difficult many challenges like crowded scenes occlusion must handled paper followed systematic procedure review different hpe models comprehensively 100 articles published since 2014 hpe deep learning selected several selection criteria image video data types investigated furthermore single multiple hpe reviewed addition available datasets different loss functions used hpe pretrained feature extraction models covered analysis revealed convolutional neural networks cnns recurrent neural networks rnns used hpe moreover occlusion crowd scenes remain main problems affecting models ’ performance therefore paper presented various solutions address issues finally paper highlighted potential opportunities future work task © 2023 authors', 'human pose estimation aims locate human body parts build human body representation eg body skeleton input data images videos drawn increasing attention past decade utilized wide range applications including humancomputer interaction motion analysis augmented reality virtual reality although recently developed deep learningbased solutions achieved high performance human pose estimation still remain challenges due insufficient training data depth ambiguities occlusion goal survey article provide comprehensive review recent deep learningbased solutions 2d 3d pose estimation via systematic analysis comparison solutions input data inference procedures 260 research since 2014 covered survey furthermore 2d 3d human pose estimation datasets evaluation metrics included quantitative performance comparisons reviewed popular datasets summarized discussed finally challenges involved applications future research directions concluded regularly updated project page provided httpsgithubcomzczcwhdlhpe © 2023 copyright held ownerauthors publication rights licensed acm', 'human pose estimation hpe computer vision application estimates human body joints images gives machines capability better understand interaction humans environment accomplishment many hpe deployed robots vehicles unmanned aerial vehicles uavs effort raised challenge balance algorithm performance efficiency especially uavs computational resources limited saving battery power despite considerable progress hpe problem face challenge highlight severity fact paper presents brief review hpe benchmark aspect algorithms performance efficiency uav operation specifically contribution hpe last 22 years covered along variety exist benchmark consists 36 pose estimation models 3 known datasets metrics fulfill paper aspect results mobilenetbased models achieved competitive performance lowest computational cost comparison resnetbased models finally benchmark results projected edge devices hardware specifications analyze appropriateness algorithms uav deployment © 2023 authors', 'estimating 3d human poses images illposed regression problem usually tackled viewpointinvariant convolutional neural networks cnns recently capsule networks capsnets introduced viable alternative cnns ensuring viewpointequivariance drastically reducing dataset size network complexity retaining high output accuracy realtime endtoend human pose estimation hpe network employs matrix capsules 1 fast variational bayesian capsule routing without relying pretraining complex data augmentation multiple datasets achieve comparable results hpe lowest error among capsnets time achieving desirable properties namely greater generalization capabilities stronger viewpoint equivariance highly decreased data dependency allowing network trained fraction available datasets without data augmentation © 2022 elsevier bv', 'improper spacing rebar spacers rss reinforced concrete structures lead corrosion damage rebars ultimately decreasing strength durability structure current rss spacing inspection entails manual inspection utilizes measuring tape laborintensive timeconsuming paper presents novel automatically inspecting rss spacing visionbased deep learning computational geometry consists three modules including rss location classification module keypoints detection module rss spacing inspection module rss location classification module employ object detection fasterregion convolutional neural network fasterrcnnbased deep learning accurately locate classify rss keypoints detection module human pose estimation cascaded pyramid network cpnbased deep learning detect three keypoints necessary accurate rss spacing calculations rss spacing inspection module computational geometry applied derive accurate expression calculating rss spacing transverse longitudinal directions rss keypoints coordinates camera parameters rss spacing subsequently evaluated check compliance experiment algorithms three modules integrated robot automatically move take pictures accomplish task automatically inspecting rss spacing large reinforced concrete component plant experimental results indicate yields average rss spacing measurement error 135 cm finally robustness validated changing conditions image acquisition processes © 2023', 'human pose detection one essential factors many surveillancebased applications fall detection humancomputer interaction activities sports fitness motion movement analysis robotics many artificial intelligence projects applications survey aim cover used human pose detectionsingle person multiple people examine efficiency required parameters realtime compatibility compare discuss different technologies used posture detection results research used improve results systems pose detection primary parameter hence helpful many lifesaving applications fall detection also aim research develop efficient model human pose detection deep neural networks model works images video human pose detection one essential factors many surveillancebased applications fall detection humancomputer interaction activities sports fitness motion movement analysis robotics many artificial intelligence projects applications survey aim cover used human pose detectionsingle person multiple people examine efficiency required parameters realtime compatibility compare discuss different technologies used posture detection results research used improve results systems pose detection primary parameter hence helpful many lifesaving applications fall detection also aim research develop efficient model human pose detection deep neural networks model works images videos model built singleperson pose estimation help machine learning model built singleperson pose estimation help machine learning © 2023 ismail saritas rights reserved', 'article comprehensive survey deep learningbased dlbased human pose estimation hpe help researchers domain computer vision presented hpe among fastestgrowing research domains computer vision used solving several problems human endeavours detailed introduction three different human body modes followed main stages hpe two pipelines twodimensional 2d hpe presented details four components hpe also presented keypoints output format two popular 2d hpe datasets cited dlbased hpe articles year breakthrough shown tabular form study intends highlight limitations published reviews surveys respecting presenting systematic review current dlbased solution 2d hpe model furthermore detailed meaningful survey guide new existing researchers dlbased 2d hpe models achieved finally future research directions field hpe limited data disabled persons multitraining dlbased models revealed encourage researchers promote growth hpe research © 2023 tech science press rights reserved', 'recently selfattention mechanisms become increasingly popular computer vision applications following success transformer natural language processing yet transformer remains underappreciated compared dominant role convolutional neural networks field computer vision study present various approaches transformers application human pose estimation novel model transnet convolutional neural network design parallel transformer encoder branch capture longrange spatial dependency simultaneously fusing local features extracted input images experiments results show transnet achieves exceptional performance human pose estimation coco dataset model outperforms competitors achieves average precision ap score 783 coco val set specifically significant improvement average score model advanced convolutional neural networks believe research contribute better understanding transformers within computer vision models © 2023 elsevier inc', '28 topics discussed human activity role identification feature vector encoding techniques natural language sentences access control biometrics future realtime privacy preserving human activity recognition mobile 1dcnnbilstm deep learning empirical study emotional state recognition multimodal fusion monitoring applicationdriven continuous affect recognition video frames human body part semantic segmentation enabled parsing human pose estimation automatic segmentation pneumothorax chest radiographs dualtask interactive learning transformer hyperspectral image classification multifeature learning optimal parameter values estimating rotational eye movement vascular images white part eyeball computational investigation precision autism metabolic disorders predictive machine learning hepatic ailment classification', 'supervised deep learning models optimised applying regularisation techniques reduce overfitting prove difficult fine tuning associated hyperparameters hyperparameters equal understanding effect hyperparameter regularisation technique performance given model paramount importance research present first comprehensive largescale ablation study encoderonly transformer model sign language improved wordlevel american sign language dataset wlaslalt human pose estimation keypoint data view put constraints potential optimise task measure impact range model parameter regularisation data augmentation techniques sign classification accuracy demonstrate within quoted uncertainties formula presented parameter regularisation none regularisation techniques employ appreciable positive impact performance find contradiction results reported similar albeit smaller scale studies also demonstrate model architecture bounded small dataset size task finding appropriate set model parameter regularisation common basic dataset augmentation techniques furthermore model configuration report new maximum top1 classification accuracy 84 100 signs thereby improving previous benchmark result model architecture dataset © 2023 authors', 'emotion detection presents challenges intelligent humanrobot interaction uri foundational deep learning techniques used emotion detection limited informationconstrained datasets models lack necessary complexity learn interactions input data elements variance human emotions across different contexts current effort introduce 1 moemo motion emotion crossattention vision transformer vit human emotion detection within robotics systems 3d human pose estimations across various contexts 2 data set offers fullbody videos human movement corresponding emotion labels human gestures environmental contexts compared existing approaches effectively leverages subtle connections movement vectors gestures environmental contexts crossattention extracted movement vectors fullbody human gesturesposes feature maps environmental contexts implement crossattention fusion model combine movement vectors environment contexts joint representation derive emotion estimation leveraging naturalistic motion database train moemo system jointly analyze motion context yielding emotion detection outperforms current © 2023', 'axial postural abnormalities apa common features parkinson ’ disease pd manifest 20 patients course disease apa form spectrum functional trunk misalignment ranging typical parkinsonian stooped posture progressively greater degrees spine deviation current research yet led sufficient understanding pathophysiology management apa pd partially due lack agreement validated userfriendly automatic tools measuring analysing differences degree apa according patients ’ therapeutic conditions tasks context human pose estimation hpe software deep learning could valid support automatically extrapolates spatial coordinates human skeleton keypoints images videos nevertheless standard hpe platforms two limitations prevent adoption clinical practice first standard hpe keypoints inconsistent keypoints needed assess apa degrees fulcrum second apa assessment either requires advanced rgbd sensors processing rgb images likely sensitive adopted camera scene eg sensor–subject distance lighting background–subject clothing contrast article presents software augments human skeleton extrapolated hpe software rgb pictures exact bone points posture evaluation computer vision postprocessing primitives article shows software robustness accuracy processing 76 rgb images different resolutions sensor–subject distances 55 pd patients different degrees anterior lateral trunk flexion © 2023 authors', 'recent years 2d human pose estimation hpe become increasingly significant complex computer vision tasks encompassing understanding human behavior interactions although faced challenges like occlusion unfavorable lighting motion blur deep learning techniques revolutionized 2d hpe enabling automatic feature learning data improving generalization however prevalent 2d heatmap encounter issues like quantization errors leading complex postprocessing needs paper innovative coordinated classification 2d hpe emphasizing enhanced prediction accuracy model parameter optimization novel model aecaprnetcc leverages modified resnet34 architecture amplified adaptive efficient channel attention aeca mechanism prioritizing essential features enhance accuracy additionally model categorizes pixels bins mitigate quantization errors thereby achieving superior accuracy comprehensive evaluations coco dataset validate models superior performance accuracy computational efficiency compared prevailing 2d hpe techniques © 2023', 'physical rehabilitation plays crucial role restoring motor function following injuries surgeries however challenge overcrowded waiting lists often hampers doctors ability monitor patients recovery progress person deep learning offer solution enabling doctors optimize time patient distinguish requiring specific attention making positive progress doctors flexion angle limbs cue assess patients mobility level rehabilitation computer vision perspective task framed automatically estimating pose target body limbs image objectives study summarized follows evaluating comparing multiple pose estimation ii analyzing subjects position camera viewpoint impact estimation iii determining whether 3d estimation necessary 2d estimation suffices purpose conduct technical study due limited availability public datasets related physical rehabilitation exercises introduced new dataset featuring 27 individuals performing eight diverse physical rehabilitation exercises focusing various limbs body positions exercise recorded five rgb cameras capturing different viewpoints person infrared tracking system named optitrack utilized establish ground truth positions joints limbs study results supported statistical tests show pose estimators perform equally presented situations eg patient lying stretcher vs standing statistical differences exist camera viewpoints frontal view convenient additionally study concludes 2d pose estimators adequate estimating joint angles given selected camera viewpoints', 'existing deep learning models human pose estimation hpe shown satisfactory performance monitoring human actions however usually face dilemma complexity accuracy address challenge effective reference learning hpe namely referpose new distance optimization strategy specifically utilize reference model pose learning representation pose representation learned entire database merged reference model providing continuous reference learning guidance intraining model addition design new cosine annealingbased reference guidance temporal denoising develop distance optimization strategy provide joint guidance pose knowledge model representation temporal experience experimental results two benchmark databases human fall monitoring system demonstrate referpose achieves promising accuracy improvement compared several representative hpe models also offers low cost high efficiency', '2d human pose estimation aims lo identify locate human body keypoints person images fundamental task human analysis understanding field human pose estimation support multiple downstream tasks applied many realworld applications recent years thanks developments deep learning techniques significant progresses made human pose estimation number persons image human pose estimation tasks summarized singleperson pose estimation challenging multiperson pose estimation respectively paper first introduces research background problem definition task difficulty keypoint representation human pose estimation task next introduce representative singleperson multiperson pose estimation respectively singleperson pose estimation section introduces regressionbased detectionbased including network structure designing heatmap encodingdecoding multitask learning categories multiperson pose estimation section introduces heatmap regressionbased summarize widelyused datasets benchmark metric performance representative datasets paper also selects representative category analyzes compares failure cases finally paper discusses remaining challenges promising research directions human pose estimation © 2024 science press rights reserved', 'threedimensional human pose estimation made significant advancements integration deep learning techniques survey provides comprehensive review recent 3d human pose estimation focus monocular images videos multiview cameras stands systematic literature review methodology ensuring uptodate meticulous overview unlike many existing surveys categorize approaches learning paradigms survey offers fresh perspective delving deeper subject imagebased approaches follow existing categorizations also introduce compare significant 2d models additionally provide comparative analysis enhancing understanding imagebased pose estimation techniques realm videobased approaches categorize types models used capture interframe information furthermore context multiperson pose estimation survey uniquely differentiates approaches focusing relative poses addressing absolute poses survey aims serve pivotal resource researchers highlighting deep learning strategies identifying promising directions future exploration 3d human pose estimation © 2023 authors', 'realtime 3d human pose estimation crucial humancomputer interaction cheap practical estimate 3d human pose monocular video however recent bonesplicingbased 3d human pose estimation brings problem cumulative error article concept virtual bones solve challenge virtual bones imaginary bones nonadjacent joints exist reality bring new loop constraints estimation 3d human joints network article predicts real bones virtual bones simultaneously final length real bones constrained learned loop constructed predicted real bones virtual bones besides motion constraints joints consecutive frames considered consistency 2d projected position displacement predicted network captured real 2d displacement camera new projection consistency loss learning 3d human pose experiments human36m data set demonstrate good performance ablation studies demonstrate effectiveness interframe projection consistency constraints intraframe loop constraints © 2016', 'aiming problem human pose estimation network model trying improve accuracy thus leads large number parameters computations paper lightweight human pose estimation network model present solution called lhrnet incorporates depthwise separable convolution sandglass module attention mechanism reduce networks parameters computational complexity maintaining high level accuracy compared highresolution network hrnet 10 lhrnets model size params 56 computational complexity flops 119 lhrnet demonstrates effectiveness efficiency benchmark dataset coco keypoint detection dataset achieving 652 ap coco testdev set 159 parameters 089 gflops code models publicly available httpsgithubcomapingjjlhrnetgit © 2023', 'human activity recognition challenging computer vision problem involves identifying human activities artificial intelligence data collected various sensors paper existing deep neural networks studied custom deep architecture human pose activity recognition architecture mainly localizing human body joints 3d space single 2d image fundamental problem wide variations appearance caused various camera angles clothing body shapes selfocclusion complications background variations human 36m patient mocap datasets used benchmark architecture shows competitive performance compared © 2023', 'video games one popular multimedia forms generate higher profits traditional film industry meantime advances deep learning computer vision algorithms become powerful analyzing video content applied fps video games advanced cheating tools taken video games industry storm algorithms including object detection human pose estimations could analyze understand video content frame help player automatically identify aim enemies extremely fast reaction compared classic cheating tools computervisionbased cheating tools harder detect defend need manipulate software system purely simulate well trained skilled human gamer plays video game paper proactive comprehensive defense generates perturbations perceptible humans yet still mislead computer vision algorithms specifically comprehensive includes two parts defense aims fail computer visionbased cheating tools detect ingame characters penalty aims fool computer visionbased cheating tools detect fake regions ingame characters worsen cheating experience also serve trigger detecting cheating behavior work first implement object detection cheating tools evaluation environment implement defense penalty comprehensive approaches evaluate performance four popular video games results show comprehensive obtains high success rate minor impact user experience quality © 2023', 'human pose estimation made great progress performance due development deep learning current including lightweight networks usually generate highresolution heatmaps rich position information ensure high accuracy however computational cost heavy sometimes unacceptable mobile devices paper construct network backbone modified mobilenetv2 generate lowresolution representations enhance capability keypoints localization model also make crucial improvements consisting bottleneck atrous spatial pyramid localspace attention coordinate attention position embedding addition design two different network heads 2d 3d pose estimation explore extensibility backbone model achieves superior performance lightweight 2d pose estimation models coco mpii datasets achieves 25 fps huwei kirin 9000 outperforms movenet device 3d model also makes nearly 50 90 reduction parameters flops compared lightweight alternatives code available httpsgithubcomnanxinyumobilelrposegit © 2024 authors exclusive license springer nature singapore pte ltd', 'parkinsons disease pd prevalent neurodegenerative condition ranking second incidence alzheimers disease presently diagnosis pd sensors computer vision recognize characteristics like speech respiration gait patients however typically require complex hardware systems extensive data analysis collecting processing motion data various joints patients research efficient diagnostic system pd introduced utilizes common smartphones capture videos 42 healthy volunteers 30 parkinsons patients walking different conditions system utilizes human pose estimation technology extract threedimensional joint coordinates simplifying data collection process afterward joint coordinates transformed twodimensional images color mapping techniques thus converting diagnosis image recognition challenge contrast video processing reduces 651 million parameters 22 reduces testing time 6292 finally combining resnet50 random forest algorithms decision accuracy improved twodimensional images experimental evidence substantiates systems capability efficiently identify parkinsonian gait recognition rate leg joint group 8889 would increase 9167 factoring fourlimb joint group system provides potential preliminary athome diagnosis holding significant clinical application potential diagnostic value movementrelated disorders © 2023 elsevier ltd', 'nowadays neural networks becoming increasingly swelling achieve high accuracy better adaptability task human pose estimation heavy neural networks applied achieve higher performance extensive networks would lead lower inference speed consume computing resources acquire preferable accuracy even smaller model become valuable research subject paper applied knowledge distillation improve performance lightweight model still performance disparities teacher networks student networks exist narrow gap multiheaded architecture promote accuracy smaller models raising performances similar level larger models © 2022 acm', '3d human pose estimation major focus area field computer vision plays important role practical applications article summarizes research progress related estimation monocular rgb images videos overall perspective integrated deep learning introduced novel imagebased videobased inputs analysis viewpoint common problems discussed diversity human postures usually leads problems occlusion ambiguity lack training datasets often results poor generalization ability model regression crucial solving problems considering imagebased input multiview commonly used solve occlusion problems multiview analyzed comprehensively referring videobased input human prior knowledge restricted motion used predict human postures addition structural constraints widely used prior knowledge furthermore weakly supervised learning studied discussed two types inputs improve model generalization ability problem insufficient training datasets must also considered especially 3d datasets usually biased limited finally emerging popular datasets evaluation indicators discussed characteristics datasets relationships indicators explained highlighted thus article useful instructive researchers lacking experience find field confusing addition providing overview 3d human pose estimation article sorts refines recent studies 3d human pose estimation describes kernel problems common useful discusses scope research © 2023 tech science press rights reserved', 'applying computer vision machine learning techniques sport tests effective way realize intelligent sportsfacing practical application design realtime lightweight deep learning network realize intelligent pullups test study main contributions follows 1 new selfproduced pullups dataset established requirement including human body horizontal bar addition semiautomatic annotating software developed enhance annotation efficiency increase labeling accuracy 2 novel lightweight deep network named peposenet designed estimate analyze human pose real time backbone network made heatmap network key point network conduct human pose estimation key points extracted human body horizontal bar depthwise separable convolution adopted speed training convergence 3 evaluation criterion intelligent pullups test defined action quality assessment aqa action quality five states ie ready end hang pull achieved resume one pullups test cycle automatically graded random forest classifier mobile application developed realize intelligent pullups test real time performance model software confirmed verification ablation experiments experimental results demonstrated peposenet competitive performance state art pck 02 frames per second fps achieved 838 30 fps respectively mobile application promising application prospects pullups test complex scenarios © 2023 guozhong liu et al', 'hrnet demonstrated effectiveness feature representation highresolution network human pose estimation however given hrnet still human pose estimation storage computation expensive article aim reducing storage computation introducing several new techniques different parts hrnet including depthwise convolution selfattention pixelshuffle swish activation function hrandom erasing etc new planning hrnet enhanced hrnet maintains highresolution information time reduces computation burden largely comprehensive experiments designed wellknown benchmark datasets mpii coco results benchmark datasets achieved 62 parameters 40 computation comparing hrnet © authors exclusive license springer nature singapore pte ltd 2023', 'paper human pose estimation algorithm impulse radio ultrawideband iruwb radar transformerbased deep learning model built iruwb radar system 8by8 multipleinput multipleoutput mimo antenna array iruwb radar system paper advantageous throughwall detection application since operates low frequency range ie 045 355 ghz compared existing works rfbased human pose estimation moreover human pose estimation iruwb radar studied well existing works since existing works used frequencymodulated continuous wave fmcw radar wifi device 3dtranspose algorithm 3d human pose estimation iruwb radar signals algorithm designed transformer architecture transformer actively studied natural language processing nlp vision domains prior work applied transformer model rfbased human pose estimation problem attention mechanism algorithm able focus relevant time segments iruwb radar signals detecting human pose eliminating needs converting radar signals voxelized 3d image gathered large dataset iruwb radar signals labeled 3d human skeletons shown algorithm able detect human skeletons high accuracy © 2013', 'depth ambiguity one main challenges threedimensional 3d human pose estimation hpe recent strategies disambiguating brought significant progress remarkable breakthroughs field 3d human pose estimation 3d hpe survey extensively reviews causes solutions depth ambiguity solutions systematically classified four categories camera parameter constraints temporal consistency constraints kinematic constraints image cues constraints paper summarizes performance comparison challenges main evaluation metrics discusses promising future research directions © 2022 authors', 'athome exercising strongly predicts physical therapy patient outcomes underscoring need analyzing patient behaviors athome via remote patient monitoring contemporary remote patient monitoring rely specialized sensors ie inertial measurement units rgbdepth cameras motion capture systems stereo vision costly scalable physical therapy patients observe lack literature monocular rgb camera paper demonstrate skeletal feedback model athome exercises video acquired smartphone camera models patient performance evaluation classifies correctness exercises ii guidance identifies exercise went wrong patient correct models dataset four common physical therapy exercises labeled physical therapist results demonstrate feasibility skeletal data 3d human pose estimation models physical rehabilitation exercise evaluation guidance thus enable remote patient monitoring guidance single camera making highly costeffective scalable © 2023 acm', 'existing deep learningbased wireless sensing models usually require intensive computation paper introduce lightweight rfbased 3d human pose estimation model ie fast rfpose enable realtime human pose estimation specifically fast rfpose first estimates human locations rf heatmap crops human location regions estimates finegrained human poses cropped small rf heatmaps experiments build radio system multiview camera system acquire rf signals groundtruth human poses compare fast rfpose experimental results demonstrate fast rfpose outperforms alternative besides deploy trained fast rfpose model laptop cpu fast rfpose achieve 66 fps processing speed means meet realtime running requirements mobile devices © 2023', 'study deep endtoend representation learning 2d 3d monocular human pose estimation common yet challenging task computer vision however current still face problem recognized 3d key points inconsistent actual joint positions strategy trains 2d 3d networks 3d human poses corresponding 2d projections solve problem effective basis build cascaded monocular 3d human pose estimation network hierarchical supervision network composite residual module crm enhanced fusion module efm main components cascaded network crms stacked form cascaded modules compared traditional residual module crm expands information flow channels addition efm alternately placed cascaded modules addresses problems reduced accuracy low robustness caused multilevel cascade test network standard benchmark human36m dataset mpiinf3dhp dataset compare results fullysupervised six algorithms results weaklysupervised five algorithms mean per joint position error mpjpe millimeters evaluation index get best results © 2022 authors exclusive licence springer sciencebusiness media llc part springer nature', 'human pose estimation hpe way identify classify nodes human body essentially way determining coordinates node arm head torso etc called key point defining position human body relationship points called pair article analyzes existing approaches predicting human behavior compares article contains comparison description tools technologies implementation human behavior detection systems describes implementation system development module detect sign deviant behavior falling development application acts client application display notifications detection human falls manual testing implemented program © 2023', 'order solve problem smallscale key point positioning accuracy multiplayer pose estimation topdown advanced yolov4tiny paper improved multiperson pose estimation stacked hourglass deep learning network coordinate attention mechanism introduced original hourglass network residual module perform feature enhancement suppresses useless features improves useful features thus improving recognition rate smallscale joint points experiments index pck05 reaches 889 mpii dataset verifies effectiveness © 2023 spie', 'growth multimedia technology leveraged available multifaceted media data humanperceptive multiple media data fusion promoted research development rd artificial intelligence ai computer vision wide range applications like remote sensing image interpretation biomedicine depth estimation multimodality form representation things rot refers description things multiple perspectives early aioriented technology focused single modality data current humanperceptive researches clarified modality relatively independent description things idot complementary representations multimodal data tend threedimensional recent processing applications multimodal data intensively developed like sentiment analysis machine translation natural language processing biomedicine critical review focused development multimodality computervisionoriented multimodal learning mainly used analyze related multimodal data aspects images videos modalitiesranged learning complemented information image detection recognition semantic segmentation video action prediction etc multimodal data priority objects description first challenged collect largescale highquality multimodal datasets due equipmentlimited like multiple imaging devices sensors next image video data processing labeling timeconsuming laborintensive limiteddataderived multimodal learning multimodal data limited context computer vision segmented five aspects including fewshot learning lack strong supervised information active learning data denoising data augmentation multifeatures samples models evolution critically reviewed mentioned 1 case insufficient multimodal data fewshot learning cognitive ability make correct judgments via learning small number samples effectively learn target features case lack data 2 due high cost data labeling process challenged obtain ground truth labels modalities strongly supervised learning model incomplete supervised composed weakly supervised unsupervised semisupervised selfsupervised learning common optimize modal labeling information costeffective manual labeling 3 active learning integration prior knowledge learning regulatory via designing model autonomous learning ability committed maximum optimization samples labeling costs effectively reduced consistency optimized options samples 4 multimodal data denoising refers reducing data noise restoring original data extracting information interest 5 order make full limited multimodal data fewsamplesconditioned data enhancement extends realistic data performing series transformation operations original data set addition data sets used multimodal learning limited data potential applications introduced like human pose estimation person reidentification performance existing algorithms compared analyzed pros cons well future development direction projected following 1 lightweight multimodal data processing argue limiteddataconditioned multimodal learning still challenge mobiledevicesoriented models applications existing fuse information multiple modalities generally necessary two networks feature extraction fuse features therefore large number parameters complex structure model limit application mobile devices future lightweight model potentials 2 commonlyused multimodal intelligent processing model existing multimodal data processing derived developed multialgorithms multitasks need trained specific tasks tailored training greatly increases cost developing models making difficult meet needs application scenarios therefore data different modalities necessary promote consensus perception model learn general representation multimodal data parameters features general model shared multiple scenarios 3 multisources knowledge data driven model possible introduce featured data knowledge multimodal data beyond establish integrated knowledgedatadriven model enhance model ’ performance interpretability © 2022 journal image graphics rights reserved', 'multiview human pose estimation achieve high accuracy leveraging complex spatial information multiple perspectives however increasing number views strain network model potentially compromising estimation accuracy limited computing resources furthermore current resnet feature extraction traditional involve deconvolution obtain largesized feature maps introduce artificial interference tackle challenges perceptual network flexible combination view feature fusion network comprised three crucial modules flexible view combination policy module enables high accuracy single reference view avoids problem increased complexity caused large number views upsampling module subpixel convolution designed achieve efficient highresolution recovery resolves issue artificial interference introduced deconvolution additionally feature fusion module maximizes utilization reference view cues enhance human pose estimation current view experiments conducted human36m dataset demonstrate reduction average mpjpe 183 mm model © 2023 authors exclusive licence springerverlag gmbh germany part springer nature', 'track body movement patients movement disorders sensors kinect cameras easily accessible recentlydeveloped deep learning models subset artificial intelligence ai analyze patients behavior rgb images smartphones stacked hourglass model novel pose estimation deep learning model accurately determine location body joints long shortterm memory network lstm determine corresponding action analyzing kinematic behavior body joints study develops deep learning model rgb images utkinect dataset input determines action performed 8414 accuracy specifically contributions developed preprocessing pipeline stack hourglass model utkinect dataset ii finetuning model handle 20 joints iii added human action recognition component accurately classify actions performed efficient replacement hardlyaccessible kinect cameras used analyze various diseases movement disorders © 2023', '180 topics discussed intelligent classification identification radar jamming signals research parking lot recommendation system collaborative filtering towards building longrange relationship superresolution design analysis oamdm underwater wireless optical communication system government enterprise data security sharing differential privacy protection research information security architecture new generation marketing system design application rhythmic gymnastics auxiliary training system kinect stacked hourglass deep learning networks attention mechanism multiperson pose estimation protection dynamic heterogeneous redundancy architecture research advanced human pose estimation deep learning application analysis machine learning cyberspace security research', 'topdown pose estimation generally employs person detector estimates keypoints detected person assumes single person exists within bounding box cropped detection however assumption leads challenges practice first loosefitted bounding box may certain body parts nontarget person second spatial interference several people exists owing occlusion single person exist cropped image scenarios pose estimation may falsely predict keypoints two persons single person tackle issues paper human bodyaware feature extractor global localreasoning features globalreasoning feature considers entire body transformers nonlocal computation property localreasoning feature concentrates individual body parts convolutional neural networks two features extract corrected features filtering unnecessary features supplementing necessary features novel architecture hence focus target persons keypoints thereby mitigating aforementioned concerns achieves noticeable improvement applied topdown pose estimation networks © 2022', 'heatmap regression become mainstream methodology deep learningbased semantic landmark localization including facial landmark localization human pose estimation though heatmap regression robust large variations pose illumination occlusion unconstrained settings usually suffers subpixel localization problem specifically considering activation point indices heatmaps always integers quantization error thus appears heatmaps representation numerical coordinates previous overcome subpixel localization problem usually rely highresolution heatmaps result always tradeoff achieving localization accuracy computational cost computational complexity heatmap regression depends heatmap resolution quadratic manner paper formally analyze quantization error vanilla heatmap regression simple yet effective quantization system address subpixel localization problem quantization system induced randomized rounding operation 1 encodes fractional part numerical coordinates ground truth heatmap probabilistic training 2 decodes predicted numerical coordinates set activation points testing prove quantization system heatmap regression unbiased lossless experimental results popular facial landmark localization datasets wflw 300w cofw aflw human pose estimation datasets mpii coco demonstrate effectiveness efficient accurate semantic landmark localization code available httpgithubcombaoshengyuh3r © 19792012', 'human motion analysis fundamental task computer vision increasing demand versatile datasets development deep learning however obtain annotations human motion 3d keypoints smpl parameters requires research work design multiview human motion capture system develop toolchain generate multimodal motion annotations additionally contribute humomm largescale multimodal dataset following characteristics 1 multiple modalities including two data formats ie rgb depth images four annotation formats ie action categories 2d keypoints 3d keypoints smpl parameters 2 largescale 18 subjects 30 actions 35k sequences 262k frames 3 multitask action recognition 2d keypoint detection 3d pose estimation human mesh recovery furthermore provide benchmark humomm test performance popular several related tasks experimental results demonstrate humomm holds significant research value expect humomm contribute human motionrelated research available httpsgithubcomscutbiplabhumomm © authors exclusive license springer nature switzerland ag 2023', '3d human pose estimation widely used motion capture humancomputer interaction virtual character driving fields current 3d human pose estimation suffering depth blurring selfobscuring problems solved paper human pose estimation network video 2d lifting 3d transformer graph convolutional networkgcn widely used natural language processing transformer obtain sequence features graph convolution extract features local joints get accurate 3d pose coordinates addition 3d pose estimation network animated character motion generation robot motion following design two systems humancomputerrobot interaction hcihri applications 3d human pose estimation network tested human36m dataset outperforms models hcihri systems designed work quickly accurately 3d human pose estimation © authors exclusive license springer nature singapore pte ltd 2023', 'motion perception pivotal myriad specialized tasks necessitates enhancement proficiency sustained repetition perceptionaction cycles meet standard benchmarks attaining advanced skill levels often demands additional practice however traditional pedagogical evaluation systems predominantly hinge subjective experiences instructors evaluators dependence precipitates two principal challenges domain motion perceptionrelated professional work firstly learners grapple securing timely adequate guidance learning practice given slow trialanderror nature key point acquisition secondly objective evaluation fraught instability fails consistently deliver accurate quantitative assessments thereby adversely impacting learning process response challenges study introduces deep learningbased standardized evaluation human pose estimation methodology begins utilization openpose body joint detection followed deep neural network dnninformed strategy posture information extraction lastly leveraging teams extensive experience dance instruction novel describing discerning differences dance movements enables quantitative evaluation provides intuitive feedback mechanics dance movements thereby enhancing monitoring participants progress validated experimentally methodology demonstrates precision motion perception quantitative evaluation offers practical guidance enhancing quality dance instruction also provides valuable reference applications © 2023 lavoisier rights reserved', 'human pose estimation hpe developed past decade vibrant field research variety realworld applications like 3d reconstruction virtual testing reidentification person information human poses also critical component many downstream tasks activity recognition movement tracking review focuses key aspects deep learning development 2d 3d hpe provides detailed information variety databases performance metrics human body models incorporated implementing hpe methodologies paper discusses variety applications hpe across domains like activity recognition animation gaming virtual reality video tracking etc paper presents analytical study major works deep learning various downstream tasks domain 2d 3d hpe finally discusses issues limitations current topic hpe recommend potential future research directions order make meaningful progress area © 2022 authors exclusive licence springerverlag london ltd part springer nature', 'objective point cloudbased 3d human pose estimation one key aspects computer vision wide range applications developing augmented reality virtual reality ar vr humancomputer interaction hci motion retargeting virtual avatar manipulation current deep learningbased 3d human pose estimation challenging following aspects 1 3d human pose estimation task constrained occlusion selfocclusion ambiguity moreover noisy point clouds depth cameras may cause difficulties learn proper human pose estimation model 2 current depthimage mainly focused single imagederived pose estimation may ignore intrinsic priors human motion smoothness leads jittery artifacts results consistent point cloud sequences potential leverage point cloud sequences highfidelity human pose estimation via human motion smoothness enforcement however challenging design effective way get human poses modeling point cloud sequences 3 hard collect largescale real image dataset highquality 3d human pose annotations fullysupervised training easy collect real dataset 2d human pose annotations moreover human pose estimation closely related motion prediction aims predict future motion available challenging issue whether 3d human poses estimation motion prediction realize mutual benefit develop obtain high fidelity 3d human pose point cloud sequence weaklysupervised deep learning architecture used learn 3d human pose 3d point cloud sequences design duallevel human pose estimation pipeline point cloud sequences input 1 2d pose information estimated depth maps background removed poseaware point clouds extracted ensure normalized sequential point clouds scale point clouds normalization carried fixed bounding box point clouds 2 pose encoding implemented via hierarchical pointnet backbone long shortterm memory lstm layers spatialtemporal features poseaware point cloud sequences improve optimization effect multitask network employed jointly resolve human pose estimation motion prediction problem order training data 2d human pose annotations release ambiguity supervision 2d joints weaklysupervised learning adopted result order validate performance algorithm several experiments conducted two public datasets including invarianttop view dataset itop nturgbd dataset performance compared popular including v2vposenet viewpoint invariant vi inference embedded weakly supervised adversarial learning wsm itop dataset mean average precision map value 0 99 point higher wsm given threshold 10 cm compared vi inference embedded map value 13 18 17 96 higher mean joint errors 3 33 cm 5 17 cm 1 67 cm 0 67 cm lower vi inference embedded v2vposenet wsm respectively performance gain could originated sequential input data constraints motion parameters like velocity accelerated velocity 1 sequential data encoded lstm units could get smoother prediction improve estimation performance 2 motion parameters alleviate jitters caused random sampling yield direct supervision joint coordinates nturgbd dataset compare current wsm map value 7 03 percentage points higher wsm threshold set 10 cm time ablation experiments carried itop dataset investigate effect multiple components understand effect input sequential point clouds design experiment different temporal receptive field sequential point clouds receptive field set 1 estimated results sequential data excluded percentage correct keypoints pck result drops lowest value 88 57 receptive field set 1 pck values increased receptive field increases 1 5 pck value becomes steadily receptive field greater 13 pck value 87 55 trained fully labeled data pck value model trained fully weakly labeled data 90 58 shows weakly supervised learning improve performance model 2 point percentage experiments demonstrate weakly supervised learning used small amount fully labeled data well compared model trained single task map human pose estimation motion prediction multi task network improved 2 percentage points conclusion obtain smoother human pose estimation results make full prior human motion continuity experiments demonstrate contributed components effective achieve performance efficiently itop dataset nturgbd dataset joint training strategy valid mutual tasks human pose estimation motion prediction weakly supervised sequential data easytoaccess training data model robust different levels training data annotations could applied scenarios require highquality human poses like motion retargeting virtual fitting related potentials sequential data input © 2022 journal image graphics rights reserved', '191 topics discussed research intelligent monitoring external damage around transmission line millimeter wave radar camera radar camera fusion target detection attention enhancement research hardware synchronization control system 3d panorama construction fusion white light infrared image registration nsct 3d human pose estimation video temporal spatial transformer design development environmental pollution source monitoring management information system oct image denoising bayesian nonlocal mean filter deep learning network extracting function relation less ideal digital curve new eyeball optical axis reconstruction headmounted eyetracking system compressing gait silhouettes graphical edge cascade attentional fusion unsupervised domain adaptation multimodal egocentric video analysis', 'human motion analysis seen drastic improvements recently however due lack representative datasets clinical inbed scenarios still lagging behind address issue implemented blanketgen pipeline augments videos synthetic blanket occlusions pipeline generated augmented version pose estimation dataset 3dpw called blanketgen3dpw used new dataset finetune deep learning model improve performance scenarios promising results code information available httpsgitlabinesctecptbrainlabbrainlabpublicblanketgenreleases © 2023', 'human pose estimation significant task computer vision last decades wide variety applications scientific field people ’ daily life people usually cameras obtain rgb images videos compose integrated dataset different deep learning combinations try estimate proper posture human body although scientists thought various kinds technical ways estimate human pose still achieve 100 percent accuracy unavoidable factors example complex environmental changes everyday life flexible body variety body shapes make prediction accuracy difficult affects confirmation keypoints limitations better research review focus advanced technical datasets metrics 3d 2d human pose estimation single multiple individuals © 2023 spie', 'human pose estimation detecting classifying key human body joints 2d human pose estimation predicts twodimensional coordinates input image classical used deformable parts models detecting body joints paper discusses deep learning approaches estimating human poses implemented either topdown bottomup technique pose estimation authors studied convolutional pose machines hourglass network movenet architectures pose estimation architectures compared terms efficiency measuring time taken accuracy evaluating percentage detected joints metric mpii dataset architectures implemented tensorflow opensource libraries built pytorch inference results joint compared individually body joints hourglass network showed best accuracy really slow movenet architecture second best according accuracy fast used realtime systems © 2022 acm', 'common technique understanding human behavior action recognition videos videos provide significantly information imagebased action recognition reducing action ambiguity well several datasetfocused studies innovative models learning strategies past ten years elevated video action identification higher level however difficulties unresolved issues particularly realtime cctv analytics data gathering labeling complex necessitating annotation data additionally actions could happen quickly making challenging distinguish paper presented video multiple human activity recognition realtime cctv camera videos har introduce enhanced timesformer model multilayer perceptron etmlp neural networks classification algorithm applies selfattention patches human regions interacting uniform classification tokens manner enhancing contextualized human activity data visual human pose estimation areas able estimate human pose © 2022 ismail saritas rights reserved', 'human pose estimation field computer vision science studies determination joint points human body images videos one applications human pose estimation evaluate human movement performance study 3d markerless human pose estimation carried direct linear transform deep learning blazepose system testing carried pushup movement comparing data results markerless system markerbased motion capture system pushups excellent exercises developing upper body strength endurance arms shoulders pushups widely used rehabilitation recovery surgical procedures american college sports medicine acsm established standard assessing persons physical endurance number successful pushups quantitatively mean absolute errors calculated 709 30 mm measuring joint angles elbows hips knees 43 5 degrees error value 30 mm indicates system used human movement analysis © 2023', '44 topics discussed visionbased containercode checking system case study international terminal robust control strategy robotic manipulators finitetime stability performance improvement pure pursuit algorithm pid control high precision tracking control modelfollowing control equivalentinputdisturbance yolov5 dual attention network object detection drone two solutions mitigating blurred output autoencoder knowledge distillation human pose estimation channel dropout strategy recalling multiple object manipulation candidates learning observation analysis capability deep learning algorithms eegbased braincomputer interface implementation devsnet dense efficient vehicle state classification drone traffic dataset', 'one key symptoms stroke hemiparesis weakness loss movement one sides body early detection strokerelated weakness improve patient outcomes minimize risk longterm disability study presents novel deep learning efficiently objectively detecting quantifying strokerelated weakness addresses limitations current technologies invasive timeconsuming study describes model processes video data classify left rightsided strokeinduced motor weakness model extracts movement key human body joints applies time series analysis classify strokeaffected side body model developed data collected 23 poststroke patients clinical setting 48 hours © 2023', 'sudden change workplace practices facetoface work work home setup due pandemic brought positive negative impacts overall health literature deep learning specialized cameras estimation human pose popular even need high computational resources complex models purpose study developed intelligent interactive system utilizing human estimation model distinct keypoint thoracic thoraco lumbar lumbar points spine objective type dataset captured work home environment knowledge guidance licensed physical therapists assess proper improper sitting posture developed study developed implemented smallscale convolutional network lowcost smartphone camera recognize body key points feature points locations extracted additional features cosine similarity point distances calculated next feature selection optimization utilized classify proper improper sitting postures result study developed 2 datasets 2 models accuracy 8518 9207 kappa 0691 0838 respectively © 2023 jheanel estrada larry vea openaccess article distributed creative commons attribution ccby 40 license', '2d human pose estimation given images activate research area computer vision existing deep learning rely highresolution input always available many scenarios address issues novel algorithm called superresolved pose estimationsrpose paper composed superresolution subnetworksrn following human pose estimation subnetworkhpen srn equipped global residual learning positionpreserving block constructs hr version lr input hpen perform pose estimation whole srpose optimized unified loss endtoentoen comprehensive experiments public benchmarks verify effectiveness generalization srpose condition lr input © 2023 authors exclusive license springer nature singapore pte ltd', 'present deep learning networks field human pose estimation improve prediction accuracy often accompany improvement network structure complexity brings improvement network model parameters computational complexity making difficult deploy devices small computing power practical application basis hrnet paper devises lightweight human pose estimation network sagnet integrates selfattention mechanism ghost module introduces selfattention module get higher prediction accuracy transition process third fourth stage hrnet replaces standard convolution hrnet ghost module network lightweight experimental results show experimental environment coco 2017 validation set sagnet greatly reduces amount parameters complexity computation compared hrnet maintaining prediction accuracy similar level © 2023', 'emergency medicine cardiopulmonary resuscitation cpr usually necessary urgent first aid measure patients whose heart stops beating due diseases accidents however success cpr saving patients depends popularity cpr education training popular education training people become rescuers emergency situations perform cpr patients improve survival rate work implement cpr quality evaluation system four modules pose detection module compression frequency module compression depth module visualized feedback module respectively addition build visualized realtime feedback system present overall information chest compression learners including whether compression pose correct frequency compression depth compression position compression © 2023', 'certain inevitable challenges human pose estimation tasks deep learning large amount network parameters high computational complexity paper lightweight network reduce scale model parameters computational complexity meanwhile improve accuracy human pose estimation task new takes highresolution networks hrnet32 basic replaces basic module mbconv lightweight module attention mechanism incorporated network model context information improve perception ability feature extraction ability module improve accuracy human pose estimation experimental results coco2017 show network detect human key points high precision even amount parameters reduced 56 verifies good lightweight performance © copyright spie downloading abstract permitted personal', '3d human pose shape estimation 3dhpse video aims generate sequence 3d mesh depict human body video current deep learning 3dhpse networks takes video input focused improving temporal consistency among sequence 3d joints supervising acceleration error predicted groundtruth human motion however overlooked geometric misalignments persistent discrepancy geometric paths drawn sequence predicted joints groundtruth joints end joint path alignment jpa modelagnostic mitigates geometric misalignments introducing temporal procrustes alignment regularization tpar loss performs groupwise sequence learning joint movement paths unlike previous rely solely perframe supervision accuracy adds sequencelevel accuracy supervision tpar loss performing procrustes analysis geometric paths drawn sequences predicted joints experiments show jpa advances network exceed previous performances benchmark datasets perframe accuracy video smoothness metric © 2013', 'challenging problem robotic interaction augmented reality estimation tracking human poses images videos pose estimation deep neural networks shown encouraging results recent approaches environmental sensitivity computational complexity conventional pose estimation major drawbacks light issues paper novel densenet cnnbased transfer learning learn explicitly exploiting skeletal data imagenet pretrained models along probabilistic regression losses used comparative study widely accepted benchmark pose estimation dataset flic frames labelled cinema serves basis evaluation comparison result experiments r2 score 0948 recommend probabilistic loss regression loss new baseline future downstream tasks finetuningbased transfer learning techniques pose estimation © 2023 authors exclusive licence springerverlag gmbh germany part springer nature', 'human pose estimation important research topic computer vision attracts researchers recently vitpose heatmap representation refreshed state art pose estimation however find vitpose still room improvement experiments one hand patchembedding module vitpose convolutional layer stride 14 × 14 downsample input image resulting loss significant amount feature information hand two decoding classical decoder simple decoder used vitpose refined enough transpose convolution classical decoder produces inherent chessboard effect upsampling factor simple decoder large resulting blurry heatmap end novel pose estimation vitpose termed refinepose refinepose design gradualembedding module fusion decoder respectively solve problems specifically gradualembedding module downsamples image 12 original size downsampling stage reduces input image fixed size 16 × 112 vitpose multiple downsampling stages time fuse outputs max pooling layers convolutional layers downsampling stage retains meaningful feature information decoding stage fusion decoder designed us combines bilinear interpolation max unpooling layers gradually upsamples feature maps restore predicted heatmap addition also design featureaggregation module aggregate features sampling upsampling downsampling validate refinepose coco dataset experiments show refinepose achieved better performance vitpose © 2022 authors', 'weightlifting demanding sport requiring power flexibility correct technique snatch clean jerk involve fast lifting weight overhead position incorrect technique posture may lead inefficient lifts even injury paper presents new biomechanics analysis barbell trajectory tracking weightlifting leveraging capabilities keypointrcnn yolov7 deep learning models extracts skeletal information weightlifting video sequences pretrained keypointrcnn model human pose estimation custom yolov7 model detect track barbell trajectories keypointrcnn model estimates human pose without manual annotation specialised apparatus yolov7 model provides realtime nonintrusive barbell tracking efficacy barbell trajectory tracking yolov7 public weightlifting dataset 973 images 7030 traintest ratio evaluated obtaining high precision 09214 recall 09678 map05 09792 map05095 07765 indicating applicability model weight training applications presents costeffective userfriendly easily accessible alternative conventional motion capture analysis systems making accessible lifters skill levels training environments © 2023', 'estimation human pose monocular camera emerging research topic computer vision community many applications recently benefiting deep learning technologies significant amount research efforts advanced monocular human pose estimation 2d 3d areas although works summarize different approaches still remains challenging researchers indepth view approaches work 2d 3d article provide comprehensive holistic 2dto3d perspective tackle problem first comprehensively summarize 2d 3d representations human body summarize mainstream milestone approaches human body presentations since year 2014 unified especially provide insightful analyses intrinsic connections evolution 2d 3d pose estimation furthermore analyze solutions challenging cases lack data inherent ambiguity 2d 3d complex multiperson scenarios next summarize benchmarks evaluation metrics quantitative performance popular approaches finally discuss challenges give deep thinking promising directions future research believe survey provide readers researchers engineers developers etc deep insightful understanding monocular human pose estimation © 2022 association computing machinery', 'unipose unified 2d 3d human pose estimation images videos unipose architecture leverages multiscale feature representations increase effectiveness backbone feature extractors significant increase network size postprocessing current pose estimation heavily rely statistical postprocessing predefined anchor poses joint localization unipose incorporates contextual information across scales joint localization gaussian heatmap modulation decoder output estimate 2d 3d human pose single stage accuracy without relying predefined anchor poses multiscale representations allowed waterfall module unipose leverage efficiency progressive filtering cascade architecture maintaining multiscale fieldsofview comparable spatial pyramid configurations results multiple datasets demonstrate unipose hrnet resnet senet backbone waterfall module robust efficient architecture single person 2d 3d pose estimation single images videos © 19792012', 'article twobranch learning model namely joint globallocal network human pose estimation hpe millimeter wave radar aim work remediate illposed problems hpe arising destructive observations superimposed reflection signals developed twobranch learning model global branch takes superimposed signals whole human body reconstruct coarse pose estimation global perspective local branch responsible fining pose estimations decomposed signals individual body parts complementary way two branch learning processes coordinated followed attentionbased fusion module terms local global consistency remarkable learning driven decomposed signals motivated exploiting spatialtemporal evolution patterns individual body parts inferring corresponding movements plays crucial yet complementary role collaboration learning driven superimposed signals twobranch learning architecture advantageous incorporating local motion constraints individual body parts coarse global estimation whole human body contributes reconstructing plausible yet accurate pose estimations local global kinematic consistency extensive experiments presented demonstrate effectiveness © 2014', 'radarbased human pose estimation faces challenge anatomically incorrect pose estimation within frames unstable pose estimation results across multiple frames primarily due sensitivity radar signals radial component motion overcome twostream model incorporates spatiotemporal constraints learning network spatial stream network captures physical connections among joints ensuring anatomical consistency within frames meanwhile temporal stream network focuses learning actionspecific synergies motion capturing temporal dependencies among joints fusion two streams enables accurate stable 3d skeleton estimation experiments demonstrate effectiveness © authors exclusive license springer nature singapore pte ltd 2023', 'monocular 3d human pose estimation used calculate 3d human pose monocular images videos still faces challenges due lack depth information traditional tried disambiguate building pose dictionary temporal information slow realtime application paper realtime named g2opose high running speed without affecting accuracy much work regard 3d human pose graph solve problem general graph optimization g2o multiple constraints constraints implemented algorithms including 3d bone proportion recovery human orientation classification reverse joint correction suppression depth human body change much outperforms previous nondeep learning terms running speed slight decrease accuracy © 2022 authors', 'improve generalization 3d human pose estimators many existing deep learning models focus adding different augmentations training poses however data augmentation techniques limited seenpose combinations hard infer poses rare unseenjoint positions address problem present camerapose weaklysupervised 3d human pose estimation single image applied 2d3d pose pairs also 2d alone annotations adding camera parameter branch inthewild 2d annotations fed pipeline boost training diversity 3d poses implicitly learned reprojecting back 2d moreover camerapose introduces refinement network module confidenceguided loss improve quality noisy 2d keypoints extracted 2d pose estimators experimental results demonstrate camerapose brings clear improvements crossscenario datasets notably outperforms baseline 3mm challenging dataset 3dpw addition combining refinement network module existing 3d pose estimators performance improved crossscenario evaluation © 2023', '3d keypoint detection lying human body important improve efficiency mobile rescue robots casualty collection point disaster eg earthquake mudslide paper efficient 3d human keypoint detection lying posture rgbd camera first current 2d human pose estimation algorithm rgb images obtain 2d keypoints whole body obtain final 3d coordinates human keypoint processing 2d coordinates filter design combining depth information coordinate transformation experiments show accurate fast enough used 3d keypoint detection lying human body mobile rescue robot casualty collection point © 2023 technical committee control theory chinese association automation', 'human pose estimation hpe one trending areas research among artificial intelligent research gained lot attention due versatile potential applications various domains including transportation healthcare gaming augmented reality sports hpe used build sports analytics personalized training selflearning systems allow players athletes trainers improve training quality evaluating various human poses detected images videos far know none exciting works considered developing pose estimation classification bowling players therefore paper deeplearning bowling players pose estimation classification bowling deeplearning bowlingdl model along movenet model bowling players pose estimation classification movenet model detects various key points human pose bowlingdl model classifies detected bowling players poses five different classes model training evaluation collected labelled dataset dataset found bowling posture model achieved 80 accuracy training dataset 83 accuracy testing dataset addition smart mobile application bowling players developed edgefriendly version bowlingdlgenerated tensorflow litewas deployed © 2023', 'yoga change way see things transforms person seesthis statement highlights suggests greatness practice seen hobby yoga paper detect correct yoga postures deep learning techniques involves dataset various yoga asanas train model detect abnormalities desired pose users real time pose hope provide useful tool individuals want learn practice yoga home without need expensive trainers yoga centers ai trainer provide interactive platform beginners learn fundamentals yoga asanas well provide recommendation system users postures help movenet human pose estimation done fast easily deployed considering keeping inference times low possible outcome model implement achieve accurate key points estimating wide variety gestures movements environments setup hardware model giving accuracy 09929 prepared application built react javascript pose estimation model integrated application help user easily maneuver interact pose estimation model © 2023', 'residential roofers often exposed awkward postures motions prolonged time may reduce body stability increase fall potential also increase risk musculoskeletal disorders msds assess risks fatal musculoskeletal injuries crucial capture 3d body poses workers roofing tasks paper novel twostage motion estimation convolution neural network estimate residential roofer ’ body poses threeview video data includes two stages 1 offline multiview model estimate 3d pose single frame 2 multiframe model apply temporal convolutions refine multiview outputs performance evaluated comparing estimation goldstandard markerbased 3d human pose one common residential roofing tasks–shingle installation evaluation results show multiframe model effectively improve accuracy coordinate sequence moreover results prove videobased motion estimation efficiently accurately locate 3d body joints pave way future onsite motion analysis roofing activities © work authored part contributor ’ official duties employee united states government therefore work united states government accordance 17 usc 105 copyright protection available works us law', 'human pose estimation hpe procedure determining structure body pose considered challenging issue computer vision cv communities hpe finds applications several fields namely activity recognition humancomputer interface despite benefits hpe still challenging process due variations visual appearances lighting occlusions dimensionality etc resolve issues paper presents squirrel search optimization deep convolutional neural network hpe ssdcnnhpe technique major intention ssdcnnhpe technique identify human pose accurately efficiently primarily video frame conversion process performed preprocessing takes place via bilateral filteringbased noise removal process efficientnet model applied identify body points person problem constraints besides hyperparameter tuning efficientnet model takes place squirrel search algorithm ssa final stage multiclass support vector machine msvm technique utilized identification classification human poses design bilateral filtering followed ssa efficientnetmodel hpe depicts novelty work demonstrate enhanced outcomes ssdcnnhpe series simulations executed experimental results reported betterment ssdcnnhpe system recent existing techniques terms different measures © 2023 tech science press rights reserved', 'face blurring images plays key role protecting privacy however computer vision especially human pose estimation task machinelearning models currently trained validated tested original datasets without face blurring additionally accuracy human pose estimation great importance kinematic analysis analysis relevant areas occupational safety clinical gait analysis privacy crucial therefore study explore impact face blurring human pose estimation subsequent kinematic analysis firstly blurred subjects ’ heads image dataset trained neural networks faceblurred original unblurred dataset subsequently performances different models terms landmark localization joint angles estimated blurred unblurred testing data finally examined statistical significance effect face blurring kinematic analysis along strength effect results reveal strength effect face blurring low within acceptable limits 1° thus shown human pose estimation face blurring guarantees subject privacy degrading prediction performance deep learning model © 2022 authors', 'paper present rfbased multiview pose machine rfmvp multiperson 3d pose estimation rf signals specifically first develop lightweight anchorfree detector module locate crop regions interest horizontal vertical rf signals afterward multiview fusion network unproject rf signals horizontal vertical millimeterwave radars unified latent space calculate correlation weighted fusion finally spatiotemporal attention network designed reconstruct multiperson 3d skeleton sequences spatial attention module recover invisible body parts nonlocal correlations among joints temporal attention module refines 3d pose sequences temporal coherency learned frame queries evaluate performance rfmvp largescale dataset multiperson 3d pose labels corresponding radar signals experimental results show rfmvp outperforms baseline locates multiperson 3d key points average error 73mm generalizes well new data occlusion low illumination © 2023', '180 special focus conference future technologies topics pretrained cnn svm classifier weld joint type recognition twostage federated transfer learning medical images classification limited data covid19 case study graph emotion distribution learning emotiongcn role depth predictions 3d human pose estimation aibased qosqoe multimedia systems snatch theft detection deep learning models deep learning fewshot learning detection skin cancer overview enhancing artificial intelligence control mechanisms current practices real life applications future views face generation skull photo gan 3d face models general particle swarm optimization artificial intelligence videogames drive forward bezier curvebased shape knowledge acquisition fusion surrogate model construction path planning landing unmanned aerial vehicles ai digital ticketing system public transport mexico avoid cases contagion artificial intelligence question practical implementation “ digital immortality ” technologies new approaches creation ai collaborative forecasting “ sliderswarms ” improves probabilistic accuracy learning solve sequential planning problems without rewards systemic analysis democracies concept humantechnological development regression algorithms artificial intelligence predict price bitcoin exploring deep learning road traffic accident recognition roadside sensing technologies integration humandriven autonomous vehicle cell reservation intersection control strategy equivalence classical epidemic model quantum tightbinding model effects various barricades human crowd movement flow classical logic continuous logic editor ’ preface', 'human pose estimation basic challenging task field computer vision basis many computer vision tasks action recognition action detection development deep learning deep learningbased human pose estimation algorithms shown excellent results study divides pose estimation three categories including single person pose estimation topdown multiperson pose estimation bottomup multiperson pose estimation development 2d human pose estimation algorithms recent years introduced current challenges twodimensional human pose estimation discussed finally outlook future development human pose estimation given © 2022 chinese academy sciences rights reserved', 'goal human pose estimation locate human body parts create human body representation eg body skeleton input data images videos received lot attention last decade used variety applications like humancomputer interface motion analysis augmented reality virtual reality even though newly developed deep learningbased algorithms achieved great performance human pose estimation insufficient training data depth ambiguities occlusion remain problems another challenge human pose estimation implementing evaluations workouts physical treatment evaluation aids determining appropriate correct ways undertake physical workouts leverage human pose estimation neural network identify human joints give users instructions exercise properly study analyze bicep curls measuring elbow flexion angle identifying key points shoulder elbow hand thereby compare standard angle determine user reached correct amplitude exercise evaluation essential user bicep curls exercise problem two different solutions openpose open source mediapipe open source presented testing coco dataset dataset results show mediapipe provides better results bicep curls workout evaluation future mediapipe used developing new application software mobile phone support humans training © 2023 authors exclusive license springer nature switzerland ag', 'computer vision human pose estimation details posture persons body structure kinematic planer volumetric image video however pose detection often critical driven distinct human actions thus survey report analysis recent progression bottomup topdown human pose evaluation models survey report focuses 2d 3d skeletonbased human pose detection captured red green bluergb images condensed performance recent pose recognition tracking detection techniques utilize pose estimation colour images captured exhibit room much refinement domain paper scrutinize study human pose estimation models like 2d 3d hpe identify human movements running dancing sport recent computer visionbased advances study included various detecting two three dimensions paper summarises deep learning models hpe dataset challenges © 2023', 'purpose 3d human pose estimation predict information 3d coordinate position angle human joint points construct human representations human bones analysis human posture continuous advancement deep learning highperformance 3d human pose estimation deep learning however due human occlusion picture large demand training scale still challenges 3d human pose estimation research purpose paper review number research recent years analyze compare reasoning process core elements comprehensively elaborate 3d human pose estimation deep learning recent years addition paper also introduces relevant data sets evaluation indicators compares experimental data models human36m dataset campus dataset shelf dataset analyzes compares experimental results finally according results survey difficulties challenges faced current 3d human pose estimation discussed future development 3d human pose estimation discussed © 2023 journal computer engineering applications beijing co ltd science press rights reserved', 'paper deep learning applied study estimation human joint points complex multiperson videosalthough many excellent research results human pose estimation single frame images combination multiple frames continuous images video contains complex temporal information human pose estimation videos challengingin order make temporal spatial continuity video sequence temporal attention spatial attention used paperto better consider amount information frame aggregate information across multiple features temporal attention fusion adopted calculating element correlation features current frame adjacent frames order take advantage spatial relationship within frame spatial attention used achieve efficient fusionthrough experimental evaluation paper recover accurate human pose skeleton high accuracy © 2023 spie', 'human pose estimation hpe deep learningbased software applications trend topic markerless motion analysis thanks accuracy technology hpe could enable gait analysis telemedicine practice hand delivering service distance requires system satisfy multiple different constraints like accuracy portability realtime privacy compliance time existing solutions either guarantee accuracy realtime eg widespread openpose software wellequipped computing platforms portability data privacy eg light convolutional neural networks mobile phones portable lowcost platform implements realtime accurate 3d hpe embedded software lowpower offtheshelf computing device guarantees privacy default design present extended evaluation accuracy performance solution conducted markerbased motion capture system ie vicon ground truth results show platform achieves realtime performance highaccuracy deviation error tolerance compared markerbased motion capture system eg less error 5∘ estimated knee flexion difference entire gait cycle correlation 091ρ099 provide proofofconcept study showing portable technology considering limited discrepancies respect markerbased motion capture system working tolerance could used gait analysis distance without leading different clinical interpretation © 2022 elsevier bv', '180 special focus conference future technologies topics pretrained cnn svm classifier weld joint type recognition twostage federated transfer learning medical images classification limited data covid19 case study graph emotion distribution learning emotiongcn role depth predictions 3d human pose estimation aibased qosqoe multimedia systems snatch theft detection deep learning models deep learning fewshot learning detection skin cancer overview enhancing artificial intelligence control mechanisms current practices real life applications future views face generation skull photo gan 3d face models general particle swarm optimization artificial intelligence videogames drive forward bezier curvebased shape knowledge acquisition fusion surrogate model construction path planning landing unmanned aerial vehicles ai digital ticketing system public transport mexico avoid cases contagion artificial intelligence question practical implementation “ digital immortality ” technologies new approaches creation ai collaborative forecasting “ sliderswarms ” improves probabilistic accuracy learning solve sequential planning problems without rewards systemic analysis democracies concept humantechnological development regression algorithms artificial intelligence predict price bitcoin exploring deep learning road traffic accident recognition roadside sensing technologies integration humandriven autonomous vehicle cell reservation intersection control strategy equivalence classical epidemic model quantum tightbinding model effects various barricades human crowd movement flow classical logic continuous logic editor ’ preface', 'automatically estimating 3d human poses video inferring meanings play essential role many humancentered automation systems existing researches made remarkable progresses first estimating 2d human joints video reconstructing 3d human pose 2d joints however monodirectionally reconstructing 3d pose 2d joints ignores interaction information 3d space 2d space losses rich information original video therefore limits ceiling estimation accuracy end paper bidirectional 2d3d transformation bidirectionally exchanges 2d 3d information utilizes video information estimate offset refining 3d human pose addition bonelength stability loss utilized purpose exploring human body structure make estimated 3d pose natural increase overall accuracy evaluation estimation error measured mean per joint position error mpjpe 465 mm much lower experimental condition improvement accuracy make machines better understand human poses building superior humancentered automation systems italicnote practitionersitalicx2014this paper motivated demand humancentered automation systems needing accurately understand human poses existing approaches mainly focus inferring 3d human pose 2d joints monodirectionally although made remarkable contributions estimating 3d human pose monodirectional way found ignore 2d3d interaction original video inferring 3d pose 2d joints paper therefore suggests bidirectional 2d3d transformation exchanges 2d 3d information utilizes video information estimate accurate 3d human pose humancentered automation systems work pioneering attempt interactively 2d 3d information accurate estimation human pose benefited accuracy expected make significant contributions many humancentered automation systems humanmachine interaction biomimetic manipulation automatic surveillance systems', 'basic technology human action recognition pose estimation attracting researchers attention edge application scenarios pose higher challenge authors lightweight multiperson pose estimation scheme meet needs realtime human action recognition edge end scheme alphapose extract human skeleton nodes adds resnet dense upsampling revolution improve accuracy meanwhile yolo used enhance alphapose ’ support multiperson pose estimation optimize model tensorrt addition authors set jetson nano edge ai deployment device model successfully realize model migration edge end experimental results show speed optimized object detection model reach 20 fps optimized multiperson pose estimation model reach 10 fps image resolution 320×240 model ’ accuracy 732 meet realtime requirements short scheme provide basis lightweight multiperson action recognition scheme edge end © 2023 polish association knowledge promotion rights reserved', 'human pose recognition current research hot spot aims detect information human joints orientations scales images videos predict specific pose human body key point information benefiting rapid evolution deep learning target detection algorithms convolutional neural networks achieved breakthroughs accuracy efficiency basis detailed literature research analysis paper provides comprehensive evaluation research progress human pose estimation specifically mainly introduce classification human action poses human action pose recognition including design ideas basic advantages disadvantages representative recognition finally summarize main challenges give outlook future research development object detection © 2023 spie', 'human pose estimation focuses locating finding key characteristics human body images videos often used many scenes auxiliary part paper firstly classifies summarizes latest research human pose estimation dividing achievements 3d pose estimation multiperson posture estimation occlusion problem summarize uptodate solve existing problems details different models solutions used different problems also introduced finally point current challenges encountered human pose estimation field © 2023', 'study addresses human pose estimation problem thermal images convolutional neural networks vision transformer architectures eight human pose estimation designed visible images extended applied thermal domain due lack large representative datasets containing labeled thermal images extension requires transfer learning visible thermal domain database finetuning networks thermal domain thus train networks grayscale version coco dataset finetune thermal domain finetuning carried new uchthermalpose database presented work database includes 600 thermal images training 200 validation 104 testing fully labeled moreover paper comparative study eight extended deepbased human pose detection carried uchthermalpose database available httpsdatosuchilecldatasetxhtmlpersistentiddoi3a10346912fuchile2f4b6na3 source code available athttpsgithubcomjsmithdlcthermalhumanposeestimation © 2013', 'physical rehabilitation focuses improvement body functions usually injury surgery patients undergoing rehabilitation often need perform exercises home without presence physiotherapist computeraided assessment physical rehabilitation improve patients performance help completing prescribed rehabilitation exercises work focus human motion analysis context physical rehabilitation low back pain lbp 2d 3d human pose estimation rgb images made impressive improvements aim compare assessment physical rehabilitation exercises movement data acquired rgb videos human pose estimation work provide analysis two types algorithms low back pain rehabilitation datasets one gaussian mixture model gmm performance metrics loglikelihood values gmm furthermore recent development deep learning graph neural networks algorithms spatiotemporal graph convolutional networks stgcn taken novel compared algorithms terms data efficiency performance evaluation performed two lbp rehabilitation datasets kimore keraal study confirms kinect openpose blazepose data yield similar evaluation scores shows stgcn outperforms gmm configurations © 2023', 'human pose gesture estimation crucial correcting physiotherapy fitness exercises recent years advancements computer vision machine learning approaches led development sophisticated pose estimation models accurately track analyze human movements real time technology enables physiotherapists fitness trainers gain valuable insights clients exercise forms techniques facilitating effective exercise corrections personalized training regimens research aims efficient artificial intelligence human pose estimation physiotherapy fitness exercises utilized multiclass exercise dataset human skeleton movement points conduct experimental research dataset comprises 133 features derived human skeleton movements various exercises resulting high feature dimensionality affects performance human pose estimation machine learning deep learning introduced novel logistic regression recursive feature elimination logrf feature selection extensive experiments demonstrate top twenty selected features random forest outperformed studies highperformance score 0998 performance applied validated kfold enhanced hyperparameter tuning study assists specialists identifying addressing potential biomechanical issues improper postures incorrect movement patterns essential injury prevention optimizing exercise outcomes furthermore study enhances capabilities remote monitoring guidance capabilities allowing physiotherapists support patients progress prescribed exercises continually © 2013', 'paper describes endtoend weakly supervised estimating 3d human pose single image model trained projecting 3d pose 2d pose matching groundtruth 2d pose supervision obtain accurate projection 3d pose 2d pose mathematical camera model intrinsic extrinsic camera parameters used specifically epnp algorithm estimate extrinsic transformation matrix transform estimated 3d pose reprojected back 2d pose advantage projection requires training robust diversity training datasets constrain pose generation adversarial generative network transformer used 3d pose generator transformer selfattention mechanism establish dependencies joint predict pose important joints reprojection achieves competitive results human36m mpiinf3dhp among weakly supervised experiments also demonstrate models generalization ability wild images © 2023', '59 topics discussed performance comparison deep learning text embeddings sentiment analysis tasks online consumer reviews literature analysis natural language processing network analysis structure design log management platform supports smooth expansion anomaly detection capability endtoend entity linking combined bertbased siamese interaction network experimental hybrid contentbased web page detection big data tax collection management bank credit financing passive optical motion capture towards occlusion conditions multivision system automated disease detection clinical text topic modeling efficient human pose estimation via multihead knowledge distillation multimodal medical image fusion average cross bilateral filtering comparison study convolutional neural network recurrent neural network image classification', 'human pose estimation refers detection tracking position human body parts input data like images videos common computer vision techniques require installation cameras rgb infrared cameras however visionbased systems susceptible challenges like complicated backgrounds occlusions lowresolution images recently ultrawideband uwb technology gained popularity due low complexity high accuracy penetration capability hence novel human pose estimation uwb technology solution retrieves 3d human pose five bodymounted uwb sensors placed pelvis limbs joints namely wrists ankles designed transformerbased architecture capable reconstructing human skeleton small subset joints measured ranging information uwb © 2023', 'challenging task reconstruct human pose millimeter wave mmwave radar point clouds due sparsity sensitivity multipath noise order enhance inference ability deep learning models processing sparse radar point clouds learning paradigm spatial temporal multiscale constraints utilizing prior relationships skeletal structure joint movements time space constrain learning pose sequences specifically spatial multiscale constraint block learns spatial constraint relationships human joints three different scales adjacent joint constraint partlevel kinematic constraint global joint constraint spatial joint constraint features aggregated fusion gate mechanism hand temporal multiscale constraint block devised learn temporal constraint relationships joint trajectories information joint local context information time domain compared singlescale constrained learning paradigm potential advantage reduce impact random missing noise radar data finally effectiveness superiority fully demonstrated experimental results two public datasets human pose estimation mmwave radar', 'estimating 3d human pose monocular video challenging mainly due depth ambiguity inaccurate 2d detected keypoints quantify depth uncertainty 3d human pose via neural network imbue uncertainty modeling depth prediction evidential deep learning edl meanwhile calibrate distribution uncertainty 2d detection explore probabilistic representation model realistic distribution specifically exploit edl measure depth prediction uncertainty network decompose xy coordinates individual distributions model deviation uncertainty inaccurate 2d keypoints optimize depth uncertainty parameters calibrate 2d deviations obtain accurate 3d human poses besides provide effective latent features uncertainty learning design encoder combines graph convolutional network gcn transformer learn discriminative spatiotemporal representations extensive experiments conducted three benchmarks human36m mpiinf3dhp humanevai comprehensive results show model surpasses stateofthearts large margin © 2022 acm', 'yoga practice aims develop allaround personality synchronizing mind body spirit however incorrect postures techniques cause damage ancient times yoga performed supervision teacher difficult find competent guru todays fastpaced world goal project develop application track evaluate physical exercise specifically yoga human pose estimation application called realtime virtual yoga assistant machine learning methodologies classify data yoga positions prerecorded realtime videos research also examines various pose estimation key point detection approaches deep learning models used posture classification © 2023', 'deep learningbased 3d human pose estimation performs best trained large amounts labeled data making combined learning many datasets important research direction one obstacle endeavor different skeleton formats provided different datasets ie label set anatomical landmarks little prior research best supervise one model discrepant labels show simply separate output heads different skeletons results inconsistent depth estimates insufficient information sharing across skeletons remedy novel affinecombining autoencoder acae perform dimensionality reduction number landmarks discovered latent 3d points capture redundancy among skeletons enabling enhanced information sharing used consistency regularization scales extreme multidataset regime 28 3d human pose datasets supervise one model outperforms prior work range benchmarks including challenging 3d poses wild 3dpw dataset code models available research purposes1 © 2023', 'study 3d attitude estimation algorithm rmpe algorithm coupled deep neural network combines human pose estimation action recognition provides new idea basketball auxiliary training compared traditional singleaction recognition present makes recognition accuracy better display effect intuitive flipped classroom teaching mode algorithm applied college sports basketball optional course explore influence teaching mode classroom teaching effect compared evaluation index action recognition experimental results various action recognition datasets compared analyzed verified good recognition effect values topi top5 4221 8877 respectively 1061 3509 higher kineticsskeleton dataset however compared ntu rgm dataset recognition rate topi significantly reduced compared traditional singleaction recognition better recognition accuracy intuitive display effect fusion human posture estimation motion recognition provides new idea basketball auxiliary training © 2022 authors', 'research study utilizes computer vision estimate multiperson human pose realtime prerecorded video computer vision examines human posture detection rgb images works well gesture control gaming human tracking action detection action tracking tracking semantic important points pose estimation twodimensional human pose estimation predicts spatial placement key human body points images videos several anatomical areas handcrafted feature extraction estimate twodimensional human position visual input data human body component locations used estimate human pose opencv mediapipe detected 33 posture landmarks research estimating human body state requires modeling modelbased used describe infer human posture 2d 3d research blazepose ghum 3d pose landmark model 2d human pose estimation output poses x z visibility image width height x landmark coordinates landmark depth z image shows landmark image video resolution number persons scene anomalies might impact visibility point accuracy 0969 © 2023', 'human pose estimation refers technique recovering key points human body given input owing wide rgbd cameras rapidly improving performance rgbd scenes widely used near future along rapid development deep learning paper two new scenes precision improvement fast calculation exploring task rgbd 3d human pose estimation test results mkt data set shows paper improve precision perform better best currently used terms precisionrelated index paper fast calculation run overall process speed nearly 30 frames quite small precision error good performance paper makes possible apply paper scenes low computational power terminals requiring realtime running believed paper promising rgbd 3d human pose estimation © 2023', 'objective application computer models continuous patient activity monitoring video cameras complicated capture images varying qualities due poor lighting conditions lower image resolutions insufficient literature assessed effects image resolution color depth noise level low light inference eye opening closing body landmarks digital images study systematically assessed effects varying image resolutions 100 × 100 pixels 20 × 20 pixels interval 10 pixels lighting conditions 42 2 lux interval 2 lux colordepths 167 colors 8 1 512 k 216 k 64 k 8 k 1 k 729 512 343 216 125 64 27 8 colors noise levels accuracy model performance eye dimension estimation body keypoint localization dlib library openpose images closed eyes wild coco datasets well photographs face captured different light intensities results model accuracy rate model failure remained acceptable image resolution 60 × 60 pixels color depth 343 colors light intensity 14 lux gaussian noise level 4 ie 4 pixels replaced gaussian noise conclusions dlib openpose models failed detect eye dimensions body keypoints low image resolutions lighting conditions color depths clinical impact established baseline threshold values useful future work application computer vision continuous patient monitoring © 2022 authors', 'despite remarkable performance rgbbased multihuman pose estimation mpe technology many practical limitations nighttime smoggy environments infrared imaging valid substitution scenarios needs efficient fast mpe paper aims design infrared mpe model encoderdecoder cnn architecture infpose perform realtime edge devices first built lightweight encoderdecoder cnn backbone hardwarefriendly inverted residual blocks secondly utilized three improve capability infpose including decoupling associative embedding head multiscale supervision crossmodal knowledge distillation addition gathered wild infrared human pose dataset train evaluate experiment results show model robust less latency inference edge gpu platforms comparison prevailing mainstream models inference time infpose xavier nx recorded 277ms inlineformulatexmath notationlatexapproxtexmathinlineformula 37 fps maintained sufficient accuracy research applied humanmachine interaction autonomous vehicles intelligent robots nighttime scenes poor visibility', 'clinical inbed videobased human motion analysis relevant computer vision topic several relevant biomedical applications nevertheless main public large datasets eg imagenet 3dpw used deep learning approaches lack annotated examples clinical scenarios address issue introduce blanketset rgbird action recognition dataset sequences performed hospital bed dataset potential help bridge improvements attained general large datasets clinical scenarios information access dataset available rdminesctecptdatasetnis2022004 © 2023', 'estimating 3d human pose single colored image presents significant challenge due depth ambiguity existing techniques often treat joint locations independently leading potential overfitting limited structural coherence address novel 3d human pose estimation generative adversarial networks gans leverage adversarial training learn realistic representations 3d human body involves regressing 2d input 3d joint location generator discriminator distinguishes 3d ground truth samples projected samples capture spatial relationships effectively generator discriminator employ graph convolutional networks gcns integration gcns gans yields accurate 3d joint location predictions coherent human body designs technique outperforms popular benchmark datasets including human36m humanevai showcasing efficacy advancing 3d human pose estimation © 2023', 'running gait assessment essential development technical optimization strategies well inform injury prevention rehabilitation currently running gait assessment relies visual assessment exhibiting subjectivity limited reliability ii instrumented approaches often carry high costs intrusive due attachment equipment body iotenabled markerless computer vision smartphone application upon google ’ pose estimation model blazepose evaluated running gait assessment lowresource settings human pose estimation architecture used extract contact time swing time step time knee flexion angle foot strike location large cohort runners goldstandard vicon 3d motion capture system used reference performs robustly demonstrating good icc21 075 excellent icc21 090 agreement running gait outcomes additionally temporal outcomes exhibit low mean error 001–0014 left foot outcomes however discrepancies right foot outcomes due occlusion study demonstrates lowcost markerless system provides accurate running gait assessment outcomes may help routine running gait assessment lowresource environments © 2023 authors', '2d human pose estimation hpe widely used many fields behavioral understanding identity authentication industrial automatic manufacturing previous studies encountered many constraints restricted scenarios strict inputs solve problem present simple yet effective hpe network called limb direction cuesaware network ldcnet limb direction cues differentiated cauchy labels efficiently suppress uncertainties prevent deep networks overfitting uncertain keypoint positions particular ldcnet suppresses uncertainties two aspects 1 differentiated cauchy coordinate encoding designed reveal limb direction information among adjacent keypoints 2 jeffreys divergence introduced loss function measure prediction heatmap groundtruth one positions keypoints perceived limb direction deep network endtoend manner extensive study two benchmark data sets ie ms coco mpii illustrates superiority ldcnet model approaches', 'knowing exact 3d location workers robots collaborative environment enables several real applications detection unsafe situations study mutual interactions statistical social purposes letter noninvasive lightinvariant depth devices deep neural networks estimate 3d pose robots external camera applied robot without requiring hardware access internal states introduce novel representation predicted pose namely semiperspective decoupled heatmaps spdh accurately compute 3d joint locations world coordinates adapting efficient deep networks designed 2d human pose estimation takes input depth representation xyz coordinates trained synthetic depth data applied realworld settings without need domain adaptation techniques end present simba dataset synthetic real depth images experimental evaluation results show made specific depth map representation spdh overcomes current state art © 2016', 'rapid development deep learning human pose estimation technology made remarkable progress recent years existing still difficult deal common occlusion problem address problem human pose estimation keypointlevel occlusion inference paper firstly baseline human pose estimation network used obtain noisy representation keypoint human body images occlusion noises occluded keypoints estimated occlusion part prediction module obtain visibility vector occlusion part prediction module study consists two submodules occlusion part classification network visibility encoder occlusion part classification network predicts occlusion state keypoint human body channel attention mechanism visibility encoder converts predicted occlusion state set weight parameters finally visibility vector noise features fused channel reweighting obtain keypointlevel occlusion aware features used calculate heatmaps keypoints experimental results mpii lspleeds sports pose datasets show compared baseline human pose estimation network better deal occlusion problem small extra computational cost achieve better results existing © 2022 science press right reserved', 'facilitate diagnosis cardiac ultrasound us clinical practice established several standard views heart serve reference points diagnostic measurements define viewports images acquired automatic view recognition involves grouping images classes standard views although deep learning techniques successful achieving still struggle fully verifying suitability image specific measurements due factors like correct location pose potential occlusions cardiac structures goes beyond view classification incorporates 3d mesh reconstruction heart enables several downstream tasks like segmentation pose estimation work explore learning 3d heart meshes via graph convolutions similar techniques learn 3d meshes natural images human pose estimation availability fully annotated 3d images limited generate synthetic us images 3d meshes training adversarial denoising diffusion model experiments conducted synthetic clinical cases view recognition structure detection yielded good performance synthetic images despite exclusively trained synthetic data already showed potential applied clinical images proofofconcept aim demonstrate benefits graphs improve cardiac view recognition ultimately lead better efficiency cardiac diagnosis © 2023 authors exclusive license springer nature switzerland ag', 'human pose estimation crucial challenge field computer vision contributing significantly diverse domains fall detection security healthcare existing models achieve high accuracy constructing complex network architectures often overlook issue impracticability deployment edge devices due substantial computational requirements response swbpose lightweight model tailored efficient pose estimation research demonstrates integration swin transformer feature extract stfe module expedite model convergence also introduce novel dynamic shift max activation function empirically shown enhance models average precision ap 02 without necessitating additional computational resources validated model coco2017 dataset achieving mean average precision map 6601 frame rate 9676 fps computational complexity 061 gflops model size 437m parameters implemented gpu experimental results clearly demonstrate model offers superior performance balancing tradeoff accuracy latency © 2023', '180 special focus conference future technologies topics pretrained cnn svm classifier weld joint type recognition twostage federated transfer learning medical images classification limited data covid19 case study graph emotion distribution learning emotiongcn role depth predictions 3d human pose estimation aibased qosqoe multimedia systems snatch theft detection deep learning models deep learning fewshot learning detection skin cancer overview enhancing artificial intelligence control mechanisms current practices real life applications future views face generation skull photo gan 3d face models general particle swarm optimization artificial intelligence videogames drive forward bezier curvebased shape knowledge acquisition fusion surrogate model construction path planning landing unmanned aerial vehicles ai digital ticketing system public transport mexico avoid cases contagion artificial intelligence question practical implementation “ digital immortality ” technologies new approaches creation ai collaborative forecasting “ sliderswarms ” improves probabilistic accuracy learning solve sequential planning problems without rewards systemic analysis democracies concept humantechnological development regression algorithms artificial intelligence predict price bitcoin exploring deep learning road traffic accident recognition roadside sensing technologies integration humandriven autonomous vehicle cell reservation intersection control strategy equivalence classical epidemic model quantum tightbinding model effects various barricades human crowd movement flow classical logic continuous logic editor ’ preface', 'human pose estimation key area computer vision benefits various fields comparative study research approaches united states china leaders domain vital understanding influencing global trends technology review collected 191 influential 2014 2022 sourced google scholar structural topic model stm utilized analyze research content preferences trends research topics specifically 10 topics summarized topic proportions preferences intensities word clouds displayed via visualization findings revealed 1 research feature extraction depth image constituted largest proportion approximately 122 data training research accounted lowest proportion around 79 2 united states china exhibited distinct research preferences united states focused model data research china emphasized deep learning neural networks 3 countries exhibited similar research trends within topics research deep learning technologies experienced slowdown recent years comparative study review offers valuable insights guidance future investigations applications human pose estimation improving quality diversity data sets © 2023 authors exclusive licence springer sciencebusiness media llc part springer nature', 'recent multimedia computer vision research focused analyzing human behavior activity images skeleton estimation known pose estimation received significant attention human pose estimation deep learning approaches primarily emphasize keypoint features conversely case occluded incomplete poses keypoint feature insufficiently substantial especially multiple humans single frame features body border visibility conditions contribute pose estimation addition keypoint feature model integrates multiple features namely human body mask features serve constraint keypoint location estimation body keypoint features keypoint visibility via mask regionbased convolutional neural network maskrcnn sequential multifeature learning setup formed share multifeatures across structure whereas maskrcnn feature could shared system region interest feature twoway upscaling shared weight process produce mask addressed problems improper segmentation small intrusion object loss maskrcnn used instance segmentation accuracy indicated percentage correct keypoint model identify 861 correct keypoints © 2023 shanghai jiao tong university', 'person identification problem received substantial attention particularly security domains gait recognition one convenient approaches enabling person identification distance without need highquality images several review studies addressing person identification utilization facial images silhouette images wearable sensor despite skeletonbased person identification gaining popularity overcoming challenges traditional approaches existing survey studies lack comprehensive review skeletonbased approaches gait identification present detailed review human pose estimation gait analysis makes skeletonbased approaches possible study covers various types related datasets tools methodologies evaluation metrics associated challenges limitations application domains detailed comparisons presented aspects recommendations potential research alternatives common trend throughout paper positive impact deep learning techniques beginning topics human pose estimation gait identification survey outcomes might useful related research community stakeholders terms performance analysis existing methodologies potential research gaps application domains possible contributions future © 2022 association computing machinery', 'human pose estimation deep learning attracted increasing attention past years shown superior performance various datasets many researchers increased number network layers improve accuracy model however deepening number network layers parameters computation model also increasing makes model unable deployed edge devices mobile terminals limited computing power also makes many intelligent terminals limited volume power consumption storage inspired lightweight human pose estimation model lightweight network solve problems designs lightweight basic block module deep separable convolution reverse bottleneck layer accelerate network calculation reduce parameters overall network model experiments coco dataset mpii dataset prove lightweight basicblock module effectively reduce amount parameters computation human pose estimation model © 2022 authors exclusive licence springerverlag gmbh germany part springer nature', 'human pose estimation among emerging fields actively researched attracting interest various purposes applications could range healthcare robotics human augmentation virtual reality especially aging societies like japan could great help monitoring status elders elderlycare centers notify prevent injuries existing vary sensor type sensor quantity utilized algorithms optical motion capture systems multiple inertial sensor systems popular ones however systems suffer high maintenance setup costs space limitations drifting errors restricted existing studies indoor activities research developed deep learningbased lower limb joint angle estimator two insole sensors placed shoe insole sensors convenient attachment less prone skin artifacts 45minute walking insole optical data collected 6 × 3 indoor space systems different sampling frequencies synchronization performed semiautomatic way optical motion capture used validate estimation system two neural network algorithms namely convolutional neural network cnn bidirectional long shortterm memory blstm neural network investigated study training cnn achieved better results average mean absolute error mae 317° four sagittal plane joint angles extensionflexion hip knee joints finally trained model tested unseen datasets showed promising result used foundation analysis © 2023', 'anthropometric detection tasks play crucial role medical military recruitment processes help identify abnormalities could otherwise missed presently measurements carried manually markers timeconsuming process prone errors paper presents computer visionbased system detecting shoulder knee abnormalities automatically measuring shoulder tilt knee distance observe knockknees bowlegs condition system employs deep learning blazepose landmark estimation accurately identify anomalies shoulders legs atan dist theoretical basis applied shoulder tilt knee distance measurements respectively system measure shoulder tilt knee distance error rate less 10 automation measurements reduces time required examination eliminates subjectivity potential errors associated manual measurements therefore system potential revolutionize shoulder knee abnormality examinations offering accurate efficient diagnoses © 2023', 'covid19 pandemic situation trainers accessible forced many people work home resulting difficulties accessing professional trainers validate exercise postures therefore need automated training facility address challenge deep learning model developed research work mediapipe machine learning computer vision solution blazepose realtime pose estimation model analyze exercise movements provide realtime feedback users model enables safer effective home workout routines validating posture offering corrective suggestions increasing accessibility reducing costs associated professional physical trainers paper presents model potential revolutionize way people exercise home model evaluated exercise video dataset performance satisfactory © 2023', 'explore application human–computer interaction hci industrial digital twins dts current application status dts intelligent manufacturing im hci problem human–computer assembly explored aiming human action recognition har machine perspective human–computer assembly human pose estimation hpe improved hrnet inroduces attention mechanism establish senewhrnet model optimization hpe addition points adaptive architecture deep learning confrontation adlc performs case analysis performance verification model accuracy senewhrnet mathrmappose indicator 751 compared models network performance improved different extents number parameters calculation amount lower loss value decreases rapidly decrease rate becomes slower 40 iterations adlc recognition accuracy branch model three domain discrimination highest reaching 8650 cases adlc performance better contrast models compared wasserstein generative adversarial networks wgan model second comprehensive performance average accuracy adlc 6972 924 higher wgan 2767 higher sourceonly therefore human recognition performs better © authors exclusive license springer nature singapore pte ltd 2023', 'paper occlusionaware mechanism used detecting tracking pedestrians videos acquired surveillance cameras includes extraction trajectory points estimation walking velocities detection groups projection final trajectories 2d plan occlusionaware mechanism introduced order manage irregularities pedestrian trajectory data derived occlusions mechanism able identify parts human body occluded skeleton data generated human pose estimation algorithms adjust dimensions bounding boxes occluded pedestrians © 2023', 'many actual images partial body camera shots significant part body visible issue especially prevalent film images less 10 fullbody shots 2d human pose estimation return incomplete poses applied partial body images lack completeness becomes problem situations example 2d pose converted 3d pose twostage 3d human pose estimation since require complete pose work article new technique called completepose consisting completing missing keypoints 2d human pose estimation applied images partial body camera shots conditional generative adversarial network used obtain complete plausible pose realistic enough predict 3d pose twostage 3d human pose estimation complete empirical validation carried human36 dataset new dataset called charade specially built made public reproducibility benchmarking research quantitative evaluation employing fréchet distance shows manages approximate actual data distribution qualitative evaluation shows completed poses enable obtaining plausible 3d poses images previously intractable © 2023 informa uk limited trading taylor francis group', 'large fieldofview fisheye camera allows capturing large area minimal numbers cameras mounted high position facing downwards topview omnidirectional setup greatly reduces work cost deployment compared traditional solutions multiple perspective cameras recent years deep learning widely employed vision related tasks including omnidirectional settings survey look application deep learning combination omnidirectional topview cameras including available datasets human object detection human pose estimation activity recognition miscellaneous applications © 2023', 'human activity recognition har theme great interest research especially thanks possible practical applications fields video surveillance humanmachine interaction gaming autonomous driving health care despite problem still remains complex challenge whose definitive solution still far away since quality feature extracted impacts dramatically results overall system human pose estimation hpe phase crucial har however problem remains establishing extracted pose according network created har work aims analyze ways pose estimation 2d poses extracted monocular images affect har system consisting recurrent network purpose various possible recurring structures examined whose input pose used different formats analysis carried shows numerical simplification inputs facilitates learning compared “ human ” contrary could consider easier start graphic visualization skeleton © 2023 authors exclusive license springer nature singapore pte ltd', 'human pose estimation aims detect keypoints human body input images development deep learning human pose estimation gradually matured widely used humancomputer interaction motion analysis augmented reality virtual reality paper novel network human pose estimation contains contextual information aggregation module multilevel branch feature fusion skeleton graph encoding optimization branch contextual information aggregation module captures contextual dependencies features aggregates obtain pixellevel feature representation multilevel branch feature fusion mechanism fuses highlevel lowlevel features gradually expand feature map enhance expressiveness extracted features addition introduce skeleton graph prior knowledge strengthen body structure constraint locate keypoints human body accurately experiments mainstream dataset ms coco human pose estimation demonstrate effectiveness © 2022 authors exclusive license springer nature singapore pte ltd', 'radiofrequency rf human sensing technologies due great practical value various applications privacypreserving nature gained tremendous attention recent years however without fully exploiting characteristics radio signals performance existing still limited first rf features moving human body different representations dimensions channel scale challenging performing feature fusion besides human body specularly reflective respect radar means human body fully captured single rf snapshot therefore radar signal reflected human body sparse incomplete difficult extract highquality features 3d human pose estimation paper present rfbased pose machines rpm novel generate 3d skeletons rf signals considering characteristics rf signals rpm includes several modules overcome challenges firstly multidimensional feature fusion mff backbone designed effectively fuse radio signals channels correlation maintain highquality feature via multiscale fusion block spatiotemporal attention network designed reconstruct 3d skeletons modeling nonlocal spatiotemporal relationships evaluate performance rpm construct largescale dataset synchronized 3d skeletons rf signals rfskeleton3d experimental results show rpm locates 3d key points human body average error 571cm maintains performance new environments occlusion bad illumination dataset codes made public © 2022', 'past years research animal pose estimation computer vision field grown many aspects 2d 3d pose estimation 3d mesh reconstruction behavior prediction promoted deep learning animal pose estimation tools animal pose datasets also made publicly available however compared human pose estimation already high accuracy high applicability complex scenes animal pose estimation still preliminary stage huge domain shift species scarce datasets uncooperative research subjects pose intractable challenges development robust accurate animal pose estimation algorithms review paper summarize recent 2013 2021 work animal pose estimation computer vision perspective order present approaches highlight challenges face field first categorize various animal pose estimation present according several keywords also sort introduce released annotated image video 3d models animal poses well promising substitute real dataset also report performances existing algorithms visualize results finally provide indepth analysis persisting obstacles field existing work offer potential solutions © 2022 elsevier inc', '3d human pose estimation refers recovering pose human body threedimensional space image estimating coordinates key points human body threedimensional coordinate system human skeleton model composed keypoints describes human pose imagethe traditional pose estimation motion capture equipment requires athletes wear auxiliary equipment affects normal training athletes development deep learning convolutional neural network shown strong representational capabilities process image feature extraction effective achieve human pose estimation however actual complex changeable threedimensional space environment noisy limbs occluded monocular capture human posture problems low accuracy instability provide sufficient semantic informationto solve poroblem paper designs multiview information fusion algorithm first twodimensional attitude feature extraction module designed connecting interacting heat maps different resolving power parallel heat map always maintains higher resolving power higher precision extracts twodimensional human pose single perspective multiview information fusion technology paper complete correct pose information different perspectives obtain accurate twodimensional pose camera pose transformation threedimensional human pose estimation realized experimental verification carried multieye machine vision pantilt zoom tracking shooting system © 2022 spie', '46 topics discussed detection workpiece end defects convolution neural network mcamnet novel encoderdecoder network multidirectional convolution attention mechanism crack detection research typical sotif trigger conditions autonomous vehicle perception layer passive detection crowdsensing technologies public transport sign language recognition system jetson tx2 yolov5 study intelligent drug delivery robot ros trend 2d human pose estimation deep learning design gesture recognition system opencv research end face defect detection small sample workpiece meta learning', 'recently human pose estimation human skeleton capture system developed rapidly raised attention posebased action recognition deep learning algorithms like cnn rnn applied many tasks past years although showed promising prospects lack ability take full advantage spatiotemporal relationship information skeletons behavior explicitly accuracy action recognition easy improve put forward new effective cnntransformer model able extract spatialtemporal information human posebased action recognition address problems firstly extract pose features original video secondly pose features processed action modeling module feed spatiotemporal information skeleton points generated modeling cnn transformer modules respectively form relative position relative speed finally output information obtained two modules fused compare model previous advanced carry comparative experiments subjhmdb sysu3d ntu rgbd datasets obviously model significantly improve action recognition performance experimental results © 2022', 'human pose estimation challenging research task field computer vision current mainstream work made great progress pose estimation works still pay enough attention negative impact background human pose estimation work human pose estimation characterized joint usage global local attention module hourglass backbone network global attention module aims reduces negative impact background local attention module designed help refine joint tested two benchmark datasets human pose estimation experimental results show model superior current mainstream algorithms © 2021', 'motion capture sparse inertial sensors solving occlusion economic problems visionbased suitable virtual reality applications works complex environments however vr applications need track location user realworld space hard obtain inertial sensors paper present fusion poser combines deep learningbased pose estimation location tracking six inertial measurement units head tracking sensor provides headmounted displays estimate human poses bidirectional recurrent neural network convolutional long shortterm memory layer achieves higher accuracy stability preserving spatiotemporal properties locate user realworld coordinates integrates results estimated joint pose pose tracker train model gathered public motion capture datasets synthesized imu measurement data well creating realworld dataset evaluation showed higher accuracy robust estimation performance especially user adopted lower poses squat bow © 2022 authors licensee mdpi basel switzerland', 'recent advances deep learning resulted emergence accurate models human pose estimation color videos distance automatically estimated manually annotated joint positions commonly used evaluation however practical point view pose estimation goal therefore work study useful deep learning pose estimation approaches practical scenario human action recognition compare different variants pose estimation models baseline provided kinect skeleton tracking recently widely used solution applications present comprehensive posebased action recognition evaluation consists classical machine learning approaches including feature extraction selection classification steps well recent endtoend extensive evaluation four publicly available datasets shows neural network models pose tracking colorbased action recognition matches even outperforms depthbased one © 2022 authors exclusive license springer nature switzerland ag', 'deep learningbased pig keypoint detection nutshell explored transfer learning adapt human pose estimation model pigs total tested three different models eventually trained openpose pig data training data annotated coco format additionally visualized pixel level response network named paf part infinity field test frames highlight model learning capabilities trained model shows promising results open new door research © 2022 society imaging science technology rights reserved', 'paper duallevel structural information learning neural network human pose estimation referring human body structure human poses highly structured 2d space thus structural information among human body parts meaningful human pose estimation however existing incorporate structural information limits achievable accuracy bidirectional memoryaugmented rnn bilstm duallevel structural information learning module compose structural information learning neural network human body parts taken sequential information keypoint heatmaps bone heatmaps predicted endtoend network structural information captured joint bone levels exploiting coarse keypoint heatmaps bone heatmaps mutual priors experimental results two challenging datasets demonstrate achieves pck detection rate improvement 03 lsp dataset compared baseline pckh detection rate improvement 02 mpii dataset sateoftheart baseline work © 2022', 'recent advancements deep learning produced significant progress markerless human pose estimation making possible estimate human kinematics single camera videos without need reflective markers specialized labs equipped motion capture systems algorithms potential enable quantification clinical metrics videos recorded handheld camera used deeplabcut opensource markerless pose estimation finetune deep network track 5 body keypoints hip knee ankle heel toe 82 belowwaist videos 8 patients stroke performing overground walking clinical assessments trained pose estimation model labeling keypoints 2 frames per video trained convolutional neural network estimate 5 clinically relevant gait parameters cadence double support time swing time stance time walking speed trajectory keypoints results compared obtained clinical system gait analysis gaitrite® cir systems absolute accuracy mean error precision standard deviation error swing stance double support time within 004 ± 011 pearsons correlation reference system moderate swing times r 04066 stronger stance double support time r 093095 cadence mean error 025 stepsmin ± 39 stepsmin r 097 walking speed mean error 002 ± 011 ms r 092 preliminary results suggest single camera videos pose estimation models deep networks could used quantify clinically relevant gait metrics individuals poststroke even assistive devices uncontrolled environments development opens door applications gait analysis inside outside clinical settings without need sophisticated equipment © 2022 authors published karger ag basel', 'worlds elderly population continues grow unprecedented rate creating need monitor safety aging population one current problems accurately classifying elderly physical activities especially falling delivering prompt assistance someone need owing advancements deep learning research vision solutions employed action recognition one popular human pose estimation action recognition fall detection nevertheless due lack largescale elderly fall datasets continuation numerous challenges varying camera angles illumination occlusion accurately classifying falls problematic address problems research first carried comprehensive study ai hub dataset collected real lives elderly people order benchmark performance human pose estimation secondly owing limited number real datasets augmentation synthetic data applied performance improvement validated changes degree accuracy third study shows transformer network applied elderly action recognition outperforms lstmbased networks noticeable margin lastly observing quantitative qualitative performances different networks paper efficient solution elderly activity recognition fall detection context surveillance cameras © 2013', 'human pose estimation technique estimating human pose input information obtained sensor widely learned field computer vision human body information provided human pose estimation widely used many applications helps lot artificial intelligence development deep learning human pose estimation made significant progress achieved significant performance deep learning techniques still problems occlusion insufficient training data etc still challenges need handeled rapid development human pose estimation gradually emerging problems article tries refine three types problems human pose estimation summarize teams ’ latest deal problems © 2022 spie', 'avatar refers representative physical user virtual world engage different activities interact objects metaverse simulating avatar requires accurate human pose estimation though camerabased solutions yield remarkable performance encounter privacy issue degraded performance caused varying illumination especially smart home paper wifibased iotenabled human pose estimation scheme metaverse avatar simulation namely metafi specifically deep neural network designed customized convolutional layers residual blocks map channel state information human pose landmarks enforced learn annotations accurate computer vision model thus achieving crossmodal supervision wifi ubiquitous robust illumination making feasible solution avatar applications smart home experiments conducted real world results show metafi achieves high performance pck50 9523 © 2022', 'human gait analysis one primary procedures diagnosis modern healthcare applications various diseases instead expensive wearable sensors patients research aims assist gait analysis classification medical diagnoses computer vision solely long shortterm memory lstm neural network mediapipe pose videobased human gait analysis assist diagnosing patients neurodegenerative diseases particularly cerebellar ataxia kinematic parameters extracted pose estimation model captured gait videos deriving spatiotemporal parameters quantitative gait analysis data augmentation applied increase dataset size fivefold crossvalidation performed verify suitability developed dataset training deep neural networks selected lstm model achieves testing accuracy 998 high precision recall metrics ataxic normal gait classes methodology applied broader applications remote rehabilitation patient monitoring clinical relevancethe developed system assist physicians diagnosing cerebellar ataxic patients monitoring gait rehabilitation process remotely via camera vision © 2022', 'development computer vision technology human pose estimation indispensable part humancomputer interaction gradually attracted attention researchers unlike existing hightolow resolution networks human pose estimation networks hrnet maintain highresolution representations throughout process resulting reliable output results however hrnet problem large amount operation parameters loss feature information multiresolution feature fusion occurs therefore hrnet32 paper makes improvements human pose estimation lightweight highresolution network polarized self attention introducing improved gaff module ghostnet first stage hrnet32 network successfully lightened spatial channel feature information extracted carefully polarized selfattention mechanism accurate human pose estimation effect obtained © 2022', 'estimating threedimensional human pose color images single person fundamental problem many applications however problems inaccuracy illposed poses well solved novel deep learning estimating 3d human poses images first voxel representation adopted joint coordinates presented represent poses second space integral regression used compute output results convolutional network finally output sent fully connected network joint training algorithm tested two standard test protocol human36m dataset experimental results show obtains higher accuracy previous achieves well generalization ability mpiinf3dhp dataset © 2022 institute computing technology rights reserved', 'human pose estimation one issues gained many benefits deep learningbased models human pose hand mesh estimation significant problem attracted attention computer vision community past decades wide variety solutions tackle problem deep learningbased approaches extensively studied recent years used address several computer vision problems however sometimes hard compare due intrinsic difference paper extensively summarizes current deep learningbased 2d 3d human pose hand mesh estimation single multiperson single doublestage methodologybased taxonomy authors aim make every step deep learningbased human pose hand mesh estimation techniques interpretable providing readers readily understandable explanation presented taxonomy clearly illustrated current research deep learningbased 2d 3d human pose hand mesh estimation moreover also provided dataset evaluation metrics 2d 3dhpe approaches © 2021 authors', 'human pose estimation images important challenging problem computer vision currently employ deep learning techniques excel task 2d human pose estimation 2d human poses used diverse broad set applications great relevance society however 3d poses lead even accurate robust results since joint coordinates 3d poses difficult estimate fully convolutional tend perform poorly one possible solution estimate 3d poses 2d poses offer improved performance delegating exploration image features mature 2d pose estimation techniques goal paper present survey recent advances twostep techniques 3d human pose estimation 2d human poses © 2022 authors exclusive license springer nature switzerland ag', 'human pose estimation hpe rgb images experienced rapid development benefiting deep learning however eventbased hpe fully studied remains great potential applications extreme scenes efficiencycritical conditions paper first estimate 2d human pose directly 3d event point cloud novel representation events rasterized event point cloud aggregating events position small time slice maintains 3d features multiple statistical cues significantly reduces memory consumption computation complexity proved efficient work leverage rasterized event point cloud input three different backbones pointnet dgcnn point transformer two linear layer decoders predict location human keypoints find pointnet achieves promising results much faster speed whereas point transfomer reaches much higher accuracy even close previous eventframebased comprehensive set results demonstrates consistently effective 3d backbone models eventdriven human pose estimation pointnet 2048 points input achieves 8246mm mpjpe3d dhp19 dataset latency 1229ms nvidia jetson xavier nx edge computing platform ideally suitable realtime detection event cameras code available https githubcom masterhoweventpointpose © 2022', 'human pose estimation always popular research topic discipline computer vision aims predict spatial position human bodys key points partsjoints given image video benefiting powerful feature representation ability convolutional neural networks pose estimation deep learning become mainstream task obtain human pose estimation results extracting human pose feature information different scales specific neural networks corresponding process paper depth study human body posture estimation divided singletask multiperson missions time correlation classified summarized respectively network architecture algorithm feature extraction performance analyzed current problems existing research field summarized research prospect future prospected © published licence iop publishing ltd', 'purpose automatically quickly analyze whether rope skipping actions conform standards give correct guidance training plans firstly aiming problem motion analysis deep learning dl obtain coordinates key points rope skipping openpose lightweight mobilenetv2 instead visual geometry group vgg 19 secondly multilabel classification model attention long shortterm memorylong shortterm memory alstmlstm according algorithm adaptive multilabel learning finally validity model verified analysis comparison simulation results results show average accuracy improved openpose 778 increase 33 alstmlstm model achieves 961 accuracy 965 precision feature extraction model vgg19 initial stage openpose replaced lightweight mobilenetv2 pose estimation accuracy improved number model parameters reduced additionally compared models performance alstmlstm model improved aspects work effectively solves problems realtime accurate analysis human pose estimation hpe simulation results show dl model effectively improve students high school entrance examination performance © 2022 mingqian li jing zhao', 'every person spends around one third hisher life bed infant young toddler percentage much higher bedbound patients go 100 time inbed pose estimation critical step many human behavior monitoring systems focused prevention prediction management atrest sleeprelated conditions adults children topic automatic noncontact human pose estimation received lot attentionsuccess especially last years computer vision community thanks introduction deep learning power artificial intelligence ai modeling however sota visionbased ai algorithms field hardly work challenges associated inbed human behavior monitoring significant illumination changes eg full darkness night heavy occlusion eg covering sheet blanket well privacy concerns mitigate largescale data collection necessary deep learningbased model training data quality challenges privacy concerns hindered advanced visionbased inbed behavior monitoring systems home recent covid19 pandemic could effective way control spread virus avoiding inperson visits clinics © 2022', 'existing human pose estimation needed predict either dense heatmap requires complex model architecture execute longdistance regression makes model unable achieve good performance work accurate efficient deep learning human pose estimation called cooperative localization sparse heatmap find keypoint ’ approximate location two shortdistance offsetmaps obtain precise coordinates realize construct two types cooperative localization networks clnetresnet clnethourglass evaluate networks three benchmark datasets leeds sports pose dataset mpii human pose dataset coco keypoints detection dataset experimental results show consistently improves average precision plain counterparts clnetresnet50 outperforms simplebaseline 114 12 gflops clnethourglass outperforms original stackedhourglass 445 coco © 2022 springer nature singapore pte ltd', 'information transmission underwater conditions depend various media thus autonomous underwater vehicle auv imperative assist divers conducting underwater operations information transmission however little work done promote interaction auv divers help resolve issue paper help navigation guidance auv predicting divers ’ headings work visual chosen estimate pose divers since adapt complex underwater environment conditions lower cost contour information used obtain keypoints diver input longshort term memory network predict headings dataset redefines skeleton divers work original open future researches compared original accuracy pose estimation dataset 85 increase 9 small error diver ’ heading prediction indicates obtain competitive prediction results © 2021 authors exclusive licence springerverlag london ltd part springer nature', 'videobased human pose estimation vhpe vital yet challenging task deep learning algorithms made tremendous progress vhpe lots approaches task implicitly model longrange interaction joints expanding receptive field convolution designing graph manually unlike prior design lightweight plugandplay joint relation extractor jre explicitly automatically model associative relationship joints jre takes pseudo heatmaps joints input calculates similarity way jre flexibly learn correlation two joints allowing learn rich spatial configuration human poses furthermore jre infer invisible joints according correlation joints beneficial locating occluded joints combined temporal semantic continuity modeling relationbased pose semantics transfer network rpstn videobased human pose estimation specifically capture temporal dynamics poses pose semantic information current frame transferred next joint relation guided pose semantics propagator jrpsp jrpsp transfer pose semantic features nonoccluded frame occluded frame rpstn achieves competitive results videobased penn action subjhmdb posetrack2018 hieve datasets moreover jre improves performance backbones imagebased coco2017 dataset code available httpsgithubcomyhdangposeestimation © 19922012', 'human pose estimation capturing collection coordinates joint arm head torso etc may used characterize persons pose initial goal create skeletonlike depiction human body processed taskspecific applications ability identify estimate position human body valuable wide range applications conditions like action recognition animation gaming crucial first step toward understanding people images media study graph neural networks utilised predict human poses modelling human skeleton unordered list greatly enhancing 3d human pose estimation paper describes efficient way determine 3d posture many persons picture model gives validation accuracy 92 © 2022', 'automated analysis mouse behaviours crucial many applications neuroscience however quantifying mouse behaviours videos images remains challenging problem pose estimation plays important role describing mouse behaviours although deep learning made promising advances human pose estimation directly applied pose estimation mice due different physiological natures particularly since mouse body highly deformable challenge accurately locate different keypoints mouse body paper novel hourglass network model namely graphical model structured context enhancement network gmscenet two effective modules ie structured context mixer scm cascaded multilevel supervision cmls subsequently implemented scm adaptively learn enhance structured context information mouse part novel graphical model takes account motion difference body parts cmls module designed jointly train scm hourglass network generating multilevel information increasing robustness whole network multilevel prediction information scm cmls develop inference ensure accuracy localisation results finally evaluate several baselines parkinsons disease mouse behaviour pdmb standard deeplabcut mouse pose datasets experimental results show achieves better competitive performance approaches © 19912012', 'human posture estimation hpe aims detect human body parts generate demonstration human body incoming data images video hpe lately gained much attention shown numerous applications real world human posture estimation may benefit deep learning view study aims examine assess deep learningbased human posture estimation algorithms describe analyze recent research multiple deep learningbased human posture estimation approaches openpose vitposeb hrnet alphapose densenet efficientpose densepose hourglass 4rsn50 deployed coco mpii datasets paper examined human posture evaluation measures like average precision ap probability correct key points pck therefore vitposeb outperform ap coco dataset hourglass achieves better pck mpii comparative analysis helps understand applicability different techniques human pose estimation furthermore unresolved issues future research challenges human pose mentioned © 2022', 'recent years human pose estimation deep learning actively studied various applications large amount training data required achieve good performance annotating human poses quite expensive task therefore growing need improve efficiency training data preparation paper take active learning reduce cost preparing training data human pose estimation active learning automatically selects images effective improving performance human pose estimation model unlabeled image sequences focusing fact human pose continuously changes adjacent frames image sequence specifically comparing estimated human poses frames select images incorrectly estimated candidates manual annotation human pose estimation model retrained adding small portion manually annotated data training data experiments confirm effectively select training data candidates unlabeled image sequences improve performance model reducing cost manual annotations © 2022 spie', 'human pose estimation prediction many applications autonomous vehicles video games development animation security many instances humans recorded video two dimensions twodimensional representation requires uplifting three dimensions fully utilised paper lifting twodimensional skeleton representations human movement three dimensional skeleton representations sequence human action 2d 3d uplift builds work hpgan 1 utilising generative adversarial network gan recurrent neural network encoder decoder generator multilayer fully connected neural network critic novel adds random noise normal distribution z dimension joint custom loss function consisting joint position 3d space bone length algorithm ganuplift successfully uplifts 2d motion sequences respective 3d motion sequences sequence mean joint accuracy 309mm outperforms several within 04mm best models human36m skeleton dataset addition uplift single pose sequence pose input ganuplift uplifts sequence human poses rather single pose © 2022', 'deep neural networks dnn widely applied many computer vision problems tasks often conducted input images high quality without consideration storage transmission costs making necessary compress images bandwidthconstrained networks recently researches deep learning image compression show promising performance compared traditional image compression codecs however image compression approaches focus improving userperceived visual quality rather achieving high dnn inference accuracy computer vision work design concrete system goal maximizing computer vision performance metric subject compression ratio constraint entire efficiently optimized endtoend manner without multiple training phases find conventional distortion metric mean squared error mse compression suffice get desirable computer vision performance system essential exploit machinecentric evaluation metrics high inference accuracy also apply classagnostic object masks combined channel attention mechanism dynamically allocate bits regions interest roi background regions bg optimize computer vision performance range bitrates experiment three diverse applications separately image classification human pose estimation semantic segmentation extensive experiments show outperforms many traditional compression codecs image compression also achieves superior computer vision performance counterparts © 2022 elsevier bv', '3d human pose estimation frequently seen task estimating 3d poses relative root body joint alternatively 3d human pose estimation camera coordinates allows effective combination 2d annotated data 3d poses straightforward multiview generalization end cast problem view frustum space pose estimation absolute depth prediction joint relative depth estimations disentangled final 3d predictions obtained camera coordinates inverse camera projection also present consensusbased optimization algorithm multiview predictions uncalibrated images requires single monocular training procedure although indirectly tied training camera intrinsics still converges cameras different intrinsic parameters resulting coherent estimations scale factor improves state art well known 3d human pose datasets reducing prediction error 32 common benchmark also reported results absolute pose position error achieving 80 mm monocular estimations 51 mm multiview average source code available httpsgithubcomdluvizon3dposeconsensus © 2022 authors exclusive licence springer sciencebusiness media llc part springer nature', 'despite advancements improve patient safety significant number errors still occur operating suites os improve medical decisionmaking resulting quality care essential monitor understand medical activities ’ workflow interactions although strategies employ different sensor devices focus generating complete workflow information combine different data sources generate final output lacking least one piece information workflow tackle challenge paper presents formula presented distributed architecture model sensor data acquisition processing formula presented ’ main contribution lies multisensor data fusion algorithms extract computational representation activities surgical procedures addition formula presented flexible accommodate different deployment configurationscombining depth cameras ultrawideband positioning systemsand deep learningbased human pose estimation hpe mechanisms workflow monitoring mechanism deployed actual hybrid os extensive evaluation proposal experiments demonstrate architecture capture information required monitor surgical workflow particular hpe methodology accurately detects poses medical staff members maximum error 5cm © 2022 elsevier bv', 'human pose estimation key step understanding human behavior images videos bottomup human pose estimation difficult predict correct pose person large scenes due challenge scale variation paper twostage hierarchical network first acquires images large scenes sends tracking command signals twodegreeoffreedom shooting platform equipped image sensor track moving target motion target detection frame locally constrains captured image stream according topdown target detection algorithm retain content related motion target image processed images fed generalized human pose estimation model pose detection deployed algorithm twodegreeoffreedom filming platform equipped camera equipment deployed experimental platform sport scenes conduct detection experiments sport figures running ski jumping sport scenes sport figure nearby area roi region generate pictures videos skeleton pose sport target guide sport training target figure investigation solve challenge scale variation extent bottomup multihuman pose estimation especially large scenes person key points located accurately experiments show investigation meet practical requirements speed accuracy sport figure pose detection large scenes daily sports © 2022 spie', 'visionbased human joint angle estimation essential remote continuous health monitoring visionbased angle estimation locations human joints extracted optical motion cameras depth cameras human pose estimation models study aimed reliable straightforward deep learning networks knee elbow flexionextension angle estimation rgb video fifteen healthy participants performed four daily activities study experiments conducted four different deep learning networks networks took nine subsequent frames input output knee elbow joint angles extracted optical motion capture system frame bilstm networkbased joint angles estimator estimate joint angles correlation 0955 knee 0917 elbow joints regardless camera view angles © 2022', 'work addresses multiview multiperson 3d pose estimation synchronized calibrated camera views recent approaches estimate neural network weights supervised way rely ground truth annotated datasets compute loss function optimize weights network however manually labeling ground truth datasets laborintensive expensive prone errors consequently preferable rely heavily labeled datasets work unsupervised estimating 3d human poses requiring offtheshelf 2d pose estimation intrinsic extrinsic camera parameters reprojection error loss function instead comparing predicted 3d pose ground truth first estimate 3d pose person plane sweep stereo depth 2d joint related person estimated selected target view estimated 3d pose projected onto views camera parameters finally 2d reprojection error image plane computed comparing estimated 2d pose corresponding person 2d poses correspond person identified virtual depth planes 3d pose projected onto reference view compared find nearest 2d pose learns estimate 3d pose endtoend unsupervised manner require manual parameter tuning yet achieved results close supervised public dataset achieves 58 points fully supervised 51 points best geometric campus dataset © 2022 authors exclusive license springer nature switzerland ag', 'detect occurrence abnormal human behaviors video surveillance real time accurately recognition algorithm abnormal human behavior pose estimation paper first human pose estimation algorithm deep learning used extract coordinates key points human skeleton form spatial temporal graph model containing spatial information time series information node model corresponded joint human contained two types edges time one space edge conformed natural connectivity joints temporal edge across continuous time spatial temporal graph subjected multistage spatial temporal graph convolution operations extract advanced features finally softmax classifier used behavior classification behavior result obtained whether abnormal behavior judged compared current advanced experimental results kth singleperson dataset hmdb51 multiperson interaction dataset show accuracy better realtime video tested frame rate realtime detection recognition reached 25 frames per second realize realtime monitoring video processing © 2022 editorial department journal beijing university technology right reserved', 'research human pose estimation remains fundamental challenging problem computer vision context computer visionbased automobile safetyassisted driving technology received comprehensive attention widely used automobileassisted driving systems powered advanced artificial intelligence scale data learned deep learning networks increasing processing tasks becoming complex model parameters also increasing field distributed parallel computing research also received extensive attention work focus challenge human posture estimation two dimensions conventional deep learning techniques emphasis studys potential trafficrelated settings recent approaches estimating human poses introduced summary extract bone joint points deep learning techniques provided conclusion elaborates present state going beyond bounds singlecomputer computing resources merging distributed architecture deep learning address challenges opportunities human pose estimation human posture estimation several technological applications including detection driving behaviour analysis fitness data © 2022', 'aiming shortcomings existing fall recognition weak adaptability less reliable recognition rate scenes paper improve reliability fall recognition via morphological analysis processes video obtained camera gain human skeleton data morphological analysis firstly calculates aspect ratio human detection frame secondly human body falls certain speed computes speed human movement thirdly calculates relative distance buttocks heel human body continuity fall reliability fall recognition improved eliminating false alarm conditions satisfied considered fall experiment result shows paper reliable recognition fall movements © 2022', 'study quantitative evaluation monocular video dance movements deep learning provides scientific quantitative dance movement evaluation system help students dance teachers quickly identify judge dance movement standards paper neural networkbased 3d human pose estimation used realize identification collection key points video human pose skeleton aiming problem difficult users ensure time consistency reference video actual video acquisition process video frame alignment ant colony algorithm aiming problem large coordinate displacement caused testers height fat thinness different video shooting angles evaluation analysis moving human body posture similarity matching feature planes used solve problem scores input movements user video benchmark video output time provided dance learners students nonstandard dance movements identified timely effective manner evaluation results fed back students friendly way students correct dance movements time achieve better training results © 2022', 'originated india yoga considered spiritual practice brings flexibility balance harmony physical mental health becomes art healthy living variety positions also known asanas exercised designed provide particular benefit body contrast incorrect action yoga session harmful muscles ligaments people comfortable home workout need instructor assess accuracy movement posture turned need autoguiding human pose estimation important field research computer vision serves several applications extending health monitoring public safety tackles multiple challenges related human posture used identify yoga asanas study develop deeplearning selfinstruction yoga classifier named yopose pose information helps individuals improve yoga postures providing personalized feedback public dataset including six asanas used train evaluate model introduction transfer learning data augmentation scheme helped achieve promising results 98 accuracy © 2022', 'due diversity human body posture problems occlusion key points difference target scale background blur among people therefore multihuman pose estimation still challenging task existing deep learningbased multibody pose estimation mainly divided topdown bottomup make full local features network paper convolutional block attention modulecbam focal l2 loss used process context information convolutional neural network consolidate local features specifically attentioncontaining stacked hourglass network ashn ashn stacked hourglass network addition convolutional block attention module cbam module improve performance combined focal l2 loss model compared existing achieves competitive performance achieving 668 ap 721 ap75 654 apm coco data sets © 2022', 'background markerless systems digital video cameras deep learning gait analysis could deep impact clinical routine recently developed system shown promising results terms joint center position yet evaluated terms gait outcomes research question novel markerless system compare markerbased reference system terms clinically relevant gait parameters deep learning behind developed markerless system trained dedicated dataset consisting fortyone asymptomatic pathological subjects performing ten walking trials system could estimate threedimensional position seventeen joint centers keypoints eg neck shoulders hip knee ankles evaluated markerless system markerbased system terms differences joint position euclidean distance detection gait events eg heel strike toeoff spatiotemporal parameters eg step length time kinematic parameters eg hip knee extensionflexion intertrial reliability kinematic parameters results markerless system able estimate threedimensional position joint centers mean difference 131 mm sd 102 mm 99 estimated gait events estimated within 10 ms corresponding reference values estimated spatiotemporal parameters showed zero bias mean standard deviation differences estimated kinematic parameters varied parameter example mean standard deviation knee extension flexion angle −30° 27° intertrial reliability measured parameters similar markerbased references significance developed markerless system measure spatiotemporal parameters within range minimum detectable changes obtained markerbased reference system moreover except hip extension flexion system showed promising results terms several kinematic parameters © 2022 elsevier bv', 'deep learningbased image dehazing made great progress still many problems inaccurate model parameter estimation preserving spatial information unetbased architecture address problems image dehazing network highresolution network called dehrnet highresolution network originally used human pose estimation paper make simple yet effective modification network apply image dehazing add new stage original network make better image dehazing newly added stage collects feature map representations branches network upsampling enhance highresolution representations instead taking feature maps highresolution branches makes restored clean images natural final experimental results show dehrnet achieves superior performance existing dehazing synthesized natural hazy images © 2022 authors licensee mdpi basel switzerland', '3d human pose estimation plays important roles various humanmachine interactive applications efficiently utilize joint structural global local features human pose deeplearningbased always challenge paper parallel structural global local joint features fusion network inspiring observation pattern human pose specific observed common similar global features local features human pose cross actions therefore design globallocal capture modules separately capture features finally fuse parallel global local joint features fusion network entitled jointfusionnet significantly improve models intrascenario h36m crossscenario 3dpw datasets lead appreciable improvements poses similar local features notably yields overall improvement 34 mm mpjpe relative 68 improvement previous best feature fusion 22 h36m dataset 3d human pose estimation © 2022 authors exclusive license springer nature switzerland ag', 'despite encouraging results achieved human pose estimation recent years remains challenging problems background similar human body parts small persons lowresolution image performance may degrade dramatically paper addresses problems backgroundinference smallperson pose estimation achieve novel pose estimation algorithm basis person semantic segmentation deep neural network different previous single pose estimation model generate mixture models pose estimation semantic segmentation introduce novel generative adversarial model auxiliary model realize semantic segmentation network handle confusion similar regions background addition address problem scale differences big small persons keypoints add additional position channel attention modules first two stages openpose conduct extensive experiments coco voc datasets compare popular human pose estimation semantic segmentation including multiposenet deterton2 deeplab v3 experimental results show accurate algorithms performs effectively tackling complex situations © 19942012', 'human pose estimation hpe longstanding yet challenging task computer vision nature problem requires comprehensive global contextual reasoning among joints different locations work explore incorporate two popular effective concepts selfattention graph neural network gnn model longrange information hpe three different ways implement selfattention 3d feature maps studied best result achieved via channelposition version accuracy improved refining queries via efficient channelwise parallel gnn explicitly models human joint graphical relationships able improve prediction accuracy strong baseline models achieve results © 2022', 'among current human pose estimation mature one twodimensional 2d human pose estimation however 2d information difficult reflect real posture human body space threedimensional 3d estimation algorithm imperfect poor accuracy paper fusion binocular stereo vision convolutional neural network cnn new 3d human pose estimation carried order verify accuracy used paper microsoft kinect v2 capture action images reconstruct 3d pose human body results showed within working distance 44504700mm minimum root mean square error rmse minimum mean absolute error mae minimum mean absolute percentage error mape action performed testers could reach 20469 16408 4508 length accuracy human joints restored higher kinect v2 restoration human actions 3d space higher concluded experimental results might ability reduce cost limitation current human motion capture may applied fields stroke patient rehabilitation community rehabilitation future © 2022', 'research human pose estimation recently promoted new high degree result existing widely used flawed theory must rethought researchers focus enhancing network structure data processing details yet neglect study encodingdecoding keypoint coordinate paper rethink recent encodingdecoding new elegant reliable one referred leastsquares estimation keypoint coordinate lsec plugin conveniently used recent sota human pose estimation models lsec mathematically rigorous unbiased compensate inherent bias introduced existing encodingdecoding besides lsec greatly improves robustness gaussian heatmap human pose estimation adversarial attack noise experiments demonstrate effective performance robustness release source code later © authors exclusive license springer nature switzerland ag 2022', 'precise location personnel indoors one crucial prerequisite data analytics workforce productivity well workplace safety health paper enhanced indoor occupancy tracking system optical camera communication occ top conventional surveillance cameras system able track workers carry unique infrared led beacons indoor occ infrared beacons adopted identity check localization accuracy enhanced deeplearningbased human pose estimation infer footfall location particularly comprehensive footfall estimation algorithm empowered deeplearningbased human pose estimation presented provides precise footfall estimation considering realtime actions person occlusion patterns order validate accuracy improvement experiment conducted open lab area 30 m2 single surveillance camera sensors according experimental results localization accuracy improved 936 average 4419 far end comparison conventional © 2022', '164 special focus conference artificial intelligence topics audiovisual fusion network conformer multimodal emotion recognition audiovisual multiperson keyword spotting via hybrid fusion new adaptive tvbased bm3d algorithm image denoising datadriven hybrid neural network modeldriven supervised learning structural dynamic impact localization browsing behavioral intent prediction product recommendation pages ecommerce platform connecting patients prediagnosis multiple graph regularized mental disorder diagnosis novel devicefree localization deep dictionary learning survey hypergraph neural networks application action recognition visual perception inference raven ’ progressive matrices semisupervised contrastive learning phn parallel heterogeneous network soft gating ctr prediction multirelational cognitive diagnosis intelligent education clinical phenotyping prediction via auxiliary task selection adaptive sharedspace correction multiview subspace clustering joint tensor representation indicator matrix learning weighted competitivecollaborative representation classifier imbalanced data classification hierarchical graph representation learning structural attention graph classification optical satellite controller diffractive deep neural network photovoltaic hot spots detection kernel entropy component analysis information gain secondorder global attention networks graph classification regression aeroengine remaining useful life prediction via tensor decomposition figci flowbased informationgeometric causal inference emotional semantic neural radiance fields audiodriven talking head multiview 3d morphable face reconstruction via canonical volume fusion dpit dualpipeline integrated transformer human pose estimation integrative analysis multiview histopathological image features diagnosis lung cancer children autism looking reality study practical utility image dehazing algorithms deep learning computer vision scene understanding preface', 'upper limb kinematic analysis employed clinical assessment motion functions rehabilitation training traditionally tested manually goniometer nowadays trend deploy different technology devices including lowcost accurate rgb cameras order save manual efforts among new deep learningbased cameras investigated provide ease accessibility manual handheld goniometer key measuring upper limb range motion rom camera estimate upper limb joints accurately many existing joint estimation algorithms focus improving accuracy performance put efficiency concerns aside still challenging apply algorithms lowcapacity budgetfriendly devices highly demanding clinical scenarios lightweight fast deep learning model estimate human pose predicted joints measure range motion upper limb joints unlike human pose estimation learn predict major joints human body model focuses upper limb improves accuracy reduces overhead prediction reduce model size latency model compact neural network architecture parameters network quantized 8bit precision result model runs 41 times faster 155 times smaller compared full sized state art human pose estimation model evaluated different upper limb functional tasks results show new achieves satisfying accuracy rom measurement high degree agreement goniometer compared goniometer measure rom presented easier operate performed remotely still retaining good accuracy © 2022', 'era deep learning human pose estimation multiple cameras unknown calibration received little attention date show train neural model perform task high precision minimal latency overhead model takes account joint location uncertainty due occlusion multiple views requires 2d keypoint data training outperforms classical bundle adjustment weaklysupervised monocular 3d baselines wellestablished human36m dataset well challenging inthewild skipose ptz dataset © 2022', 'nearly existing human pose estimation techniques address problem lineofsight los setting many reallife applications rescue missions autonomous driving contrast require estimating pose hidden subjects paper present nonlineofsight nlos pose estimator produces skeletal representation hidden human poses bruteforce would first conduct albedo reconstruction hidden subject apply los pose estimation show implementation effectively exploit features unique nlos subsequently yields artifacts missing joints instead first generate comprehensive nlos human pose dataset 19 subjects 9 motions present spatially aware deep learning technique convolutional neural networks explicitly employ nlos features comprehensive experiments synthetic real data show new estimator effective robust seamlessly integrated learningbased nlos scene reconstruction hiddenpose transient dataset contains synthetic transients groundtruths volumes joints realworld transients captured nlos imaging system extensive assessments demonstrate hiddenpose transient dataset valuable effective nlos research make data code publicly available © 2022', 'human pose estimation gained significant attention researchers present era personal exercise sessions monitored supervised help pose recognition existing work exercise classification primarily relies external wearable sensors recognizing poses however sensors often fail differentiate amongst similar exercises essential extensions human pose estimation activity detection activity prediction paper first classify individual ’ exercises predict whether pose corresponding exercise correct tasks mentioned performed help 2dimensional pose coordinates used rgb camera capture poses exercises performed individuals formulate model 2d coordinates obtained 2d pose consider 2d coordinates 18 joints human body primary features classify different exercises predict correctness poses developed benchmark dataset consisting human subjects various age groups varying heights accuracy 9701 obtained better existing work tested dataset © 2022 authors exclusive licence springer sciencebusiness media llc part springer nature', 'deep regression models widely employed solve computer vision tasks human age pose estimation crowd counting object detection etc another possible area application knowledge systematically explored far proportion judgment prerequisite successful decision making individuals often proportion judgment strategies estimate magnitude one stimulus relative another larger stimulus makes estimation problem interesting application machine learning techniques regard various deep regression architectures tested three original datasets different origin composition novel assumption model learn concept proportion without explicitly counting individual objects comprehensive experiments demonstrated effectiveness models predict proportions reallife datasets reliably human experts considering coefficient determination 095 amount errors mae 2 rmse 3 significant number errors determining ground truth appropriate size learning dataset additional reduction mae 014 achieved used datasets publicly available serve reference data sources similar projects © 2022 authors licensee mdpi basel switzerland', 'worldwide covid19 infections caused various problems throughout different countries case korea problems related demand medical care concerning wards doctors serious already slowly worsening problems korea covid19 pandemic paper direction developing system combining artificial intelligence technology limited areas require high expertise rehabilitation medical field improved korea prevention bedsores leg rehabilitation regarding introduction artificial intelligence technology medical related laws regulations quite limited actual needs domestic rehabilitation doctors advice hospital environment obtained satisfaction test content high degree provision important medical data 95 angular error within 5 degrees suitable recovery confirmation © 2022 authors licensee mdpi basel switzerland', '70 topics discussed deep learning onstreet parking violation prediction monocular weaklysupervised camerarelative 3d human pose estimation image driven optimal personalized route recommendation parting illusions synthetic data hunis highperformance unsupervised nuclei instance segmentation electric load demand forecasting greek energy market lightweight neural networks rain estimation smart city ’ eband links context enhanced traffic segmentation traffic jam road surface segmentation aerial image temperature estimation fusion devices machine learning techniques infrared specular synthetic data detection powerline elements efficient transformers dynamic tomography reconstruction projectiondomain separable modeling contextaware memory attention network videobased action recognition drone footage wind turbine surface damage detection virtual validation multiobject tracker intercamera tracking automotive fisheye surround view systems', 'pose estimation detection human action poses important task computer vision requirements tasks unmanned driving intelligent surveillance also contributed development human pose estimation context deep learning dl remarkable impact human pose estimation study investigate pose estimation algorithm called convolutional pose machines cpm implement mobile platform ie android combining classical cpm mobile deep learning mobile ai compute engine mace model deployed mobile platform estimate human pose directly locally mobile phone without relying internet thus achieve effective performance protecting users personal data experimental results realworld data validate system achieved expected results accurate recognition common actions simple interface © published licence iop publishing ltd', '3d human pose estimation always important task computer vision especially crowded scenes multiple people interact many stateofthearts object detection single view however recovering location people complicated crowded occluded scenes due lack depth information single view lack robustness multiview human pose estimation multiperson became effective previous multiview 3d human pose estimation attributed strategy associate joints person 2d pose estimation however incompleteness noise 2d pose inevitable addition associate joints challenging solve issue ctp center point pose network multiview directly operates 3d space 2d joint features cameras projected 3d voxel space ctp network regresses center one person location 3d bounding box activity area one person ctp network estimates detailed 3d pose bounding box besides ctp network nonmaximum suppression free stage regressing center one person makes efficient simpler outperforms competitively several public datasets shows efficacy center point pose network representation © 2022 liu et al open access article distributed terms creative commons attribution license permits unrestricted distribution reproduction medium provided original author source credited', 'prevalence wearable devices inertial measurement unit imu data utilized monitoring assessment human mobility human activity recognition har training deep neural network dnn models tasks require large amount labeled data hard acquire uncontrolled environments mitigate data scarcity problem design cromosim crossmodality sensor simulator simulates high fidelity virtual imu sensor data motion capture systems monocular rgb cameras utilizes skinned multiperson linear model smpl 3d body pose shape representations enable simulation arbitrary onbody positions dnn model trained learn functional mapping imperfect trajectory estimations 3d smpl body trimesh due measurement noise calibration errors occlusion modeling artifacts imu data evaluate fidelity cromosim simulated data utility data augmentation various har datasets extensive experiment results show model achieves 67 improvement baseline har task © 2022', 'nonlineofsight technology image objects hidden cameras view wide range application prospects robotic vision national defense remote sensing medical imaging unmanned driving active nonlineofsight imaging mainly relies timeresolved optical impulse responses nonlineofsight imaging system emits ultrashort light pulses illuminate diffuse reflection wall ultrafast timeresolved singlephoton detectors collect multiple reflected photon information thereby obtaining information hidden scene finally various reconstruction algorithms used reconstruct hidden scene however existing reconstruction algorithms problems slow reconstruction speed fuzzy reconstruction results especially aspect human pose estimation article describe active nonlineofsight human pose estimation deep learning order solve problem lack deep learning data simulate large amounts pseudotransient images network including various complex actions walking jumping turning bending back forth rotating confocal nonlineofsight imaging model train simulated transient images light cones transformation unet coding decoding network structure finally examine performance synthetic experimental datasets prediction results show estimate pose real measured nonview human pose data also significantly improve quality reconstruction © 2022 spie', 'human pose estimation basis human action recognition guarantee realize humancomputer interaction occlusion interference complex background different human scales great influence singlehuman pose estimation obtain semantic information reduce impact different human scales improved snhrnet deep learning network achieve accurate singlehuman pose estimation improved snhrnet network established hrnet network optimized nhrnet network obtained cutting lowresolution highlevel features hrnet compared hrnet parameters nhrnet network reduced attention mechanism missing basis se module integrated basic residual module make lack attention mechanism nhrnet network snhrnet network model comes finally public dataset used singlehuman pose estimation experiment results relevant experiment show snhrnet network effective singlehuman pose estimation © 2022', 'automatic grading yoga poses may help yoga selfpractitioners rectify poses follow correctly teacher existing wearable depth sensor require user equip additional devices paper presents automatically grading yoga poses images taken conventional rgb cameraphone camera consists three main phases first estimate human joints rgb images blazepose model second investigate various deep models select vgg16 model yoga pose recognition finally define score takes difference angles important joints yoga pose label account solution lowcost easy light implement mobile devices gives confident score yogapose dataset selfcollected dataset © 2022', 'human pose estimation active research area computer vision received great attention till date motion tracking activity recognition applications utilize pose estimation define human pose estimation localization human joints joints called key pointselbows wrists etc well labeling data comprising either images videos chapter describe fundamentals human pose estimation study different research approaches available literature discuss different categories within pose estimation mention key differences chapter begin reporting classical pose estimation human pose recognition motion segmentation well deep learningbased explored recent years chapter also touch upon drawbacks classical models evolution convolutional neural networks developed overcome shortcomings © 2022 scrivener publishing llc', 'computerassisted rehabilitation environment caren system plays important role training rehabilitation patients capture patients 3d pose gait critical assessing patients requirements effective training visionbased highly effective task due low cost high speed noninterference although various general visionbased pose estimation developed recently performance limited caren system due specific environment address problems improved accurate 2d 3d pose estimation caren system multiview videos first 2d pose estimation coarsetofine heatmap shrinking cfhs strategy gradually reduces kernel size heatmap joints training improve performance second obtain 3d pose estimations novel spatialtemporal perception network fuses 2d results multiple views multiple moments multiview early fusion complementary spatial information different views multimoment late fusion leverages temporal information sequential input higher accuracy experimental results caren videos 225 orthopedic patients showed accuracy 2d human pose estimations cfhs training strategy reached 9985 pckh05 3d results mean per joint position error 2522 mm 3dpck reached 9871 outperformed existing general videobased results showed system capable estimating human poses high accuracy clinical applications © 2013', 'long time situation students learning physical education pe optimistic especially basic movement learning class lacked effective online learning tools indepth research deep neural network rapid development computer hardware artificial intelligence technology deep learning performed well field basic teaching therefore paper intelligent teaching system basic movements pe designed first information coordinate points collected according gaussian model pose students estimated openpose second overall architecture functional modules system designed finally deviation limbs affect standard overall movements identified matching algorithm realises evaluation feedback basic movements pe teaching system teachers obtain learning situation students movements students adjust movements feedback achieves convenient interaction pe teaching © 2022 xiaodi liu published sciendo 2022', 'garment detection complex image processing task multitude applications industry retrieval similar garments artificial intelligencepowered fashion recommendation models automatic labeling catalogs retailers fashion stores benefit knowing vital information types garments customers interested thus ensure profitable business model chapter novel detection garments interest footage surveillance camera video frames processed gmg background subtraction model obtain relevant foreground information along foreground masks mask rcnn object detection model used identify customers multiple image processing techniques used obtain active garments frames detected customers tracked openpose human pose estimation utilized obtain useful landmarks garments interest determined filtration confidence scores calculated active garment tested cctv video dataset found effective despite facing arduous obstacles background noise occlusions © 2022 scrivener publishing llc', 'order identify dangerous behaviors fighting public area deep learning model combines human post estimation action analysis behavior recognition model takes video frame sequence input first make hrnet backbone network detect human body joints generate human pose frame sequence make recurrent neural network action analysis pose frame sequence thus determine whether dangerous behavior video first part optimization made hrnet reduce parameter improve accuracy joint location second part introduced cubic lstm comprehensive model action recognition analysis motion human joints temporal sequence spatial sequence thus achieve better inference score experiments show recognition accuracy reach 928 last part dangerous behavior recognition system developed model monitoring host analysis captured video frames cameras identify dangerous behavior automatically thus serve public security tasks © 2022 acm', 'musculoskeletal disorders unavoidable occupational health problem particular workers perform repetitive tasks onsite manufacturing industry suffer musculoskeletal problems paper system evaluates posture workers manufacturing industry singleview 3d human poseestimation estimate posture 3d rgb camera easily acquire posture worker complex workplace system builds duckyangauto worker health safety environment dywhse manufacturingindustryspecific dataset estimate wrist pose evaluated rapid limb upper assessment rula additionally evaluate quality built dywhse dataset human36m dataset applicability system verified comparing evaluation results experts system provides quantitative assessment guidance working posture risk assessment assisting continuous posture assessment workers © 2022 authors', 'hardjoint localization human pose estimation challenging task reasons disappearance joint points caused clothing lighting shelter caused complex environment destruction dependence among joint point majority existing approaches hardjoint pose estimation achieve high accuracy obtaining highlevel feature information however networks suffer information loss caused downsampling would result loss joint location compensation information loss introduces useless information network learning affecting extraction useful information associated hard joints herein residual downsampling module replace pooling layer downsampling fuse highlevel features lowresolution feature maps module aims address information loss issue strategy guide network learning attention mechanism makes network focus useful feature information convolutional block attention module combined residual module outside basic subnetwork network learn effective highlevel features eightstack hourglass used basic network validated mpii lsp human pose dataset compared eightstack hourglass hrnet achieves higher accuracy hardjoint localization experimental results show effective hardjoint localization © 2021 authors exclusive licence springerverlag gmbh germany part springer nature', 'estimating 3d human pose single image challenging task work attempts address uncertainty lifting detected 2d joints 3d space introducing intermediate state partcentric heatmap triplets hemlets shortens gap 2d observation 3d interpretation hemlets utilize three jointheatmaps represent relative depth information endjoints skeletal body part convolutional network convnet first trained predict hemlets input image followed volumetric jointheatmap regression leverage integral operation extract joint locations volumetric heatmaps guaranteeing endtoend learning despite simplicity network design quantitative comparisons show significant performance improvement bestofgrade eg 20 percent human36m naturally supports training inthewild images weaklyannotated relative depth information skeletal joints available improves generalization ability model validated qualitative comparisons outdoor images leveraging strength hemlets pose estimation design append shallow yet effective network module regress smpl parameters body pose shape term entire hemletsbased human pose shape recovery pipeline hemlets posh extensive quantitative qualitative experiments existing human body recovery benchmarks justify results obtained hemlets posh © 19792012', 'automatic monkey pose estimation great potential quantitative behavior analysis monkeys provides indispensable information studies drug safety assessments medical trials development deep learning performance human pose estimation greatly improved however study monkey pose estimation rare robustness performance unsatisfactory due lack data variations monkey poses work complete solution address problems terms data methodology data collect comprehensive caged monkey dataset 6021 samples labeled poses methodology mask guided attention network mgan focus foreground target automatically locate occluded keypoints precisely deal complex monkey postures evaluated collected monkey dataset achieving 792 average precision ap 61 improvement baseline comparable performance human pose estimation hope attempt caged monkey pose estimation serve regular configuration drug safety assessments medical trials future © authors exclusive license springer nature switzerland ag 2022', 'person reidentification critical component target identification tracking perception systems par ticularly longterm target tracking required kinematic track may reliable identifying individual agnostic outward visual appearance particularly challenging problem plagues many existing reidentification models exist today one growing area research performing appearance ag nostic identification timesequence images identify individuals gait several performing gait identiffcation exist today existing require image preprocessing human pose estimation either requires human keypoint labels pretrained model may optimized type data variance would observed new scene example aerial perspective architecture performs gait classification person reidentification without need additional labels training concept posetransfer learns human pose estimation landmarks si multaneously gait encoder may used timesequence fingerprint person longterm tracking systems © 2022 spie rights reserved', '98 topics discussed explainability guided covid19 detection ct scans learning shape reconstruction sparse measurements neural implicit functions smunet style matching unet brain tumor segmentation missing modalities learning adaptive acquisition policies undersampled multicoil mri reconstruction negative evidence matters interpretable histology image classification interpretable interactive deep multiple instance learning dental caries classification bitewing xrays bridging gap point clouds merging neurons connectomics position regression unsupervised anomaly detection domain adaptation anatomical constraints 3d human pose estimation cover automatic planning liver tumor thermal ablation deep reinforcement learning torchxrayvision library chest xray datasets models', '137 topics discussed stochastic recursive gradient descent optimizationbased foreground features fisher vector evaluation aggregate morphological characteristics twodimensional digital image technique semantic segmentation road scene multiscale feature extraction deep supervision dynamic spectral–spatial multiscale feature extraction network hyperspectral image classification improved twostage detection model multitask model human pose estimation person detection ship detection optical remote sensing images saliency rotationinvariant feature defect detection plastic gears deep learning machine vision face tampering detection spatiotemporal attention residual network sentinel2 imagery detecting oil spills via spatial roughness mixed normalized difference index', 'address problem generalizability multiview 3d human pose estimation standard first detect 2d keypoints images apply triangulation multiple views even though existing achieve remarkably accurate 3d pose estimation public benchmarks limited single spatial camera arrangement number several address limitation demonstrate significantly degraded performance novel views stochastic human pose triangulation demonstrate superior generalization across different camera arrangements two public datasets addition apply fundamental matrix estimation problem showing successfully apply computer vision problems stochastic achieves 88 improvement 3d pose estimation task compared 30 improvement fundamental matrix estimation compared standard algorithm © 2022', 'motor functions individuals parkinsons disease pd studied many sensors 3d measurement devices smartphone applications deep learning tools openpose deep learning tool human pose estimation however studies focused gait arm swing analyzed separately healthy subjects furthermore none previous studies compared arm swing data measured video images openpose relevant items mdsupdrs study calculated thresholds distinguish normal abnormal gaits data healthy subjects compared peaktopeak pp data left right arm swing arm swing asymmetry asa openposebased gait analysis system developed previous study mdsupdrs scores showed 72738235 accuracy thus threshold normal abnormal gaits improved results conclude significant relationship mdsupdrs score magnitude gait arm swing angle measured openposebased gait analysis system study simpler easier clinical practice previous studies moreover videos timed upandgo tug tests used hope study enable estimation early detection pd symptoms simpler index © 2013', 'human pose estimation long researched significant topic computer vision however studies via deep learning models insufficient lack 2d 3d skeleton data various domains augmentation technique applied solve problem data scarcity data augmentation techniques improve performance analytical model increasing amount data however models performance degraded augmented results differ significantly actual distribution therefore necessary implement optimized augmentation policy image datasets study dimensional expansion timeseries data augmentation policy pose estimation skeletons improves models performance 3d skeleton data 3d skeleton data preprocessed affine transformation data augmented dimensional expansion 2d 3d data addition sampling applied data consideration timeseries features thus number frames per unit time redefined subsequently part information lost cutout thus data size rather data shape changed image video augmentation policies cutout expansion search candidates 3d timeseries data augmentation policies extracted number cases generated combination 16 skeleton augmentation policies 11 probabilities ten intensities finally 20 candidates extracted five bestperforming policies applied © 2013', '64 topics discussed flippingfree tabletop integral imaging large viewing angle spacemultiplexed voxel screen compound lensarray improved 3d human pose estimation model temporal convolution gaussian error linear units brain inspired keypoint matching 3d scene reconstruction throat modeling massspring unity 3d surgery training 3dmmbased deformation measurement face rehabilitation may remain seated pilot study impact reducing roomscale trainings seated conditions long procedural virtual reality trainings new parameter model consumerlevel virtual reality equipment evaluation virtual realitybased simulator needle interventions liver biopsy improved adaptive algorithm deep learning barzilaiborwein step size virtual reality financial trading investing review literature applications', 'pose estimation artificial intelligence computer vision human posture estimate advanced version pose estimation technology graphically depicts position orientation human body one appealing fields research gaining popularity thanks practicality versatility—utilized range industries including gaming healthcare agriculture augmented reality sports research project intends establish deep learningbased human posture identification system used identify diverse agricultural operations intention introducing concept automation agriculture field proprietary dataset farmer postures used run system picture dataset preprocessed deep neural network used detect body points image opencv creates graphical representation points angle body components crucial determining posture derived various calculations finally result compared threshold value processed model could accurately measure farmers humans posture three major categories sitting bending standing test accuracy 77 © 2022 authors exclusive license springer nature switzerland ag', 'human action recognition continues evolve improve deep learning techniques studies success field action recognition focused traditional dance dance actions especially traditional african dance long involve fast movements research novel applies data science algorithms field cultural preservation applying various deep learning techniques identify classify model traditional african dances videos traditional dances important part african culture heritage digital preservation dances multitude form challenging problem dance dataset constituted freely available youtube videos four traditional african dances used dance classification process adowa swange bata sinte dance five convolutional neural network cnn models used classification achieved accuracy 93 98 additionally human pose estimation algorithms applied sinte dance model sinte dance exported environments obtained © 2022 ownerauthor', 'paper various deep learning ai techniques evaluate detection prediction individual falling taken images video falls serious issue within medical sector falls often associated senior centers retirement homes falls responsible approximately 373 million injuries severe enough medical attention goal paper utilize human pose estimation detect predict falls may occur © 2022', 'advancement artificial intelligence computer vision assistive devices visually impaired people active research area last decade people visual disabilities face challenges detection recognition various human actions leads lack confidence constant dependency performing daily routine activities objective research recognize classify different human actions position posture body research captured video processed human pose estimation extraction 2d body skeleton openpose extracted body points process feature extraction pretrained vgg19 cnn classifies human actions svm classifier furthermore integrates voiceenabled feature deliver instructions classified human activities deep learning train test three different dataset weizmann dataset gesture human actions b kinetics dataset interaction human actions c ck dataset behaviour human actions accuracy classification different human actions reached 9308 9503 9312 f1 score reached 9359 9519 9366 respectively results indicate significance assistance visually impaired people © 2022 authors exclusive license springer nature switzerland ag', 'human pose estimation important task many realtime applications existing directly slim cnn deploying welldesigned lightweight modules however lack privileged information guidance knowledge distillation technique stays less explored work novel namely pyramid knowledge distillation pkd efficient human pose estimation specifically pkd composes pyramid structured map distillation psmd pyramid feature map distillation pfmd psmd formulate structured map encoding robust interjoint correlation structured map spatial dependencies keypoints better transferred cumbersome teacher network compact student model promote efficiency student pfmd used distill rich local global features teacher experiments demonstrate pkd achieves optimal tradeoff cost accuracy coco mpii benchmarks even much faster inference speed © 2022', 'gait defined individuals manner walking analysis provide significant information identity health opening wide range possibilities field medical diagnosis paper introduced innovated implement artificial intelligence classify human gait video sequence exploited modern library deep learning estimate human pose extract gait features analyses videos used conventional neural networks classify normal pathological gait used anaconda code python packages jupyter kerastensorflow mediapipe deeppose © 2022', 'purpose safety management construction machines primary importance considering traditional construction machine safety monitoring evaluation adapt complex construction environment monitoring sensor equipment cost much paper aims introduce computer vision deep learning technologies yolov5fastpose yfp model realize pose estimation construction machines improving alphapose human pose model designmethodologyapproach model introduced object detection module yolov5m improve recognition accuracy detecting construction machines meanwhile better capture pose characteristics fastpose network optimized feature extraction introduced singlemachine pose estimation module smpe alphapose study used alberta construction image dataset acid construction equipment poses dataset cepd establish dataset object detection pose estimation construction machines data augmentation technology labelme image annotation software training testing yfp model findings experimental results show improved model yfp achieves average normalization error ne 1294 × 10–3 average percentage correct keypoints pck 9848 average area pck curve auc 3750 × 10–3 compared existing model higher accuracy pose estimation construction machine originalityvalue study extends optimizes human pose estimation model alphapose make suitable construction machines improving performance pose estimation construction machines © 2022 emerald publishing limited', 'emergence artificial intelligence ai driven human pose estimation applications mobile smart devices could prove evolution identification overuse musculoskeletal disorders work sport developing human estimation pose models requires techniques computer vision subfield ai generates digital information images videos b machine learning statistical models algorithms understand digital images videos c deep learning subfield machine learning structures algorithms layers create artificial neural network ” human pose estimation identification human body joints twodimensional 2d images video compared goldstandard optical motion capture systems current human pose estimation single camera reached acceptable level accuracy laboratory measurement standards however improved attheedge computing hardware mobile smart devices increased bandwidth 5g cellular communications offer opportunity develop aidriven human pose estimation applications study provides survey technical challenges uncovered human pose estimation monocular single camera device principal findings related key technical challenges required learning pose estimation lack threedimensional training data b depth ambiguity c lack high frame rate datasets occlusion e lack diverse datasets f background clutter literature survey study present seven technical hypotheses could minimize key technical challenges human pose estimation model development © 2022 iise annual conference expo 2022 rights reserved', 'fundamental task computer vision human pose estimation hpe achieved significant improvement rise deep learning however many existing focus much model accuracy leading high complexity models hard deployed especially computationlimited devices paper lightweight hpe network named efficient highresolution human pose estimation ehrhpe ehrhpe network first adopts highresolution pattern acquire accurate heatmaps efficient shuffle block reduce model complexity boost model performance finally efficient dense connections designed improve model accuracy extensive experiment results two benchmark datasets show ehrhpe network achieves great tradeoff accuracy model complexity ehrhpe network achieve 701 mean average precision scores common objects context coco testdev dataset 17m parameters 091 gflops © 2022 authors exclusive license springer nature switzerland ag', 'sports videos blowing internet enriching material life higher pursuit spiritual life people thus automatically identifying detecting helpful information videos arisen relatively novel research direction accordingly present work human pose estimation hpe model automatically classify sports videos detect hot spots videos solve deficiency traditional algorithms firstly deep learning dl introduced amounts human motion features extracted region proposal network rpn next hpe model implemented deep convolutional neural network dcnn finally hpe model applied motion recognition video classification sports videos research findings corroborate effective accurate hpe model implemented dcnn recognize classify videos effectively meanwhile big data technology bdt applied count playing amounts various sports videos convinced hpe model dcnn effectively accurately classify sports videos provide basis following statistics various sports videos bdt finally new outlook apply new technology entertainment industry copyright © 2022 zhang tang zereg xu', 'crossview 3d human pose estimation model made significant progress better completed task human joint positioning skeleton modeling 3d multiview fusion multiview 2d pose estimation part model important training cost also high deep learning networks generate heatmaps view therefore article tested new deep learning networks pose estimation tasks deep networks mobilenetv2 mobilenetv3 efficientnetv2 resnet performance drawbacks networks built multiple deep learning networks better performance call network article lhpenets mainly includes lowspan network rdns network lhpenets network structure evenly distributed channels inverted residuals external residual blocks processing smallresolution samples achieve training saturation faster also designed static pose sample simplification 3d pose data implemented lowcost sample storage also convenient models read samples experiment used several recent models two public estimation indicators experimental results show superiority work fast startup network lightweight 15 epochs faster resnet34 training also show accuracy improvement work estimating different joints estimated performance approximately 60 joints improved performance overall human pose estimation exceeds networks 7mm experiment analyzes network size fast startup performance 2d 3d pose estimation model paper detail compared pose estimation models performance also reached higher level application © 2022 wang et al open access article distributed terms creative commons attribution license permits unrestricted distribution reproduction medium provided original author source credited', 'rapid developments computer vision deep learning technologies artificial intelligence takes important role sports analyses paper attain objective automated golf swing analyses lightweight temporalbased 2d human pose estimation hpe called golfpose achieves improved performance imagebased hpe unlike traditional imagebased temporalbased designed efficient effective golf swing analyses takes advantage temporal information improve estimation accuracy fastmoving partially selfoccluded keypoints furthermore order make sure golf swing analyses run mobile devices optimize model architecture achieve realtime inference around 10 parameters half gflops used hrnet golfpose model achieve 916 mean pixel error mpe golf swing dataset compared 920 mpe hrnet furthermore temporalbased facilitated golf club detectiongcd significantly improves accuracy keypoints golf club 1398 921 mpe © 2022', 'human pose estimation particularly focused hands important topic metaverses researchers deep learning increased accuracy keypoints detection applied deformable objects respect handcrafted human hand pose recognition color images encouraged new application cases paper focused visual hand keypoints estimation estimation real time deployable memory computationally constrained embedded devices particular implementing technology microcontrollers one challenging tasks since model must tiny enough deployable less one megabyte memory ensuring adequate accuracy paper image regression achieve proper key points approximation accuracy within required memory computation assets microcontroller quality projection complexity reliability solution validated comparative analysis several hyperparameters combinations trained adhoc quality measurements result modified memory optimized version mobilenetv2 developed required 073 mbytes flash memory 0385 mbytes ram memory percentage detected joints achieved equal 777 inference time example 398 ms 480mhz stm32h7 featuring arm cortex m7 instruction set © 2022', 'paper presents gopose 3d skeletonbased human pose estimation system wifi devices home system leverages wifi signals reflected human body 3d pose estimation contrast prior systems need specialized hardware dedicated sensors system require user wear carry sensors reuse wifi devices already exist home environment mass adoption realize system leverage 2d aoa spectrum signals reflected human body deep learning techniques particular 2d aoa spectrum locate different parts human body well enable environmentindependent pose estimation deep learning incorporated model complex relationship 2d aoa spectrums 3d skeletons human body pose tracking evaluation results show gopose achieves around 47cm accuracy various scenarios including tracking unseen activities nlos scenarios © 2022 acm', '3d human pose estimation describes estimating 3d articulation structure person image video technology massive potential enable tracking people analyzing motion real time recently much research conducted optimize human pose estimation works focused reviewing 3d human pose estimation paper offer comprehensive survey 3d human pose estimation referred pose estimation solutions implementations images videos different numbers people advanced 3d human pose estimation techniques furthermore different kinds algorithms subdivided subcategories compared light different methodologies best knowledge first comprehensive survey recent progress 3d human pose estimation hopefully facilitate completion refinement applications 3d human pose estimation © 2022 world scientific publishing company', 'human pose estimation hpe many wide applications multimedia processing behavior understanding humancomputer interaction previous studies encountered many constraints restricted scenarios rgb inputs mitigate constraints estimating human poses general scenarios present efficient human pose estimation model ie ehpe joint direction cues gaussian coordinate encoding specifically anisotropic gaussian coordinate coding describe skeleton direction cues among adjacent keypoints best knowledge first time skeleton direction cues introduced heatmap encoding hpe task multiloss function constrain output prevent overfitting kullbackleibler divergence introduced measure predication label ground truth one performance ehpe evaluated two hpe datasets ms coco mpii experimental results demonstrate ehpe obtain robust results significantly outperforms existing hpe lastly extend experiments infrared images captured research group experiments achieved impressive results regardless insufficient color texture information author', 'injury assessment sporting collisions requires estimation associated kinematics markerbased solutions widely accepted providing accurate reliable measurements setup times lengthy always possible outfit athletes restrictive equipment sporting situations new generation markerless motion capture deep learning techniques holds promise enabling measurement movement wild aim work evaluate performance popular deep learning model “ box ” human pose estimation dataset ten staged rugby tackle movements performed markerbased motion capture laboratory system three highspeed video cameras analysis discrepancy joint positions estimated markerbased markerless systems shows deep learning performs acceptably well instances although high errors exist challenging intervals heavy occlusion selfocclusion total 756 joint position estimates found mean absolute error mae less equal 25 formula presented 178 mae 25 50 formula presented 67 mae greater 50 formula presented mean per joint position error 47 formula presented © 2022 informa uk limited trading taylor francis group', 'paper deals subject networked inertial navigation term networked inertial navigation refers pnt positioning navigation timing technique whereby measurements network individual inertial measurement units imu fused generate navigation solution mitigating drift error inherent process integrating accelerations angular rates kinematic constraints imus network exploited help reduce overall drift error pnt solution however kinematic constraints practice difficult derive precisely known priori limits utility networked inertial navigation prior work literature common conservative way dealing kinematic constraints formulate inequality constraints upper bound constraint account poor knowledge constraints projection often used solve inequality constraint problems however shown paper projection comes several shortcomings dealing constraints paper deals one key challenge encountered networked inertial navigation problem unknown poorly known constraints paper deeplearningbased learn kinematic constraints within networked inertial system training data sets fills void poorly known unknown stochastic constraints allows us obtain constraints given uncertainty data tested human pose estimation problem experimental results show able effectively mitigate error drifts accurately capture relative pose imu components within networked inertial system © 2022 international technical meeting institute navigation itm rights reserved', 'research progress human pose estimation deep learning comprehensively summarized basis comparison analysis various singleperson pose estimation variety multiperson pose estimation algorithms summarized topdown bottomup approaches topdown solutions local area overlap articulation point confusion difficulty detecting articulation point atypical parts human body mainly introduced bottomup contribution clustering articulation point detection emphasized representative achieve excellent performance current public datasets compared analyzed review enables researchers understand familiarize existing research results field expand research ideas look forward possible research directions future © 2021 universitat zu koln rights reserved', 'human pose estimation fundamental yet challenging computer vision task studied many researchers around world recent years basic task computer vision multiperson pose estimation core component many practical applications paper extensively reviews recent works multiperson pose estimation specifically illustrate analyze popular detail compare pros cons fill gaps existing surveys addition commonly used datasets evaluation metrics opensource systems also introduced respectively finally summarize development multiperson pose estimation discuss research trends © 2021 elsevier ltd', 'paper mutually enhanced modeling meme presented human pose estimation focuses enhancing lightweight model performance low complexity obtain higher accuracy traditional model scale largely expanded heavy deployment difficulties however lightweight model large performance gap compared former thus urgent need way fill therefore meme reconstruct lightweight baseline model effbase transferred intuitively efficientdet efficient effective pose eeffpose net contains three mutually enhanced modules enhanced effnet eeffnet backbone total fusion neck tfneck final attention head fahead extensive experiments coco mpii benchmarks show memebased models reach performances limited parameters specifically conditions eeffposep0 256 × 192 898 parameters achieve 754 ap coco val set outperforms hrnetw48 14 parameters © 2022 authors licensee mdpi basel switzerland', 'task 2d human pose estimation known significant gain performance advent deep learning task aims estimate body keypoints people image video however reallife applications bring new challenges underrepresented general context datasets instance driver status monitoring consumer road vehicles introduces new difficulties like self background bodypart occlusions varying illumination conditions cramped view angles etc monitoring conditions currently absent general purposes datasets paper two main contributions firstly introduce dripe driver pose estimation new dataset foster development evaluation human pose estimation drivers consumer vehicles first publicly available dataset depicting drivers real scenes contains 10k images 19 different driver subjects manually annotated human body keypoints object bounding box secondly new keypointbased metric human pose estimation metric highlights limitations current metrics hpe evaluation current deep neural networks pose estimation general drivingrelated datasets © 2021', 'vehicle incabin monitoring increasing fulfil specifications european safety regulations regulations present several requirements detecting driver distraction complex requirements soon expected higher automation levels todays restraint systems provide optimal protection standard frontal seat positions deviations might cause severe airbaginduced injuries makes incabin monitoring critical improve safety mitigate dangerous situations case crash especially high levels autonomous driving defining best sensor positioning inside vehicles cabin challenge due constraints limitations main aim work verify simulated 3d human models integrated 3d modelled vehicle interior environment used run deep learning human pose estimation models perform task utilized software makehuman combined blender build virtual environment create photorealistic scenes containing selected front occupants postures cases feed openpose mask rcnn models results showed 2d hpe human pose estimation network pretrained real data detect successfully photorealistic synthetic data humans complex scenarios also shown complex rare postures cause failure 2d hpe detections shown literature review work helps define suitable camera positions combination specific camera lenses deliver quality images robust pose detection © 2021', 'continuous development computer vision technology deep learning technology well applied human pose estimation however latest human pose estimation algorithms solve problem interdependence occlusion joints key point detection human pose estimation still challenging task paper masked pretraining model lightweight shnet combined random occlusion performed key points human body picture pretraining model used predict problems mutual occlusion interdependence solved experiments show algorithm effective key point detection mpii coco 2017 datasets © 2021 technical committee control theory chinese association automation', 'modern 3d human pose estimation builds deep learning network requiring expensive amounts training data pairs 2d 3d pose annotations paper selfsupervised 3d human pose estimation without 3d annotations instead exploit multiview images camera parameters make network learn 3d human pose geometric consistency merit validated via experiments © 2021', '55 special focus conference advances information communication technology computing topics energy savings data mining kmeans clustering rprogramming effective detection localization text natural scene images adaptive kuwahara filter classification breast cancer histopathology images efficientnet architectures online teaching strategies education smart intelligent health monitoring system energyefficient olsr routing protocol flying ad hoc networks challenges malware detection effecting areas survey smart society development analysis control inductive inference integrated smart iot infrastructure management window blockchain whale lstm approaches driving analysis load fuel consumption obdii diagnostics 2d 3d human pose estimation analysis deep learning comprehensive art atmospheric turbulence mitigation methodologies visible infrared sequences graphical interpretation multidimensional data visualization heart disease dataset violence detection videos deep learning survey comparative study noma optimum power allocation dls algorithm dnn partofspeech pos tagging nyishi language multimodelbased preclinical prediction system heart diseases rfebpnn human computer interaction proclivity formed analysis interpretation survey emergence sustainability adoption healthcare sector covid19 mesh variants massively parallel systems matlab blockchainbased secure file storage hybrid cryptography machine learning malware detection security amplification iot blockchain', 'human pose estimation human activity recognition two researched domains computer vision applications surveillance humancomputer interaction video retrieval benefit robust solutions domain due disparities interpersonal parameters mobility efficiency recording configurations process challenging paper deep learning algorithms used pose estimation activity recognition models pose estimation model generates array objects keypoint coordinated specific interval time series data consumed activity recognition model trained temporal convolutional neural network attained accuracy model found 927 loss 019 © 2022', 'driver monitoring systems play crucial role detecting drivers distraction assure perception reaction driver loaded secondary activities specifically making transition control authority automated driving system driver conditional automation sae level l3 activities cell phone operating radio drinking talking passenger instance preventing driver percepting road scene degrading perception reaction driver possible hazards paper new presented detect driver activity deep learning driver image classification driver pose estimation driver pose information obtained pretrained deep network used machine learning classifier case random forest results two predictive models deep learning image classifier random forest classifier leveraged pose estimation combined activity detection performed fusion mechanism effectiveness presented driver activity recognition system investigated publicly available dataset multiclass activity detection test accuracy found 09703 © 2021', 'rise deep learning technology broadly promoted practical application artificial intelligence production daily life computer vision many humancentered applications video surveillance humancomputer interaction digital entertainment etc rely heavily accurate efficient human pose estimation techniques inspired remarkable achievements learningbased 2d human pose estimation numerous research studies devoted topic 3d human pose estimation via deep learning backdrop paper provides extensive literature survey recent literature deep learning 3d human pose estimation display development process research studies track latest research trends analyze characteristics devised types literature reviewed along general pipeline 3d human pose estimation consists human body modeling learningbased pose estimation regularization refinement different existing reviews topic paper focus deep learningbased learningbased pose estimation discussed two categories singleperson multiperson one categorized data type imagebased videobased moreover due significance data learningbased paper surveys 3d human pose estimation according taxonomy supervision form last paper also enlists current widely used datasets compares performances reviewed literature survey concluded branch 3d human pose estimation starts fullysupervised still much room multiperson pose estimation supervision image video besides significant development 3d human pose estimation via deep learning inherent ambiguity occlusion problems remain challenging issues need better addressed © 2021 authors licensee mdpi basel switzerland', '141 topics discussed learning disambiguate strongly interacting hands via probabilistic perpixel part segmentation skeletondriven neural occupancy representation articulated hands exemplar finetuning 3d human model fitting towards inthewild 3d human pose estimation body size depth disambiguation multiperson reconstruction single images 3d reconstruction novel object shapes single images algebraic constraint preserving convexity planar homography visual camera relocalization graph neural networks relative pose supervision practical pose trajectory splines explicit regularization nonlinear anisotropic diffusion memoryefficient computed tomography superresolution reconstruction spectral reconstruction disparity spatiospectrally coded light fields via multitask deep learning neural disparity refinement arbitrary resolution stereo monocular depth estimation primed salient point detection normalized hessian loss', 'human pose estimation hpe aims retrieving 3d position human joints images videos show current 3d hpe suffer lack viewpoint equivariance namely tend fail perform poorly dealing viewpoints unseen training time deep learning often rely either scaleinvariant translationinvariant rotationinvariant operations maxpooling however adoption procedures necessarily improve viewpoint generalization rather leading datadependent tackle issue novel capsule autoencoder network fast variational bayes capsule routing named deca modeling joint capsule entity combined routing algorithm preserve joints hierarchical geometrical structure feature space independently viewpoint achieving viewpoint equivariance drastically reduce network data dependency training time resulting improved ability generalize unseen viewpoints experimental validation outperform depth images seen unseen viewpoints topview frontview rgb domain network gives results challenging viewpoint transfer task also establishing new topview hpe code found httpsgithubcommmlabcvdeca © 2021', 'deep learning achieved unprecedented accuracy monocular 3d human pose estimation however current learningbased 3d human pose estimation still suffers poor generalization inspired skeletal animation popular game development animation production put forward simple intuitive yet effective interpolationbased data augmentation synthesize continuous diverse 3d human body sequences enhance model generalization transformerbased lifting network trained augmented data utilizes selfattention mechanism perform 2dto3d lifting successfully infer highquality predictions qualitative experiment quantitative result crossdataset experiment demonstrates resulting model achieves superior generalization accuracy publicly available dataset © 2022', 'threedimensional 3d human pose estimation involves estimating articulated 3d joint locations human body image video due widespread applications great variety areas human motion analysis human–computer interaction robots 3d human pose estimation recently attracted increasing attention computer vision community however challenging task due depth ambiguities lack inthewild datasets large number approaches many deep learning developed past decade largely advancing performance existing benchmarks guide future development comprehensive literature review highly desired area however existing surveys 3d human pose estimation mainly focus traditional comprehensive review deep learning remains lacking literature paper provide thorough review existing deep learning works 3d pose estimation summarize advantages disadvantages provide indepth understanding area furthermore also explore commonlyused benchmark datasets conduct comprehensive study comparison analysis study sheds light state research development 3d human pose estimation provides insights facilitate future design models algorithms © 2021 authors', 'contextbased twostage 3d human pose estimation network structure first stage obtain 2d human pose 2d keypoints video stream data stage crucial subsequent work entire process analyzing limitations shortcomings existing models contextbased human pose estimation network structure incorporate bilstm structure pose machine model invisible keypoints jointly predicted human pose current frame context information quantification visualization experiments proved good mitigating effect invisible key points caused occlusion wrong linking human keypoints second stage 3d human pose obtained sparse representation 3d reconstruction experimental results show designed higher accuracy existing human body pose estimation video streaming better performance occlusion problem © 2021', 'development signal processing deep learning visionbased loosening detection progressed recent years however existing visual inspection mainly used measure loosening angle requires prior knowledge initial tightening states applicable hexagon bolts nuts clear edges paper quantitatively calculate length exposed bolt detecting loosening visionbased deep learning geometric imaging theory newly consists three modules region interest roi location keypoint detection length calculation modules roi location module applies faster regional convolutional neural network fasterrcnnbased deep learning algorithm locate exposed bolt whereas keypoint detection module employs cascaded pyramid network cpnbased deep learning algorithm detect five keypoints exposed bolt accurate expression derived geometric imaging theory coordinates five keypoints camera parameters imported expression calculate length exposed bolt experiment resolution input image 640 × 640 pixels test speed approximately 200 milliseconds per image experimental results showed average measurement error 061 mm outperforming measurement networks human pose estimation moreover robustness validated varying shooting conditions bolt sizes colours finally factors influencing detection measurement accuracies discussed prospects research © 2021 elsevier bv', 'visionbased 3d human pose estimation approaches typically evaluated datasets limited diversity regarding many factors eg subjects poses cameras lighting however reallife applications would desirable create systems work arbitrary conditions “ inthewild ” advance towards goal investigated commonly used datasets humanevai human36m panoptic studio discussed biases limitations diversity illustrated crossdatabase experiments used surrogate roughly estimating inthewild performance purpose first harmonized differing skeleton joint definitions datasets reducing biases systematic test errors crossdatabase experiments scale normalization significantly improved generalization across camera viewpoints subjects datasets additional experiments investigated effect less cameras training multiple datasets applying anatomybased pose validation step openpose basis 3d pose estimation experimental results showed usefulness joint harmonization scale normalization augmenting virtual cameras significantly improve crossdatabase indatabase generalization time experiments showed dataset biases could compensated call new datasets covering diversity discussed results promising directions future work © 2021 authors licensee mdpi basel switzerland', 'apply artificial intelligence techniques browser resource constrained computing devices machine learning ml intimidating subject know essentials applications works book takes advantage intricacies ml processes simple flexible portable programming language javascript work approachable fundamental coding ideas javascript programming features along standard libraries youll first learn design develop interactive graphics applications move neural systems human pose estimation strategies training deploying ml models browser tensorflowjs libraries emphasized conquering fundamentals youll dig wilderness ml employ ml processing p5 libraries human gait analysis building gait recognition themes youll come understand variety ml implementation issues example ’ learn classification normal abnormal gait patterns beginning machine learning browser ’ way becoming experienced machine learning developer ’ learn work ml models calculations information gathering implement tensorflowjs libraries ml models perform human gait analysis ml techniques browser book computer science students research scholars novice programmersweb developers domain internet technologies © 2021 nagender kumar suryadevara', 'recent years task human pose estimation become increasingly important due large scale usage including vr applications well higherlevel tasks human behavior understanding paper introduce novel twostage deep learning named segmentationguided pose estimation sgpe pipeline two neural networks working sequential fashion models effectively process unorganized point clouds input first segmentation network performs pointwise classification corresponding body regions next step point cloud perpoint region assignment forming fourth input channel passed regression network way local global features point cloud preserved helping model fully maintain body pose structure strategy achieves competitive results examined benchmark datasets outperforms © 2021 slovak academy sciences rights reserved', 'rapid evolution smart devices realtime applications many new kinds computationintensive services emerged successively corresponding requirements growing dramatically extended cloud computing mobile edge computing mec novel technology provide powerful computing resource proximity resourcerestrained mobile devices thus enables collaboration edge server mobile device improve quality experience users article intelligent collaborative inference ici realtime computationintensive services mec network achieve intelligent service partitioning partial task offloading since machine learning algorithms applied many applications advancement big data computing power focus services deeplearning particularly research service posenet model achieve human pose estimation field computer vision design relevant ici algorithm achieve finegrained video stream processing consideration video service requirement deep neural network dnn model structure mobile device capability wireless network condition cooperative server workload python programming language tensorflow library test ici practical simulation parameters real hardware platforms experiment results show presented ici superior performance terms service frame rate client energy consumption benchmark approaches © 2021 john wiley sons ltd', 'development artificial intelligence field automatic drive machine learning well field human posture estimation could help people many cases imitation human body entertainment sports field achievement also shows importance human posture estimation future 2013 toshev introduced deep pose human posture estimation study human body posture estimation began shift traditional deep learning cnn firstly applied detection human joints transform estimation human body posture regression problem joints secondly cpm mainly heatmap represent joints position position constraint relationship fully consider spatial position relationship various joints process top bottom bottom top hourglass convolution pooling images resolution adjusted best information image integrated cpn also topdown detect body frame first detect human body nodes body frame refinenet correct results large errors hrnet followup simple baseline enables multiscale fusion information exchange across resolution subnetworks discussed lot human posture deep learning led rapid development academy technology development technology improve future analysis human detection data example detect daily walking state different occasions images data analyze characteristics human posture help us develop future crucial impact development © 2021', 'computer vision techniques essential autonomous driving applications consequently many deep learningbased developed large number outdoor images captured vehiclemounted cameras however less attention paid inside vehicles although invehicle applications driver monitoring action recognition also essential autonomous driving scenario paper investigate existing invehicle datasets especially human pose estimation provide details © 2022', 'research study evaluation fullbody sketches principle human pose estimation openpose library detect 18 keypoints human structure dataset used research drawing sketches 22 firstyear students drew three drawings three models detected keypoints calculated determine angle distance keypoints provides 26 features features modeled ann predicting grades drawings classified good moderate poor resulting keypoints taken find angles distances skeleton extracting 26 features taking features create model ann classification performance model evaluated 56 accuracy © 2021 acm', 'observed remarkable impressive performance imagebased human pose estimation achieved deep convolutional neural networks cnn nevertheless directly applying imagebased models videos computionally intensive also may cause jitter loss main reason imagebased models purely focus local features individual frames totally ignore temporal information among adjacent frames existing address temporal coherency issue however need designed carefully combined existing imagebased paper simple yet effective module refine estimated pose exploiting temporal coherency among heatmaps adjacent frames easily inserted imagebased networks plugin show temporal coherency issue among heatmap frames could reformulated graph path selection optimization problem moreover speed refinement process hierarchical graph optimization achieve refinement coarse fine experimental results two largescale video pose estimation benchmarks show module improve performance little speed loss combined imagebased efficient plugin', 'background deep learningbased human pose estimation estimate joint centers position achieved promising results publicly available human pose datasets eg human36 however datasets may less efficient gait study particularly clinical applications limited number subjects homogeneity asymptomatic adults errors introduced marker placement subjects ’ regular clothing research question new human pose dataset adapted gait study could contribute advancement evaluation markerless motion capture systems markerless system deep learningbased pose estimation new dataset ensam dataset collected twentytwo asymptomatic adults one adult scoliosis one adult spondylolisthesis seven children bone disease performed ten walking trials recorded markerless system reference system – combining markerbased motion capture system medical imaging system eos dataset split training test sets pose estimation already trained human36 dataset evaluated ensam test set reevaluated training ensam training set joints coordinates evaluated blandaltman bias 95 confidence interval joint position error euclidean distance estimated joint centers corresponding reference values results blandaltman 95 confidence intervals substantially improved finetuning pose estimation ensam training set eg 1069 mm 174 mm hip joint new dataset mean joint position error varied 62 mm ankles 211 mm shoulders significance markerless system achieved promising results terms joint position errors future studies necessary assess system terms gait parameters © 2021 elsevier bv', 'rehabilitation important improve quality life mobilityimpaired patients smart walkers commonly used solution embed automatic objective tools datadriven humanintheloop control monitoring however present solutions focus extracting specific metrics dedicated sensors unified fullbody investigate general realtime fullbody pose estimation two rgbd camera streams nonoverlapping views mounted smart walker equipment used rehabilitation human keypoint estimation performed twostage neural network 2dstage implements detection module locates body keypoints 2d image frames 3dstage implements regression module lifts relates detected keypoints cameras 3d space relative walker model predictions lowpass filtered improve temporal consistency custom acquisition used obtain dataset 14 healthy subjects used training evaluating offline deployed real walker equipment overall keypoint detection error 373 pixels 2dstage 4405 mm 3dstage reported inference time 266 ms deployed constrained hardware walker present novel patient monitoring datadriven humanintheloop control context smart walkers able extract complete compact body representation realtime inexpensive sensors serving common downstream metrics extraction solutions humanrobot interaction applications despite promising results data collected users impairments assess performance rehabilitation tool realworld scenarios © 2021 elsevier ltd', 'human actions conform usual behavior considered anomalous actors called anomalous entities detection anomalous actions visual data challenging problem computer vision paper presents new detect anomalous actions complex situations examination halls cascade deep convolutional neural network models first stage apply pretrained model human pose estimation frames videos extract key feature points body patches extracted key point utilized second stage build densely connected deep convolutional neural network model detecting anomalous actions experiments collect video database students undertaking examination hall results show detect anomalous actions warrant unusual behavior high accuracy © 2021', 'human pose estimation action recognition related tasks since problems strongly dependent human body representation analysis nonetheless recent literature handle two problems separately article multitask jointly estimating 2d 3d human poses monocular color images classifying human actions video sequences show single architecture used solve problems efficient way still achieves comparable results task running throughput 100 frames per second benefits high parameters sharing two tasks unifying still images video clips processing single pipeline allowing model trained data different categories simultaneously seamlessly way additionally provide important insights endtoend training multitask model decoupling key prediction parts consistently leads better accuracy tasks reported results four datasets mpii human36m penn action ntu rgbd demonstrate effectiveness targeted tasks source code trained weights publicly available httpsgithubcomdluvizondeephar © 19792012', 'human pose estimation achieved tremendous advances accuracy emergence various deep neural network architectures however lowresolution lr images far achieving acceptable accuracy deep learning superresolution sr proven helpful addressing challenges face recognition object detection lr images following idea integrate sr existing human pose estimation networks increase accuracy lr images work novel endtoend network architecture effective combination sr human pose estimation moreover presented guide sr network generate intermediate highresolution hr images contribute pose estimation rather simply taking sr upstream task experimental results show accuracy 20 improved interpolationassisted pose estimation network downsampled mpii dataset © 2021 elsevier ltd', 'technological advances enable design systems interact closely humans multitude previously unsuspected fields martial arts outside application techniques point view modeling human movement relation learning complex motor skills martial arts interest articulated around system movements predefined least bounded governed laws physics execution must learned continuous practice time literature suggests artificial intelligence algorithms used computer vision model movements performed thus compared good execution well analyze temporal evolution learning exploring application model psychomotor performance karate combats called kumites characterized explosiveness movements addition modeling psychomotor performance kumite requires modeling joint interaction two participants current research efforts human movement computing focus modeling movements performed individually thus work explore apply pose estimation algorithm extract features predefined movements ippon kihon kumite one‐step conventional assault compare classification metrics four data mining algorithms obtaining high values © 2021 authors licensee mdpi basel switzerland', 'accurately detecting keypoints small persons image bottomup multiperson pose estimation algorithms exceptionally difficult owing scale variation challenges higherhrnet initially solved challenge multiplayer scale change pose estimation however repeated crossscale fusion owing inherent defects channel reduction semantic information lost furthermore aliasing effects produced miscellaneous feature maps formed crossscale fusion significant impact detection accuracy small persons paper novel bottomup human pose estimation algorithm higherhrnet called channelenhanced higherhrnet cehigherhrnet cehigherhrnet comprises three main components multiscale subpixel skip fusion module lightweight attention mechanism channel attention enhanced spatial attention modules highresolution feature pyramid added dupsampling module lightweight attention mechanism optimizes feature map fusion deconvolution replaced dupsampling strengthens network ’ scale awareness makes sensitive robust scale changes average precision ap cehigherhrnet coco testdev dataset 719 improvement 14 compared higherhrnet furthermore average detection accuracy small persons 681 ap improvement 15 ap results verify cehigherhrnet robust processing scale changes stronger ability handle crowded environments thus accurate positioning small persons images human bodies crowded environments © 2022 rights reserved', 'threedimensional human mesh reconstruction single video made much progress recent years due advances deep learning however previous still often reconstruct temporally noisy pose mesh sequences given inthewild video data address problem human pose refinement network hprnet nonlocal attention mechanism pipeline consists weightregression module weightedaveraging module skinned multiperson linear smpl module first weightregression module creates pose affinity weights 3d human pose sequence represented unit quaternion form next weightedaveraging module generates refined 3d pose sequence performing temporal weighted averaging generated affinity weights finally refined pose sequence converted human mesh sequence smpl module hprnet simple effective postprocessing network substantially improve accuracy temporal smoothness 3d human mesh sequences obtained input video existing human mesh reconstruction experiments show noisy results existing consistently improved various real datasets notably reduces pose acceleration errors vibe existing human mesh reconstruction 14 665 respectively 3dpw dataset © 2021 authors licensee mdpi basel switzerland', 'background many available gait monitoring technologies expensive require specialized expertise time consuming widely available clinical advent videobased pose tracking provides opportunity inexpensive automated analysis human walking older adults video cameras however need validate gait parameters calculated algorithms gold standard measuring human gait data population compared quantitative gait variables 11 older adults mean age 852 calculated video recordings three pose trackers alphapose openpose detectron calculated 3d motion capture system performed comparisons videos captured two cameras two different viewing angles viewed front back also analyzed data including gait variables individual steps participant participant ’ averaged gait variables results findings revealed temporal cadence step time spatial variability gait measures step width estimated margin stability coefficient variation step time width calculated video pose tracking algorithms correlate significantly motion capture system ii minimal differences two camera heights walks viewed front back terms correlation gait variables iii gait variables extracted alphapose detectron highest agreement openpose lowest agreement conclusions important opportunities evaluate models capable 3d pose estimation video data improve training posetracking algorithms older adult clinical populations develop videobased 3d pose trackers specifically optimized quantitative gait measurement © 2021 authors', 'popularity king kong pirates caribbean 2 avatar films virtual characters films become popular well loved audiences creation virtual characters different traditional 3d animation real character movements expressions overview several mainstream motion capture systems field motion capture presented application motion capture technology film animation explained detail current motion capture technology mainly complex human markers sensors costly deeplearningbased human pose estimation becoming new option however existing single person picture estimation many challenges video multiperson estimation experimental results show simple design human motion capture system achieved © 2022 yating wei', 'paper learning human positioning algorithm image overcome disadvantages fingerprint mapbased localization hardware devices human positioning accuracy also verified experiment core parts algorithm learning phase human positioning map hpm human positioning phase result experiment human recognition accuracy algorithm showed high recognition accuracy 9974 person moves om 6m © 2021', 'pedestrian tracking important task field computer vision basis advanced vision tasks human pose estimation motion recognition behavioural analysis widely used emerging areas autonomous driving intelligent security service robotics detectionbased tracking paper relies heavily pedestrian detection excellent detection algorithms significantly improve tracking performance address problem poor pedestrian detection accuracy presence small targets occlusion congestion paper designs joint attention module combines module yolov4 target detection model design joint attentionbased pedestrian detection algorithm conducts experiments otb100 upt pedestrian detection dataset experimental results demonstrate effectiveness algorithm © 2021', 'aiming develop annotationefficient algorithm selecting informative samples labeled human pose estimation novel active learning generalized standard coreset dynamically incorporating uncertainty representativeness cues designed assignment cost consists fast spatial consistency domainshift scaled distance adaptive information measurement data selection criterion extensive experiments conducted two network architectures various datasets demonstrate effectiveness superiority comparison © 2021', 'singleperson human pose estimation facilitates markerless movement analysis sports well clinical applications still models human pose estimation generally meet requirements reallife applications proliferation deep learning techniques resulted development many advanced approaches however progresses field complex inefficient models also introduced caused tremendous increases computational demands cope complexity inefficiency challenges novel convolutional neural network architecture called efficientpose exploits recently efficientnets order deliver efficient scalable singleperson pose estimation efficientpose family models harnessing effective multiscale feature extractor computationally efficient detection blocks mobile inverted bottleneck convolutions time ensuring precision pose configurations still improved due low complexity efficiency efficientpose enables realworld applications edge devices limiting memory footprint computational cost results experiments challenging mpii singleperson benchmark show efficientpose models substantially outperform widelyused openpose model terms accuracy computational efficiency particular topperforming model achieves accuracy singleperson mpii lowcomplexity convnets © 2020 authors', 'human pose estimation become hot problem humancomputer interaction intelligent application technologies results obtained previous deep learning networks satisfactory facing smallscale human instances therefore order solve problem scale variation human pose estimation especially pinpoint keypoints smallscale human instances improved highresolution network improved hrnet paper main improvement work paper follows paper double attention mechanism added forward transmission parallel subnetwork aim assigning weights propagated information without changing number channels assigning information high weights useful information reducing interference caused irrelevant information paper network structure validated coco dataset average accuracy average precision ap improved hrnet 664 23 higher average accuracy highresolution network hrnet 04 increase parameters © published licence iop publishing ltd', 'multiview human pose estimation techniques assume cameras fixed dynamic scenes cameras able move seek best views avoid occlusions extract 3d information target collaboratively paper address problem online view selection fixed number cameras estimate multiperson 3d poses actively exploits distributed multiagent deep reinforcement learning camera modeled agent optimize action cameras interagent communication protocol developed transfer cameras ’ relative positions agents better collaboration experiments panoptic dataset show outperforms view selection large margin given identical number cameras best knowledge first address online active multiview 3d pose estimation multiagent reinforcement learning © 2021 authors licensee mdpi basel switzerland', 'human pose estimation hpe defined problem human joints ’ localization also known keypoints elbows wrists etc images videos also defined search specific pose space articulated joints hpe recently received significant attention scientific community main reason behind trend pose estimation considered key step many computer vision tasks although many approaches reported promising results domain remains largely unsolved due several challenges occlusions small barely visible joints variations clothing lighting last years power deep neural networks demonstrated wide variety computer vision problems especially hpe task context present paper deep fullbodyhpe dfbhpe rgb images convnets fifteen human joint positions predicted exploited large range applications gesture recognition sports performance analysis humanrobot interaction evaluate deep pose estimation model apply recognize daily activities person unconstrained environment therefore extracted features represented deep estimated poses fed svm classifier validate architecture tested two publicly available benchmarks pose estimation activity recognition namely jhmdband cad60datasets obtained results demonstrate efficiency convnets svm prove deep pose estimation improve recognition accuracy means comparison achieve best hpe performance well best activity recognition precision cad60 dataset © 2021 authors licensee mdpi basel switzerland', 'due numerous application human posture application real world recently gained lot attention paper gives complete assessment deep learningbased human pose estimation analyses methodology used deep learning improve performance human pose estimation pipelines single people multiperson examined individually first alpha pose open pose deep learning used pipelines compared analyzed © 2021', 'physical abuse become societal problem mostly children women old age people vulnerable especially cases domestic violence workplace aggression reporting challenge especially preexisting relationship abuser victim paper deep learning technique human action recognition human pose identification tackle physical abuse detecting real time 3d convolution neural network cnn architecture built 3d convolution feature extractors extract temporal spatial data video multiple convolution layer subsampling layer input video converted feature vector human pose estimation done detection key points body points tracking one frame another gives spatialtemporal features feed neural network nn present metrics measure accuracies systems real time reporting fault tolerance capabilities utmost importance weighted metrics shows accuracy 8942 precision 8582 thus shows effectiveness system © 2021 institute advanced engineering science rights reserved', 'twodimensional human pose estimation hpe deep learning attracted much attention application potential work constructing specific neural network architecture processing extracted feature information corresponding feature fusion information association strategy obtain human pose estimation result paper systematically summarizes studies twodimensional human pose estimation deep learning recent years categorizing data set benchmarks pose estimation evaluation standardsthe existing divided singleperson pose estimation multiperson pose estimation analyzed summarized terms network architecture design output feature representation loss function selection finally current challenges paper discusses development directions future research application prospects twodimensional human pose estimation © 2021 editorial office computer engineering rights reserved', 'objective developed investigated feasibility machine learningbased automated rating 2 cardinal symptoms parkinson disease pd resting tremor bradykinesia openpose deep learningbased human pose estimation program analyzed video clips resting tremor finger tapping bilateral upper limbs 55 patients pd 110 arms key motion parameters including resting tremor amplitude finger tapping speed amplitude fatigue extracted develop machine learningbased automatic unified parkinsons disease rating scale updrs rating support vector machine svm evaluate performance model calculated weighted intraclass correlation coefficients iccs model gold standard rating movement disorder specialist trained certified movement disorder society updrs rating values compared weighted icc nontrained human rater gold standard rating results resting tremors svm model showed good excellent reliability range gold standard rating 0791 icc 0927 values higher nontrained human rater 0662 icc 0861 finger tapping svm model showed good reliability range gold standard rating 0700 icc 0793 comparable nontrained human raters 0627 icc 0797 conclusion machine learningbased algorithms automatically rate pd cardinal symptoms feasible accurate results nontrained human ratings classification evidence study provides class ii evidence machine learningbased automated rating resting tremor bradykinesia people pd good reliability compared rating movement disorder specialist © 2020 american academy neurology', 'innovation convolutional neural networks motivated study many aspects image processing object detection one topic interest field object detection human detection human pose estimation scheme human pose estimation scheme enables computer vision higher accuracy identifying human motion movement pose estimation scheme applied making 2d images 3d representation paper aims determine distance camera human subject realtime application takes advantage existing pose estimation schemes distance scaling applied paper showed high performance 027 error actual distance © 2021 authors exclusive licence springer sciencebusiness media llc part springer nature', 'last two decades development state oftheart artificial intelligence ai models significantly increased utilization commercial taskspecific robots service domain additional level intelligence introduced ai models enabled service robots coexist within different human environments collaborate endusers one promising ai techniques deep learning dl provide service robots wide range abilities detecting human pose emotions understanding natural languages well scene understanding achieved abilities enable mobile service robots execute specific tasks real stochastic environments mind chapter provide indepth analysis tasks bestsuited dl within service robots domain moreover study dl models object detection semantic segmentation human pose estimation carried end authors presented thorough examination training process analysis results one promising convolutional neural network models deeplabv3 used semantic segmentation © 2021 nova science publishers inc rights reserved', 'tensor natural representation multidimensional data tensor computation avoid possible multilinear data structure loss classical matrix computationbased data analysis book intended provide nonspecialists overall understanding tensor computation applications data analysis benefits researchers engineers students theoretical computational technical experimental details presents systematic uptodate overview tensor decompositions engineer ’ point view comprehensive coverage tensor computation data analysis techniques addition practical examples machine learning signal processing data mining computer vision remote sensing biomedical engineering also presented easy understanding implementation data analysis techniques may applied applications neuroscience communication psychometrics chemometrics biometrics quantum physics quantum chemistry etc discussion begins basic coverage notations preliminary operations tensor computations main tensor decompositions properties series tensorbased data analysis techniques presented tensor extensions classical matrix counterparts including tensor dictionary learning low rank tensor recovery tensor completion coupled tensor analysis robust principal tensor component analysis tensor regression logistical tensor regression support tensor machine multilinear discriminate analysis tensor subspace clustering tensorbased deep learning tensor graphical model tensor sketch discussion also includes number typical applications experimental results image reconstruction image enhancement data fusion signal recovery recommendation system knowledge graph acquisition traffic flow prediction link prediction environmental prediction weather forecasting background extraction human pose estimation cognitive state classification fmri infrared small target detection heterogeneous information networks clustering multiview image clustering deep neural network compression © springer nature switzerland ag 2022', 'estimation human pose important field study area computer vision due significant applications various important fields human computer interaction action recognition video surveillance threat prediction etc focus researchers mainly dramatic effects deep learning techniques resulted major steps important developments estimating human poses proposing fast efficient deep learning detecting 2d pose monocular image multiple persons vector field pvf part affinity learns well limbs connected architectural design provides global pixellevel interpretation number people enabling bottomup maintains high accuracy real time architecture designed two branches convolutional neural network cnn sequential prediction understand reference part locations around successfully help ms coco human pose dataset implemented © 2021 elsevier ltd rights reserved', 'human pose estimation image video key task many multimedia applications previous achieve great performance rarely take efficiency consideration makes difficult implement networks lightweight devices nowadays realtime multimedia applications call efficient models better interaction moreover deep neural networks pose estimation directly reuse networks designed image classification backbone optimized pose estimation task paper efficient human pose estimation two parts efficient backbone efficient head implementing differentiable neural architecture search customize backbone network design pose estimation reduce computational cost negligible accuracy degradation efficient head slim transposed convolutions spatial information correction module promote performance final prediction experiments evaluate networks mpii coco datasets smallest model requires 065 gflops 881 pckh05 mpii large model needs 2 gflops accuracy competitive large model hrnet takes 95 gflops © 2021 authors', 'numerous applications human activity recognition har paper presents minisurvey recent har studies originally developed benchmark datasets two types environmental sensors first dataset specifically examine human pose estimation slight motion recognition related activities daily living adl employs openpose describes feature vectors without effects objects scene features convolutional neural network cnn vgg16 backbone recognizes behavior patterns classifying obtained images learning verification subsets first dataset comprises timeseries panoramic images obtained fisheye lens monocular camera wide field view attempted recognize five behavior patterns eating reading operating smartphone operating laptop computer sitting even panoramic images including distortions results demonstrate capability recognizing properties characteristics slight motions posebased behavioral patterns second dataset obtained five environmental sensors thermopile sensor co2 sensor air pressure humidity temperature sensors sensor system obviates need constraint also preserves subject ’ privacy long shortterm memory lstm network combined cnn deeplearning model dealing timeseries features recognized eight behavior patterns eating operating laptop computer operating smartphone playing game reading exiting taking nap sitting recognition accuracy second dataset lower first dataset consisting images demonstrated recognition behavior patterns timeseries weak sensor signals recognition results first dataset accuracy evaluation reused automatically annotated labels applied second dataset actualizes semiautomatic annotation false recognized category detection sensor calibration feasibility study results show new possibility har used adl unique sensors two types © 2021 authors licensee mdpi basel switzerland', 'event cameras provide number benefits traditional cameras ability track incredibly fast motions high dynamic range low power consumption however application computer vision problems many primarily dominated deep learning solutions limited lack labeled training data events work leverages existing labeled data images simulating events pair temporal image frames convolutional neural network train network pairs images events adversarial discriminator loss pair cycle consistency losses cycle consistency losses utilize pair pretrained selfsupervised networks perform optical flow estimation image reconstruction events constrain network generate events result accurate outputs networks trained fully end end network learns generative model events images without need accurate modeling motion scene exhibited modeling also implicitly modeling event noise simulator train pair downstream networks object detection 2d human pose estimation events simulated data large scale image datasets demonstrate networks abilities generalize datasets real events code dataset paper available httpsgithubcomalexzzhueventgan © 2021', 'development convolutional neural network significant progress made human pose estimation recent human pose estimation approaches adopt single branch network provide limited surveillance information aiming problem adopt multibranch network 2d human pose estimation 3d human pose estimation leading results multiple datasets verify effectiveness multibranch network © 2022', 'highresolution representations essential positionsensitive vision problems human pose estimation semantic segmentation object detection existing first encode input image lowresolution representation subnetwork formed connecting hightolow resolution convolutions series eg resnet vggnet recover highresolution representation encoded lowresolution representation instead network named highresolution network hrnet maintains highresolution representations whole process two key characteristics connect hightolow resolution convolution streams parallel ii repeatedly exchange information across resolutions benefit resulting representation semantically richer spatially precise show superiority hrnet wide range applications including human pose estimation semantic segmentation object detection suggesting hrnet stronger backbone computer vision problems codes available httpsgithubcomhrnet © 19792012', 'human pose estimation made significant progress deep learning techniques estimation occlusion keypoints still unsolved problem one important reason comes insufficiency existing benchmark datasets imbalanced body keypoints annotation lack occluded training samples address problem nearbyperson occlusion data augmentation noda provides synthetic nearbyperson occlusion images utilizing existing annotations first generate rough mask human bodies keypoints annotation build foreground human body pool one foreground human body crop randomly sampled properly placed training human body synthesis nearbyperson occlusion training images data augmentation easy implement deploy extensive experimental results mpii benchmark demonstrate effectiveness simple hrnet backbone models especially easilyconfusable joints makes significant improvement © 2021 apsipa', 'deep learning key core technology driven artificial intelligence achieve breakthroughs recent years deep learning pivotal tool realizing machine learning however existing deep learning difficult meet requirements easy development efficient execution time paper new deep learning called mindspore developed huawei technologies co ltd introduced mindspores features automatic parallelism automatic differentiation endside cloud collaborative training greatly improve training efficiency deep learning models positive effect model coding debugging enables mindspore achieve three major goals easy development efficient execution fullscene coverage shall implement deep learning model human pose estimation tracking mindspore field computer vision experimental results show mindspore advantages easy programming high accuracy well high efficiency © 2021', 'human pose estimation enabled numerous applications classifying tracking body movements although open datasets emerged facilitate evaluation pose detection generic benefit domain specific applications physical therapy quantitative clinical metrics requires precise differentiation measurement address issue construct first human keypoints detection dataset physical therapy particular lower body rehabilitation dataset consists 1885637 distinctive human poses 31 lower body rehab exercises performed 20 actors guidance licensed physical therapist motion captured state art motion tracking system 3d 2d establish ground truth furthermore optimize number deep learning models applied unique dataset utilize extremely efficient spatial pyramid eesp attention mechanism reduce models computational complexity experiment results show optimized models achieve comparable performance nearly 4x reduction complexity © 2021', 'paper presents gopose 3d skeletonbased human pose estimation system commodity wifi devices home system leverages wifi signals reflected human body 3d pose estimation contrast prior systems need dedicated sensors system require user wear sensors reuse wifi devices already exist home environment mass adoption realize system leverage 2d aoa estimation signals reflected human body deep learning techniques preliminary results show gopose achieves high accuracy 45cm various scenarios © 2021 ownerauthor', 'multivariate time series mts classification important various applications signature verification person identification motion recognition deep learning classification tasks usually learned crossentropy loss related yet different task predicting trajectories observed mts important cases handwriting reconstruction shape analysis human pose estimation goal align arbitrary dimensional time series ground truth accurately possible reducing error prediction distance loss variance similarity loss although learning losses multitask learning mtl helps improve trajectory alignment learning often remains difficult tasks contradictory novel neural network architecture mtl notably improves mts classification trajectory regression performance online handwriting onhw recognition achieve jointly learning crossentropy loss combination distance similarity losses onhw task handwritten characters multivariate inertial visual data inputs able achieve crucial improvements lower error less variance trajectory prediction still improving character classification accuracy comparison models trained individual tasks © 2022', 'fingerspelling communication system expresses character series hand gestures system important deaf people indicate new nouns expressed sign language since expression style system existing writing system understanding unique characteristics writing system important recognize fingerspelling properly paper syllablelevel fingerspelling recognition utilizes characteristics hangul unlike conventional recognize fingerspelling phonemelevel adopts hangulspecific attention mechanism pays attention important video frames terms recognizing syllable experiments keti korean fingerspelling dataset exhibits accurate results phonemelevel recognition © 2021 icros', 'order solve problem current gait recognition contour joint models gait features perform well presence covariates clothing carrying objects multiple viewing angles paper gait recognition human walking feature vector diagram wfvd deep learning firstly video stream input human wfvd obtained human pose estimation model human part affinity fields spatiotemporal gait network wfvd input designed gait feature learning recognition learns spatial temporal gait features residual learning module long shortterm memory lstm network respectively classification performed softmax layer network finally experiments performed dataset casiab developed institution automation chinese academy sciences casia selfbuilt dataset experimental results show makes full spatiotemporal gait information avoids interference redundant information allows better solve recognition difficulties due covariates clothing carrying objects complex real environments © 2021 science press right reserved', 'recent years human pose tracking become important topic computer vision cv improve privacy human pose tracking considerable interest techniques without video camera end radiofrequency identification rfid tags lowcost wearable sensor provide effective solution 3d human pose tracking article rfidpose visionaided realtime 3d human pose estimation system deep learning assisted cv rfid phase data calibrated effectively mitigate severe phase distortion high accuracy low rank tensor completion employed impute missing rfid data system estimates spatial rotation angle human limb utilizes rotation angles reconstruct human pose realtime forward kinematic technique prototype developed commodity rfid devices high pose estimation accuracy realtime operation rfidpose demonstrated experiments kinect 20 benchmark © 19632012', 'task 6d object pose estimation rgb images important requirement autonomous service robots able interact real world work present twostep pipeline estimating 6 dof translation orientation known objects keypoints part affinity fields pafs predicted input image adopting openpose cnn architecture human pose estimation object poses calculated 2d3d correspondences detected model keypoints via pnpransac algorithm evaluated ycbvideo dataset achieves accuracy par recent literature pafs assemble detected keypoints object instances proves advantageous heatmaps models trained predict keypoints single object class perform significantly better models trained several classes © 2022 springer nature switzerland ag', 'order ensure fairness examination solve problems traditional electronic invigilator system automatically analyze surveillance video video censors suffer high labor intensity work presents analyzing cheating behaviors candidates deep learning technology analyzing action examinee single frame surveillance video traditional object detection algorithm yolo combined human posture estimation project openpose extract position information candidates label candidates suspected cheating due particularity cheating behavior dataset deep learning training constructed marks two kinds human behavior examination scene including two types cheating behavior peeping passing notes time order improve detection speed interframe difference used extract key frames finally accuracy test set detection speed reach better result © 2021', 'exercise prevailing topic modern society people pursuing healthy lifestyle physical activities provide significant benefits human wellbeing inside human pose estimation action recognition repetitive counting fields developed rapidly past several years however works combined together assist people exercise paper multitask system covering three domains different existing heatmaps byproducts 2d human pose estimation models adopted exercise recognition counting recent heatmap processing proven effective extracting dynamic body pose information inspired deeplearning multitask model exercise recognition repetition counting best knowledge attempted first time meet needs multitask model create new dataset reppenn action counting speed labels multitask system estimate human pose identify physical activities count repeated motions achieved 9569 accuracy exercise recognition reppenn dataset multitask model also performed well repetitive counting 0004 mean average error mae 0997 offbyone obo accuracy reppenn dataset compared existing obtained results © 2021 authors licensee mdpi basel switzerland', 'inferring human pose monocular rgb image remains interesting field research computer vision serves fundamental key many realworld applications including humancomputer interaction animation detecting abnormal illegal human behavior despite considerable progress made area last decade face serious problems due huge variations human appearance occlusions noisy backgrounds viewpoints factors change context captured information paper introduce survey highlight various research tackle 2d 3d pose estimation tasks number persons image two main pipelines identified singleperson multiperson categories divided two groups according architectures also provide brief description current datasets different metrics applied evaluate performances finally discussion advantages disadvantages mentioned strategies © 2021 elsevier bv', 'deep learning subfield machine learning inspired structure human brain composed lot neurons able perform simple operation interact make decision recent years deep learning grown exponentially brought revolutionary advances computer visionbased applications image classification object detection simultaneous localisation mapping slam action activity recognition age estimation well human pose estimation literature review deep learning techniques applied visionbased application convolutional neural networks cnns restricted boltzmann machines rbms autoencoder sparse coding mostly focused solving feature extraction classification problems however techniques apply image processing applications image fusion image reconstruction require quantitative knowledge solve problems fields process obtaining quantitative knowledge image fusion reconstruction regression problem case deep feedforward neural network applied solve regression problem due ability map complicated nonlinear functions © published licence iop publishing ltd', '3d human pose estimate recently made novel pipeline architecture 3d human posture estimation inthewild pictures challenge major goal paper reduce keypoint reprojection loss allowing model trained inthewild pictures 2d ground truth annotations model several networks employed various phases process coco publicly available dataset utilized train validate results pipeline model consists yolo v3 recognize segment person image scale picture without distorting aspect ratio inferring 17 essential points human body posenet measure performance difficulties highlighted subset pictures 2d keypoint recognition ’ well model showing better performance hmr posenet © 2022 authors exclusive license springer nature singapore pte ltd', 'human pose estimation longstanding challenging problem computer vision many recent advancements field relied complex structure refinement specific human joint graphical relations however progress saturated terms accuracy time new approaches improve accuracy less 03 mpii test set despite complicated model structures recent developments summarized two main ideas 1 refinement subnetwork improve predictions iteratively 2 exploitation human joint graphical relations work present efficient simple iterative subnetworks linear hierarchical ordering aforementioned ideas help improve accuracy strong backbone models different versions iterative subnetwork examined significant improvements difficult body part predictions wrists ankles simple convolution subnetwork observed improvements made large receptive field subnetwork axialtransformer 1 © 2021', 'increase age elderly often fall seriously threatens lives investigations studies shown timely assistance fall reduce risk death paper fall recognition human skeleton video first video data set passed human pose estimation algorithm obtain new video data set containing human skeleton information annotated build twoclass model falls nonfalls threedimensional convolutional neural network train test fall behavior model experimental results show high accuracy recall rate good application value © 2022 authors exclusive license springer nature singapore pte ltd', 'learning models realworld robot spatial perception tasks one might access partial labels occurs example semisupervised scenarios labels available subset training instances types selfsupervised robot learning robot autonomously acquires labeled training set acquires labels subset output variables instance introduce general deal class problems auxiliary loss enforcing expectation perceived environment state abruptly change instantiate solve two robot perception problems simulated ground robot learning longrange obstacle mapping 400binarylabel classification task selfsupervised way static environment real nanoquadrotor learning human pose estimation 3variable regression task semisupervised way dynamic environment cases yields significant quantitative performance improvements average increase 6 auc percentage points former relative improvement r2 metric ranging 7 33 latter baselines © 2016', 'human posture estimate technique determining position body single often monocular image varieties cnn architectures available human pose estimation paper presents realtime multiperson human pose estimation video utilizing convolutional neural networks essentially indeed set parameters related define pose individual proper relation two components known pair limb nevertheless many considerations layer thickness width number feature maps kernel size batch size etc article performance assessment cnn performed constructing basic architecture image classification tested output network architecture popular image repository called cnn used identification classification mission research findings reveal suggested network delivers highest classification accuracy compared current techniques finally identified results describes better understand cnn models variety image classification missions furthermore convolutional neural network cnn category deep neural networks commonly used visual recognition © 2022 springer nature switzerland ag', 'detecting human posture one challenging problems machine vision however advancement deep learning technique brought significant progress specifically support fpga platform three stages performed consists deep learning pose detection construction tensorflow convolutional neural network cnn perform testing trained neural network configuring de1soc board usb camera interfacing secure shell ssh setup de1soc host computer interfacing transferring images study aims evaluate effectiveness according combinations human pose estimation performed pre trained model resnet almost 83000 images coco dataset used training 50 random images coco dataset 20 images captured de1soc platform used testing network respectively significantly demonstrating promising performance 98 able detect major keypoints human skeleton © 2022 authors exclusive license springer nature singapore pte ltd', 'one main challenges computer science image processing 2d human pose estimation specifically occlusion particular occlusion human joints caused camera angle paramount importance paper new highly accurate network estimate 2d human poses video images deep learning employ single shot multibox detector network detect centre position human within video frame stacked hourglass network estimate 2d human pose approximate human motion linear motion different frames certain period optimize human centres local outlier factor kalman filters applied optimize human pose estimations video address inaccurate prediction caused human joints occlusion adaptive network tested two wellknown benchmarks human pose estimation mpii jointannotated human motion data datasets also generate 2d human pose estimating qualitative results single multiple people internet videos experimental results show network strong practicability achieve high accuracy adaptive estimating 2d human pose video © 2020 john wiley sons ltd', 'deep learning utilized many intelligent systems including computer vision techniques human pose estimation one popular tasks computer vision benefited modern feature learning strategies regard recent advances partbased approaches since pose estimation parts produce accurate results human shape considered holistically one unbreakable deformable object however realword scenarios problems like occlusion cluttered background make difficulties partbased paper unite two attitudes partbased holistic pose predictions make accurate robust estimations two schemes modeled convolutional neural networks regression classification tasks order combined three multitasking series parallel settings advantages experimental results lsp test set demonstrate essential observe subjects parts holistically order achieve accurate robust estimation human pose challenging scenarios © 2020 springerverlag gmbh germany part springer nature', 'paper new single shot multiperson 3d human pose estimation complex images model jointly learns locate human joints image estimate 3d coordinates group predictions full human skeletons deals variable number people need bounding boxes estimate 3d poses leverages extends stacked hourglass network multiscale feature learning manage multiperson situations thus exploit robust 3d human pose formulation fully describe several 3d human poses even case strong occlusions crops joint grouping human pose estimation arbitrary number people performed associative embedding significantly outperforms state art challenging cmu panoptic previous single shot mupots3d dataset furthermore leads good results complex synthetic images newly jta dataset © 2020', 'recent years 3d human pose estimation deep learning developed one important elements realtime processing however topdown one 3d multiperson human pose estimation fast enough realtime processing paper downsize input resolution heatmap rootnet posenet subframeworks 3dmppe heatmap resolution becomes smaller computation cost processing time decrease hence studied tradeoff relation performance processing time heatmap resolution reduced input resolution reduced 256 128 processing time rootnet decreased 30 performance decreased 4 © 2021', 'human pose estimation hpe received considerable attention past years improving performance thanks deep learning introducing new interesting application sport physical exercise spe aim systematic review analyze literature related application hpe spe available data performance opportunities challenges one reviewer applied different inclusion exclusion criteria well quality metrics perform paper filtering paper databases association computing machinery digital library web science dblp included 500 related initial filtering finally resulting 20 addition research carried regarding publicly available data related topic concluded even related public data found much data needed able obtain good performance different contexts relation authors general purpose systems openpose combined adaptations specific case found finally limitations opportunities challenges presented © 2021 authors licensee mdpi basel switzerland', 'reconstructing 3d pose multiple people viewed several cameras complex problem 2d human pose estimation performed image separately eg deep learning algorithm next step matching individuals different images present contribution novel nondeep learning parameterfree algorithm developed end geometrical cues ie reprojection errors shown achieve perfect precision recall figures challenging 102person benchmark © 2022 authors exclusive license springer nature switzerland ag', 'event camera emerging imaging sensor capturing dynamics moving objects events motivates work estimating 3d human pose shape event signals events hand unique challenges rather capturing static body postures event signals best capturing local motions leads us twostage deep learning called eventhpe firststage flownet trained unsupervised learning infer optical flow events events optical flow closely related human body dynamics fed input shapenet second stage estimate 3d human shapes mitigate discrepancy imagebased flow optical flow shapebased flow vertices movement human body shape novel flow coherence loss introduced exploiting fact flows originated identical human motion inhouse eventbased 3d human dataset curated comes 3d pose shape annotations far largest one knowledge empirical evaluations dhp19 dataset inhouse dataset demonstrate effectiveness © 2021', '106 special focus conference image vision intelligent systems topics fuzzy image processing deep learning survey fool hashingbased video retrieval system perturbing last 8 frames video multilevel road damage identification algorithm vehiclemounted smartphone digital twin model battery management systems concepts algorithms platforms research remaining useful life solenoid valve millimeter wave radar multilink data congestion control algorithm spatial delay tolerance network datadriven fault prognosis pneumatic valves train electropneumatic brake system rcs periodicity extraction algorithm ballistic target semisupervised generative adversarial network face antispoofing improving apple detection retinanet megavoltage computed tomography mvct imaging quality improvement via convolutional neural network research application railway turnout gap detection improved canny algorithm 3d vision transformer postoperative recurrence risk prediction liver cancer attentionaware unet network segmentation retinopathy region lesn lowlight image enhancement via siamese network lightweightimproved cnn vgg16 identification classification rice diseases pests compressed channel attention mechanism 3d medical image segmentation liver survey object tracking deep learning action detection transfer learning human pose estimation finegrained visual classification wisely feature map filtering mechanism study prostate image segmentation improved unet music autotagging attention mechanism multilabel classification overview text steganalysis', 'present human pose estimation depth images faces challenges deep learning perform well rely massive amounts data traditional machine learning simple implement depend feature extraction low accuracy deal paper novel manifold gaussian process combines tomographic image denoising feature fusion solve human pose estimation depth images experimental prediction accuracy itop datasets outperforms machine learning achieving 833 779 full body front view top view respectively proves effectiveness manifold gaussian process human pose estimation depth images © 2022 spie', 'recent advances deep learning approaches computer vision problems led renewed interest task predicting 3d human joint locations raw image data application areas including sports analysis humancomputer interaction physical rehabilitation although supervised learning deep neural networks proven effective pose estimation requires wealth varied data generalise well previously unseen examples consequently progress 3d human pose estimation slowed fact 3d keypoint annotations notoriously difficult obtain traditionally requiring large array cameras andor wearable markerssensors paper describe methodology obtaining 3d human pose annotations three video cameras without wearables apply methodology construct aspset510 australian sports pose dataset large collection natural sportsrelated video 3d pose annotations aspset510 additional source training examples found could improve pose model generalisation established mpiinf3dhp benchmark make aspset510 publicly available provide strong baseline results future work compare © 2021 elsevier bv', 'occlusion major challenge effective human pose estimation occurring naturally high percentage realworld images handling occlusion difficult challenge literature due lack proper dataset actual focus occlusion prompting researchers create artificial datasets means data augmentation however datasets lack features realworld occlusion work introduce new realistic data augmentation built top dataset human36m tackles issue creating realistic samples similar found wild arguing cnn models pay higher attention local opposed global features define occlusion levels select many toocclude objects different categories blend within original image dataset test topperforming 2d human pose estimation models without occlusionaugmented dataset called realocc display drop performance occlusion train new dataset show increase accuracy model occlusion without change models © 2021', 'supervised deep learning pixelwise training labels great successes multiperson part segmentation however data labeling pixellevel expensive solve problem people exploring synthetic data avoid data labeling although easy generate labels synthetic data results much worse compared real data manual labeling degradation performance mainly due domain gap ie discrepancy pixel value statistics real synthetic data paper observe real synthetic humans skeleton pose representation found skeletons effectively bridge synthetic real domains training takes advantage rich realistic variations real data easily obtainable labels synthetic data learn multiperson part segmentation real images without humanannotated labels experiments show without human labeling performs comparably several approaches require human labeling pascalpersonparts cocodensepose datasets hand part labels also available realimages training outperforms supervised large margin demonstrate generalizability predicting novel keypoints real images real data labels available novel keypoints detection code pretrained models available httpsgithubcomkevinlin311twcdclhumanpartsegmentation © 19912012', 'researchers provide us newer deeper structures neural networks every year confirming effectiveness concerning earlier versions architectures common assumption creating current structures ignore performance issues one hand theoretically obtain better better classification regression hand despite significant development mobile devices lose possibility implementation systems accessible humans mobile phones aware social expectations towards technologies developed create algorithms improve previous versions operation optimize performance study compares operation inceptionv3 v4 networks precision speed regression process estimation ability determined part studying position human joints 3d space loco architecture one leading 3d human pose estimation used experiments © 2021', 'deep learningbased human pose estimation require large volumes training data achieve superior performance however data acquisition classroom environments raises privacy concerns undoubtedly hinder development latest deep learning techniques education domain due absence large richly annotated classroom datasets research classroom observation done manually collecting annotating datasets unfortunately annotation data timeconsuming challenging overcrowded classrooms break limitations open source synpose large densely labeled synthetic dataset specifically designed crowded human pose estimation classroom meeting scenarios moreover novel ctgan bridge domain gap comprehensive experiments realworld classroom images show dataset deliver important performance benefits compared existing datasets revealing potential synpose future studies © 2022', '44 topics discussed multiscale regional attention networks pain estimation brain modeling guided direct volume rendering biomedical image segmentation classification supervision deep multitask learning shadow detection removal category map guided ordinal depth prediction 3d human pose estimation whole genome analysis polyadenylation sites long noncoding rnas oryza sativa molecular cloning sequence analysis perforin granzyme b coding sequences grass carp ctenopharyngodon idella extracting evaluating features rna virus sequences predict host species susceptibility deep learning molecular cloning identification stat4 grass carp ctenopharyngodon idella identification genes altered roles transcription network alzheimer ’ disease', 'recent improvements deepfake creation made deepfake videos realistic moreover opensource software made deepfake creation accessible reduces barrier entry deepfake creation could pose threat people ’ privacy potential danger deepfake creation techniques used people ulterior motive produce deepfake videos world leaders disrupt order countries world therefore research automatic detection deepfaked media essential public security work deepfake detection upper body language analysis specifically manytoone lstm network designed trained classification model deepfake detection different models trained varying hyperparameters build final model benchmark accuracy achieved 9439 accuracy deepfake test set experimental results showed upper body language effectively detect deepfakes © 2021 authors', 'health crisis construction companies tremendous pressure adopt innovative techniques employee management meet health safety demands reinstate productivity starting managing attendance monitoring performance construction industry implement equipments robots address worker safety maintaining safe distance thus reducing labor force intern results manmachine interaction conflicts accidents due failure machines components needs addressing workplace accidents south korea occur construction industry 60 due falls heights aging population south korea makes construction industry prone mental fatigue resulting accidents thus study aims integrate systems health safety performance pose estimation techniques deep learning techniques iiot industrial internet things ict wireless access points wearable devices smart bands shoes helmets aids monitor workers performance health safety indoor conditions structural frame completion combining previous warning alert systems pose estimation monitoring alert workers unsafe uncomfortable poses improve productivity significantly future version system could lora extend outdoor environments © 2021', 'deep learning technology improved performance visionbased action recognition algorithms require large number labeled training datasets resulting weak universality address issue paper novel selfdeployable ubiquitous action recognition enables selfmotivated user bootstrap deploy action recognition services called follower main idea build “ fingerprint ” library actions small number userdefined sample action data matching complete action recognition key step construct suitable “ fingerprint ” thus pose action normalized feature extraction threedimensional pose sequence designed follower mainly composed guide process follow process guide process extracts pose action normalized feature selects inner class central feature build “ fingerprint ” library actions follow process extracts pose action normalized feature target video motion detection action filtering adaptive weight offset template identify action video sequence finally collect action video dataset human pose annotation research selfdeployable action recognition action recognition pose estimation experimenting dataset results show follower effectively recognize actions video sequence recognition accuracy reaching 9674 © 2021 authors licensee mdpi basel switzerland', 'despite success deep learning human pose estimation remains challenging problem particular dense urban traffic scenarios robustness important followup tasks like trajectory prediction gesture recognition interested human pose estimation crowded scenes overlapping pedestrians particular pairwise constellations new topdown relies pairwise detections input jointly estimates two poses pairs single forward pass within deep convolutional neural network availability automotive datasets providing poses fair amount crowded scenes limited extend eurocity persons dataset additional images pose annotations 46 975 images poses 279 329 persons new eurocity persons dense pose dataset largest pose dataset recorded moving vehicle experiments dataset show improved performance poses pedestrian pairs comparison state art human pose estimation crowds © 2021', 'order solve problem impact speed poor detection performance common keypoints detection caused uncertain number people image relative size different human bodies body parts improved densenet network structure human pose estimation network structure singlestage endtoend network deep convolutional neural networks feature extraction end convolutional network get 6 different scales feature maps specific scaletransfer structure network integrate different levels features multiscale keypoints detection effectively improves detection accuracy keypoints bottomup adopted ensure processing speed multiperson pose estimation task experiments show improves mean average precision multiperson keypoints detection 1 compared general provides new balancing speed accuracy attitude estimation copyright ©2021 control decision', 'study human body motion throughout different activities one challenging longstanding problems computer vision recent advances deep learning algorithms information acquired conventional frame sensors used infer human body pose analysis specifically one contemplate performance body movements throughout physical exercises provide feedback individual therefore system workout repetition counting validation set skeletonbased deep semantic features obtained 2d human pose estimation network end acquired 130 participants performing five popular crossfit exercises order train convolutional neural network predict exercises ’ moments frame level hence underlying idea inferring moment exercise twofold provide information exercise execution finelevel detail ii ability detect invalid repetitions promptly finally repetition counting validation module receives predicted moment outputs current number valid repetitions one performing 92 precision scores 4 5 considered exercises © 2021 elsevier bv', '3d human pose shape recovery monocular rgb image challenging task existing learning highly depend weak supervision signals eg 2d 3d joint location due lack inthewild paired 3d supervision however considering 2dto3d ambiguities existed weak supervision labels network easy get stuck local optima trained labels paper reduce ambituity optimizing multiple initializations specifically threestage named multiinitialization optimization network mion first stage strategically select different coarse 3d reconstruction candidates compatible 2d keypoints input sample coarse reconstruction regarded initialization leads one optimization branch second stage design mesh refinement transformer mrt respectively refine coarse reconstruction result via selfattention mechanism finally consistency estimation network cen find best result mutiple candidates evaluating visual evidence rgb image matches given 3d reconstruction experiments demonstrate multiinitialization optimization network outperforms existing 3d mesh multiple public benchmarks © 2021 acm', 'introduce novel collecting table tennis video data perform stroke detection classification diverse dataset containing video data 11 basic strokes obtained 14 professional table tennis players summing total 22111 videos collected setup temporal convolutional neural network model developed 2d pose estimation performs multiclass classification 11 table tennis strokes validation accuracy 9937 moreover neural network generalizes well data player excluded training validation dataset classifying fresh strokes overall best accuracy 9872 various model architectures machine learning deep learning approaches trained stroke recognition performances compared benchmarked inferences performance monitoring stroke comparison players model discussed therefore contributing development computer vision sports analytics system sport table tennis focuses previously unexploited aspect sport ie players strokes extremely insightful performance improvement © 2021', '3d animation human body movement quite challenging involves huge setup several motion trackers persons body track movements every limb time consuming may cause person discomfort wearing exoskeleton body suits motion sensors work present trivial yet effective solution generate simple 3d animation human movement multiple persons 2d video deep learning although significant improvement achieved recently 3d human pose estimation prior works work well case single person pose estimation multiperson pose estimation still challenging problem work firstly supervised multiperson 3d pose estimation animation namely animepose given input rgb video sequence pipeline system consists various modules multiperson 2d pose estimation ii depth map estimation iii lifting 2d poses 3d poses iv person trajectory prediction human pose tracking system produces comparable results previous 3d multiperson pose estimation publicly available dataset mupots3d dataset also outperforms previous competing human pose tracking significant margin 117 performance gain mota score posetrack 2018 dataset © 2021 elsevier bv', 'paper presents rlbased improve performance realtime 3d human pose estimation positioning feedback dermdrone micro sized quadrotor designed metaoptima capture high resolution full body images dermatology application camera viewpoint identified key parameter accuracy monocular 3d human pose estimation present deep reinforcement learning determining best viewpoint given flight trajectory goal present reliable accurate positioning feedback dermdrone 3d human pose estimation algorithm dqn variants double dqn dueling dqn employed performances investigated conducting several simulations results confirm rlbased viewpoint selection improve performance 3d human pose estimation © 2021 icros', 'automatic athlete pose estimation images recently received considerable attention computer vision community understand correct pose athletes training competitions however human pose estimation images videos still challenging task inadequate training data depth obscurity occlusion paper presented realtime 2d athlete pose estimation system openpose 1 system captured 2d positions hurdles athletes body parts wrists ankles hips knees image experiments recorded videos toptier athlete hurdling infer motion timing kinematic parameters also explore performance different feature extractorsin openpose terms accuracy runtime experimental results show system extended version vgg19 2 feature extractor outperforms algorithms © 2021', 'vehicle pose estimation useful applications selfdriving cars traffic monitoring scene analysis recent developments computer vision deep learning achieved significant progress human pose estimation little work applied vehicle pose vehipose efficient architecture vehicle pose estimation multiscale deep learning achieves high accuracy vehicle pose estimation maintaining manageable network complexity modularity vehipose architecture combines encoderdecoder architecture waterfall atrous convolution module multiscale feature representation aims reduce loss due successive pooling layers preserve multiscale contextual spatial information encoder feature representations waterfall module generates multiscale features leverages efficiency progressive filtering maintaining wider fieldsofview concatenation multiple features multiscale results robust vehicle pose estimation architecture incorporates contextual information across scales performs localization vehicle keypoints endtoend trainable network © copyright spie downloading abstract permitted personal', 'paper tackles problem predicting 3d human pose ai edge devices ai edge device aienabled internet things iot device run deep learning dl models right device without connecting internet recent years researchers numerous dlbased models 3d human pose estimation hpe work focuses solving task edge devices building dl model edge device unique challenges limited computation capacity small memory low power paper investigates optimize big heavycomputing 3d pose estimation dl model lightweight smallsize model run efficiently device specially endtoend pipeline run 3d human pose prediction hpp realtime ai edge devices furthermore endtoend pipeline general employed ai edge device realworld applications © 2021 authors exclusive license springer nature singapore pte ltd', '3d human pose reconstruction hpr challenging task due less availability 3d ground truth data projection ambiguity address limitations threestage deep network workflow 2d human pose estimation hpe followed 3d hpr utilizes frame specific pose estimation fspe multistage cascaded feature connection mscfc feature residual connection frc sublevel strategies first stage fspe concept mscfc strategy used 2d hpe second stage basic deep learning concepts like convolution batch normalization relu dropout utilized frc strategy spatial 3d reconstruction last stage lstm deep architecture used temporal refinement effectiveness technique demonstrated mpii human36m humanevai datasets experiments observed gives competitive results recent techniques © 2020 elsevier inc', 'imagebased virtual tryon vton systems deep learning attracted research commercial interests although show strengths blending person tryon clothing image synthesizing disoccluded regions results complexposed persons often unsatisfactory due limitations geometry deformation texturepreserving capacity address challenges clothvton seamlessly integrating imagebased deep learning strength 3d model shape deformation specifically fully automatic pipeline developed 3d clothing model reconstruction deformation reference human model first tryon clothing matched target clothing regions simple shaped reference human model 3d clothing model reconstructed reconstructed 3d clothing model generate natural pose shape transfer retaining textures clothes clothing refinement network refines alignment eliminating misalignment due errors human pose estimation 3d deformation deformed clothing images combined utilizing conditional generative networks inpaint disoccluded areas blend experiments existing benchmark dataset demonstrate clothvton generates higher quality results comparison vton systems clothvton clothvton incorporated extended applications multipose guided video vton © 2013', 'perceptions motion capture mocap technology increasing every day variety applications doubling leveraging resources offered mocap technology human activity characteristics captured used source animation devices involved technology therefore costly hence practical personal scenario implement capable producing mocap data standard rgb video animate character 3d space action person original video help deep learning techniques human mesh recovery hmr scheme used extract mocap data input video determine joints person input video located 3d space 2d pose estimation locations 3d joints used mocap data transferred blender simple 3d character character animated subjective evaluation metric called observation factor performed yielded accuracy value 735 © 2020', 'falls one major causes injury death among elderly people aged 65 industries fall detection system necessary identify falling activities activities daily lives even though new shown research paper number studies visionbased systems still increasing existing visionbased fall detection systems lot weakness generalized mainly due difficulties variations physical appearances different camera viewpoints occlusions background clutter darkness head upper body location human provides critical information time fall paper presents visionbased fall tracking upper body joints grouped one segment increase fall classification ratio segment consist head shoulders joints combinations represents upper region head region segment beneficial achieve efficient tracking human activities provide strong technic distinguish falls activities daily lives © 2021', 'gait analysis widely used clinical practice help understanding gait abnormalities association certain underlying medical condition better diagnosis prognosis several technologies embedded specialized devices computerinterfaced video cameras measure patient motion electrodes placed surface skin appreciate muscle activity force platforms embedded walkway monitor forces torques produced ambulatory patient ground inertial measurement unit imu sensors wearable devices used purpose technologies require expert translate data recorded said embedded specialized devices typically done medical expert recent improvements field artificial intelligence ai especially deep learning possible create mechanism translation data performed deep learning tool convolutional neural network cnn therefore work presents human pose estimation combined cnn classification normal abnormal gait human ability provide information detected abnormalities form extracted skeletal image realtime © 2020 institute electrical electronics engineers inc rights reserved', 'paper presents novel dynamic gesture recognition multifeatures extracted rgb data input challenges gesture recognition revolve around axis presence multiple actors scene occlusions viewpoint variations paper develop gesture recognition hybrid deep learning rgb frames 3d skeleton joint information body part segmentation used overcome problems extracted rgb images multimodal input observations combined multimodal stream networks suited different input modalities residual 3d convolutional neural networks resnet architecture 3dcnnresnet rgb images color body part segmentation modalities long shortterm memory network lstm 3d skeleton joint modality evaluated model four public datasets utd multimodal human action dataset gaming 3d dataset ntu rgbd dataset msrdailyactivity3d dataset experimental results datasets proves effectiveness © 2020 authors', 'despite remarkable success supervised deep learning models 3d human pose estimation performance models mostly limited constrained laboratory settings models exhibit alarming level dataset bias also fail operate unconstrained videos presence external variations camera motion partial body visibility occlusion etc acknowledging shortcomings firstly aim formalize motion representation learning effectively utilizing constrained artificially generated unconstrained video samples datasets 3d pose annotation without ignoring inherent uncertainty pose estimation truncated video frames devise novel probabilistic amodal pose completion enable generation multiple plausible posefilling outcomes secondly address dataset bias probabilistic amodal reutilized design novel selfsupervised objectives enables adaptation model target unannotated datasets wild youtube videos also encourages learning generic motion representations beyond available supervised data even unconstrained scenarios training regime helps us achieve stateofthe art performance unsupervised crossdataset pose estimation significant improvement partiallyvisible unconstrained scenarios © 2020', 'paper novel baseline deep person reid introducing human posebased attention mechanism benefiting deep convolutional network great progress person reidentification reid recent years aims retrieving person identities images captured different cameras existing focus designing complex network structures achieve higher scores public datasets works pay attention baseline design strong baseline crucial experiments could make elaborated convincing present study makes pretrained human pose estimator extract human keypoint information novel manner fuse pose information global feature resnet50 could lead network concentrate discriminative keypoint feature areas work could achieve 948 rank1 accuracy 874 mean average precision map market1501 outperform existing baselines resnet50 best knowledge whats experiment results also suggest help pose information work could naturally robust misalignment occlusion problems © 2021 spie', 'research singleperson pose estimation deep neural networks recently witnessed progress accuracy execution efficiency however multiperson pose estimation still challenging topic partially object regions selected greedily proposals via classagnostic nonmaximum suppression nms misalignment redundant detection yields inaccurate human poses therefore consider obtain optimal input human pose estimation conditions intermediate label information available supervised learningbased alignment generalize well unseen samples human pose space article present maskaware deep reinforcement learning modify detection result mask information remove adverse effects cluttered background select optimal action according revised reward function also new regularization term punish joints outside silhouette region human pose estimation stage evaluate mpii multiperson dataset mscoco keypoints challenge results show yields competing inference results compared approaches © 2020 acm', 'previous studies shown athletic jump mechanics assessments valuable tools identifying indicators individuals anterior cruciate ligament injury risk assessments drop jump test often relied camera systems sensors always accessible practical screening individuals sports setting human pose estimation deep learning models improve envision transitioning biometrical assessments mobile devices addressed two preclusive hindrances current models accuracy lower limb joint prediction slow runtime inthewild inference tackle issue accuracy adding postprocessing step compatible inference outputs 3d key points additionally overcome lengthy inference rate depth estimation runs realtime function 2d human pose estimation model outputs coco key points solution paired model 3d human pose estimation significantly increased lowerlimb positional accuracy furthermore paired realtime joint depth estimation algorithm plausible solution developing first mobile device prototype athlete jump mechanics assessments © 2021', 'present deep learningbased multitask joint 3d human pose estimation action recognition rgb sensors simple cameras proceeds along two stages first realtime 2d pose detector run determine precise pixel location important keypoints human body twostream deep neural network designed trained map detected 2d keypoints 3d poses second stage efficient neural architecture search enas algorithm deployed find optimal network architecture used modeling spatiotemporal evolution estimated 3d poses via imagebased intermediate representation performing action recognition experiments human36m msr action3d sbu kinect interaction datasets verify effectiveness targeted tasks moreover show requires low computational budget training inference particular experimental results show monocular rgb sensor develop 3d pose estimation human action recognition reaches performance rgbdepth sensors opens many opportunities leveraging rgb cameras much cheaper depth cameras extensively deployed private public places build intelligent recognition systems © 2020 authors licensee mdpi basel switzerland', '6d human pose estimation studied assist autonomous uav control human environments autonomous robotsuavs become increasingly prevalent future workspace autonomous robots must detectestimate human movement predict trajectory plan safe motion path utilize deep convolutional neural network calculate 3d torso bounding box determine location orientation human objects training loss function includes 3d angle translation errors trained model delivers lt10degree angular error outperforms reference rsn © 2021 springer nature switzerland ag', 'paper analyze locationfollowing processing image successive approximation need directed privacy solve detection problem moving human body dynamic background motion target detection module integrates two ideas feature information detection human body model segmentation detection combines deep learning complete detection human body detecting feature points key parts human body detection human key points depends human pose estimation algorithm research paper bottomup model multiperson pose estimation firstly human key points image detected feature extraction convolutional neural network accurate labelling human key points achieved heat map offset fusion optimization feature point confidence map prediction finally human body detection results obtained study correlation algorithm paper combines hog feature extraction kcf algorithm scale filter dsst algorithm form fusion correlation filter principle study mosse correlation filter algorithm solves problems lack scale estimation kcf algorithm low realtime rate dsst algorithm improves tracking accuracy ensuring realtime performance algorithm © 2021 ying miao et al', 'nowadays improvement life quality people gradually realize importance mass entertainment physical fitness growing number people watch sports events plan run together leisure time existing researches human running speed detection always ignored characteristics human body complex detection auxiliary equipment needs worn detection intelligent detection achieved therefore designed system intelligently detects running speed people provides platform runners monitor share running status realtime auxiliary means sports events paper design human pose system extracts key point information images detect speed high precision additionally system realize intelligent detection solving detection relatively static position human body data collected participant walkingrunning different speeds treadmill experiment key point accuracy human pose estimation system simple baselines combined mpii data set 892 model meet accuracy requirements speed detection actual operation process system completed detection task experiment average relative error speed sequences 489 © 2020', 'paper investigate benefit 3d hand skeletal information task sign language sl recognition rgb videos within multiplestream deeplearning recognition system sl datasets available traditional rgbonly video lacking depth information infer 3d coordinates hand joints rgb data via powerful architecture primarily introduced literature task 3d human pose estimation fuse estimates additional sl informative streams namely 2d skeletal data well convolutional neural networkbased hand mouthregion representations employ attentionbased encoderdecoder recognition evaluate corpus isolated signs greek sl dataset continuous fingerspelling american sl reporting significant gains inclusion 3d hand pose information also outperforming databases evaluate 3d hand pose estimation technique standalone © 2020 springer nature switzerland ag', 'existing deep learning 3d human pose estimation deployed resourceconstrained devices paper tinyhourglassnet 3d human pose estimation improves efficiency stacked hourglass networks guarantee estimation performance concrete develop efficient tinyhourglass backbone two different types shufflenet v2 blocks meantime present simple estimation head seh reduce resource utility depth prediction furthermore develop two feature enhancement modules namely feature enhancement module fem intermediate enhancement module iem improve feature representation ability tinyhourglassnet without evident increment resource usage computational complexity experimental results demonstrate tinyhourglassnet achieves equivalent accuracy reduction 80 complexity flops comparison baselines © 2020', 'deep learning revolutionized data science recently popularity grown exponentially amount employing deep networks vision tasks human pose estimation escape trend large number deep models small changes network architecture data preprocessing together stochastic nature optimization procedures produce notably different results making extremely difficult sift significantly outperform others situation motivates current study perform systematic evaluation statistical analysis vanilla deep regression ie convolutional neural networks linear regression top layer first comprehensive analysis deep regression techniques perform experiments four vision problems report confidence intervals median performance well statistical significance results surprisingly variability due different data preprocessing procedures generally eclipses variability due modifications network architecture results reinforce hypothesis according general generalpurpose network eg vgg16 resnet50 adequately tuned yield results close without resort complex adhoc regression models © 19792012', 'action recognition 3d human pose estimation fundamental problems computer vision closely related areas work multitask neural network action recognition 3d human pose estimation results previous usually errorprone especially tested images taken inthewild leading error results action recognition solve problem principled generate high quality 3d pose ground truth given inthewild image person inside achieve first devising novel stereo inspired neural network directly map 2d pose high quality 3d counterpart highquality 3d labels carefully design multitask action recognition 3d human pose estimation architecture utilize shallow deep features images inthewild 3d human keypoints guide precise result high quality 3d keypoints fully reflect morphological features motions thus boost performance action recognition experimental results demonstrate 3d pose estimation leads significantly higher performance action recognition separated learning also evaluate generalization ability quantitatively qualitatively architecture performs favorably baseline 3d pose estimation addition reported results penn action ntu datasets demonstrate effectiveness action recognition task © 2020', 'injury resulted repetitive loadbearing works frequent workrelated musculoskeletal disorders wmsd cumulative trauma disorders ctd comes overload repetitive loadbearing actions resulting fatigue inflammation even injuries musculoskeletal system according annular report labor insurance bureau taiwan wmsd 8588 payment thus aim study evaluate risk wmsd work simple quick correct deep learning algorithms research collection videos hand repeated movements ergonomic injuries evaluated 2d human pose estimation key indicator manual handling operations kimmho model predefined classifications deep learning approaches manual handling operating tasks built analysis results show classification accuracy 80 compared doctors judgment goal study get accuracy 90 achieve fast accurate assistance deciding risk ergonomics immediately give proper feedback © 2020', '3d human pose shape estimation plays vital role many computer vision applications many deep learning attempting solve problem relying singleview rgb images training network however since public datasets captured multiview cameras system novel tackle problem putting optimizationbased multiview modelfitting regressionbased learning loop multiview images firstly convolutional neural network cnn regresses pose shape parametric human body model smpl multiview images utilizing regressed pose shape initialization improved multiview optimization smplify mvsmplify fit smpl model multiview images simultaneously subsequently optimized parameters adopted supervise training cnn model whole process forms selfsupervising combine advantages cnn optimization collaborative process addition multiview images provide comprehensive supervision training experiments public datasets qualitatively quantitatively demonstrate outperforms previous approaches number ways © 2021', 'falls major cause injuries deaths elderly age 65 factor social costs various detection techniques introduced existing sensor fall detector devices still ineffective due user inconvenience response time limited hardware resources however since rnn recurrent neural network provides excellent accuracy problem analyzing sequential inputs paper fall detection skeleton data obtained 2d rgb cctv cameras particular feature extraction classification improve accuracy fall detection gru experiments conducted public datasets sdufall find featureextraction achieve high classification accuracy result various experiments find feature extraction achieve high classification accuracy effective detecting falls unprocessed raw skeletal data processed anything © 2021 authors exclusive license springer nature switzerland ag', 'detecting locations multiple actions videos classifying realtime challenging problems termed ” action localization prediction ” problem convolutional neural networks convnets achieved great success action localization prediction still images major advance occurred alexnet architecture introduced imagenet competition convnets since achieved performances across wide variety machine vision tasks including object detection image segmentation image classification facial recognition human pose estimation tracking however works exist address action localization prediction videos current action localization research primarily focuses classification temporally trimmed videos one action occurs per frame moreover nearly current approaches work offline slow useful realworld environments work fast accurate deeplearning perform realtime action localization prediction convolutional neural networks localize multiple actions predict classes real time starts appearance motion detection networks known ” look ” yolo networks localize classify actions rgb frames optical flow frames twostream model fusion step increases localization accuracy moreover generate action tube frame level detection frame frame processing introduces early action detection prediction top performance terms detection speed precision experimental results demonstrate superiority terms processing time accuracy compared recent offline online action localization prediction approaches challenging ucf10124 jhmdb21 benchmarks © 2020 elsevier ltd', 'deep learning technology machine learning computer vision developing stage recognize objects image video analyze meaning image general position lower body region important human body pose estimation technique estimating human position paper predicted seamless human positioning algorithm mrcnn obstacle environment indoor localization obtain continuous position estimation results even blind spots obstacles human detection pretest accuracy according amount movement range 0m5m confirmed 100 confirmed mrcnn pshp 2455 shorter mrcnn bbox cumulative error distance mrcnn pshp 3402 shorter mrcnn bbox respectively however error occurred left ankle coordinates ankles human image disappeared © 2021', 'order quickly accurately detect occurrence falls elderly paper presents realtime fall detection algorithm pose estimationfirstly human pose estimation algorithm deep learning used obtain coordinates joint point calculating falling speed centroid point human body falls whether ordinate value neck joint point fall greater threshold relative positional relationship shoulderwaist joint point image whether fall occurs determinedthe algorithm monocular camera detect easily used embedded way robotsthe experimental results show compared current advanced algorithm achieved good results © 2020 editorial office control decision right reserved', 'driven recent computer vision robotic applications recovering 3d human poses become increasingly important attracted growing interests fact completing task quite challenging due diverse appearances viewpoints occlusions inherently geometric ambiguities inside monocular images existing focus designing elaborate priors constraints directly regress 3d human poses corresponding 2d human poseaware features 2d pose predictions however due insufficient 3d pose data training domain gap 2d space 3d space limited scalabilities practical scenarios eg outdoor scene attempt address issue paper simple yet effective selfsupervised correction mechanism learn intrinsic structures human poses abundant images specifically mechanism involves two dual learning tasks ie 2dto3d pose transformation 3dto2d pose projection serve bridge 3d 2d human poses type free selfsupervision accurate 3d human pose estimation 2dto3d pose implies sequentially regress intermediate 3d poses transforming pose representation 2d domain 3d domain sequencedependent temporal context 3dto2d pose projection contributes refining intermediate 3d poses maintaining geometric consistency 2d projections 3d poses estimated 2d poses therefore two dual learning tasks enable model adaptively learn 3d human pose data external largescale 2d human pose data apply selfsupervised correction mechanism develop 3d human pose machine jointly integrates 2d spatial relationship temporal smoothness predictions 3d geometric knowledge extensive evaluations human36m humanevai benchmarks demonstrate superior performance efficiency compared competing © 19792012', 'tackle issue data imbalance different poses human pose estimation problem explore unusual poses rare occupy small portion pose dataset order identify rare pose without additional learning simple kmeans clustering algorithm applied given dataset experimental results mpii coco datasets show outliers far nearest cluster center defined rare poses accuracy decreases distance data point cluster center increases order improve performance rare poses three problem data scarcity addition rare pose duplicates addition synthetic rare pose data weighted loss distance cluster highest increasing score 135 map rare pose data © 2020 institute electrical electronics engineers inc rights reserved', 'poststroke care encounters challenges including high cost lack professionals insufficient rehabilitation state evaluation computer technology alleviate issues allows health care professionals hcp quantify workload thus enhance rehabilitation care quality paper novel multimodel fusion terms pose dualstream network pdsn devised aiming test feasibility monitoring training actions rehabilitating stroke patients care management particular deeplearningbased algorithm combines human pose estimation dualstream networks innovative way utilize improved openpose estimate human pose videos obtained lowcost monocular camera dualstream networks spatial motion streams flexibly integrated spatial stream network combines gated recurrent unit gru attention mechanism extract spatiotemporal data motion stream network composed improved multilayer 1d convolutional neural networks cnn enhanced causal dilated convolution skillfully additionally adaptive weight fusion strategy used fuse two networks final action classification results show high accuracy two public datasets dataset created us validate superiority feasibility © 2013', '25 special focus conference frontiers computer vision topics challenges applications face deepfake study image processing capillaries microscope initial considerations stairstep feature pyramid networks object detection 2nd korean emotion recognition challenge results crack detection location estimation convolutional neural network rice leaf diseases recognition deep learning hyperparameters customization stgcn human action recognition abstracted three features optical flow image gradient deep visual anomaly detection negative learning video analysis wheel pushing actions wheelchair basketball players robust tracking via feature enrichment overlap maximization efficient spatialattention module human pose estimation gcncalculated graphfeature embedding 3d endoscopic system active stereo uncalibrated photometric stereo superquadrics cast shadow robust training deep neural networks noisy labels graph label propagation fast separation specular diffuse global components via polarized pattern projection saliency prediction relationaware global attention module emerging field graph signal processing moving object segmentation multiscale global reasoning unit semantic segmentation multimodality affective video summarization game players focusing discrimination road conditions weather driving video analysis age estimation age period triplet network development algae counting application support vegetation surveys fishing grounds ishigaki region extraction grabcut algorithm support kumamoto castle reconstruction', 'easytosearch semantic information human clothing attributes important research value field computer vision existing attribute recognition encounter problems interference environmental factors result show poor clothing positioning accuracy address problems human attribute recognition human pose estimation multiplefeature fusion first retrieval results obtained subsequent attribute recognition appearance feature matching deep ssdbased human pose estimation foreground area belonging human image located background interference excluded finally analytical results various combined iterative smoothing process maximum posteriori probability assignment adopted enhance correlation attribute labels pixels final attribute recognition results obtained experiments benchmark dataset show performance model improved solves problems inaccurate clothing label recognition pixel resolution area deviation single recognition mode © 2020 springerverlag london ltd part springer nature', 'computer vision human pose estimation following action recognition active research topics wide application human computer interaction smart homes athlete assistance training etc driven applications numerous algorithms models recent years among deep learning becomes dominant study summarize recent progress deep learning human pose estimation action recognition also point research challenges future research directions © 2021', 'approximately 37 million falls occur year worldwide requiring medical attention victims often helpless able call help risk elderly persons living alone detect falls home several approaches video cameras used increasingly recently high accuracy realtime human pose estimation videos achieved novel machine learning techniques work multicamera system videobased fall detection augment human pose estimation openpifpaf algorithm support multicamera multiperson tracking long shortterm memory lstm neural network predict two classes fallor fall poses extract five temporal spatial features processed lstm evaluation identification tracking multiple cameras used videos recorded smart home living lab two persons walking interacting evaluation fall detection used upfall detection dataset achieve f1 score 925 observed tendency towards false positive classifications due lack activities publicly available datasets look similar falls stem normal activity moreover lack variation activities also results higher amount false positives requires acquisition balanced datasets future work conclusion realtime fall detection multiple camera inputs multiple persons feasible lstm neural network combined features obtained via human pose estimation source code available httpsgithubcomtaufeeque9humanfalldetection © copyright spie downloading abstract permitted personal', 'deep bayesian networks hot topic deep learning makes possible minimize epistemic homoscedastic uncertainty time self balancing multiple complementary losses given task simply employing standard operations dropout mean squared error crossentropy hand capsule networks novel dnn architecture offer richer representation concept represented number different vectors bayesian formulation capsule networks still open problem address paper present bayesian formulation capsule networks compare performance illposed regression problem estimating 3d human pose single 2d image results show network competitive much straightforward solution enable fair comparisons source code openly available httpsgithubcomrollervanbcn3dpose © 2019 elsevier bv', 'landmarkpose estimation single monocular images received much effort computer vision due important applications remains challenging task input images come severe occlusions caused eg adverse camera views circumstances biologically implausible pose predictions may produced contrast human vision able predict poses exploiting geometric constraints landmark point interconnectivity address problem incorporating priors structure pose components novel structureaware fully convolutional network implicitly take priors account training deep network explicit learning constraints typically challenging instead inspired human identifies implausible poses design discriminators distinguish real poses fake ones biologically implausible ones pose generator g generates results discriminator fails distinguish real ones network successfully learns priors training network follows strategy conditional generative adversarial networks gans effectiveness network evaluated three poserelated tasks 2d human pose estimation 2d facial landmark estimation 3d human pose estimation significantly outperforms several almost always generates plausible pose predictions demonstrating usefulness implicit learning structures gans © 19792012', 'graph convolution network gcn successfully used 3d human pose estimation videos however often built fixed humanjoint affinity according human skeleton may reduce adaptation capacity gcn tackle complex spatiotemporal pose variations videos alleviate problem novel dynamical graph network dgnet dynamically identify humanjoint affinity estimate 3d pose adaptively learning spatialtemporal joint relations videos different traditional graph convolution introduce dynamical spatialtemporal graph convolution dsgdtg discover spatialtemporal humanjoint affinity video exemplar depending spatial distancetemporal movement similarity human joints video hence effectively understand joints spatially closer andor consistent motion reducing depth ambiguity andor motion uncertainty lifting 2d pose 3d pose conduct extensive experiments three popular benchmarks eg human36m humanevai mpiinf3dhp dgnet outperforms number recent sota approaches fewer input frames model size © 19922012', 'government keeps attaching importance public security nonoverlapping viewsheds surveillance systems deployed widely become especially important recognize pedestrian target matching cameras different viewsheds nowadays deep learning relies big data solve overfitting however current videobased person reidentification small data volume homogeneous learning features solve put forward improve person reidentification video increase sample quantity generating video frame sequence generative adversarial network also adds feature information pedestrian joints improve model efficiency experiment result shows modified discussed paper improve recognition rate public datasets effectively experiments prid2011 ilidsvid rank 1 attained 802 663 respectively copyright © 2020 acta automatica sinica rights reserved', 'deep neural networks dnn dominate state art results computer vision cv fields one primary reasons dnn outperform existing algorithms produce superior results labelled data used unlike classic cv techniques nonetheless well known dnn requires large amount data generalise well collecting labelling datasets expensive timeconsuming sometimes impossible therefore researchers tried alternative techniques graphics simulators automatically generate labelled datasets however techniques still expensive require domain knowledge produce good datasets paper therefore graphics simulator presented automatically generates multimodel datasets realtime providing corresponding ground truth annotation tool concentrates pedestrian crowd analysis including 3d human pose estimation pedestrian detection well crowd density flow estimation © 2020', 'rapid development computer vision human pose tracking attracted increasing attention recent years address privacy concerns desirable develop techniques without video camera end rfid tags used lowcost wearable sensor provide effective solution 3d human pose tracking user adaptability another big challenge rf pose tracking ie welltrained model untrained subjects paper cyclepose subjectadaptive realtime 3d human pose estimation system deep learning assisted computer vision model training cyclepose rfid phase data calibrated effectively mitigate severe phase distortion high accuracy lowrank tensor completion halrtc employed impute missing rfid data cycle kinematic network remove restriction paired rfid vision data model training resulting system subjectadaptive achieved learning transform rfid data human skeleton different subjects prototype system developed commodity rfid tagsdevices evaluated experiments compared traditional system rfidpose higher pose estimation accuracy subject adaptability demonstrated cyclepose experiments kinect 20 data ground truth © 2020', 'real world estimation human pose gained considerable consideration owed diverse application 2d pose estimation remarkable research achieves targeted output however challenges still remain 3d pose estimation deep learning improve presentation human pose estimation also brings closest result literature review deep learning human pose estimation presented analyzes methodology used paper also includes realworld video crowded scene pose estimation latest research information methodologybased taxonomy sum discuss recent works also addresses compares datasets used function thus survey makes interpretable phase approximation pipeline help reader easy comprehensive information future work challenges detected © 2021 authors exclusive license springer nature singapore pte ltd', 'visionbased monocular human pose estimation one fundamental challenging problems computer vision aims obtain posture human body input images video sequences recent developments deep learning techniques brought significant progress remarkable breakthroughs field human pose estimation survey extensively reviews recent deep learningbased 2d 3d human pose estimation published since 2014 paper summarizes challenges main benchmark datasets evaluation metrics performance comparison discusses promising future research directions © 2019', 'innovation convolutional networks artificial intelligence led many studies generally image processing object detection study aims make human detection 2d images correctly estimate human pose addition due recent advances pose estimation aim apply multiperson human detection distance scaling 2d images system makes multiperson 2d human detection pose detection convert image 3d model estimates distance human poses create 3d scale system divided three modules 2d image multiperson human detection 2d 3d pose estimation 3d pose distance scaling © 2021 authors exclusive license springer nature singapore pte ltd', '491 topics discussed remote twowheel robot control opcua wireless energy harvesting ic low power iot sensor accurate occupancy detection via label noise filtering technique delayaware tdma scheduling deep reinforcement learning tactical manet user profilebased smart home energy management system vibauth enabling accurate usable deployable user authentication leveraging vibration mathematical model simulation nutrientplant interaction analysis design 4d8psktcm hybrid talgorithm deep learning lightweighted network human pose estimation mobile ar service reinforcement learning real time aerial bs positioning dense urban 5g mobile network', 'human pose estimation fundamental yet challenging task computer vision human pose estimation single image challenging problem due limited information 2d images large variations configuration appearance body parts recent works largely improved result human pose estimation development convolutional neural network however still exists many difficult cases occluded keypoints complex background scale variations human body keypoints well dealt paper design novel scaleaware network attentional selection extracts multiscale semantic information meaningful features specifically feature pyramid supervision module fpsm improve estimation accuracy scale variations meanwhile spatial channel attention module scam designed recalibrating spatial channel features algorithm achieve result lsp dataset make competitive performance mpii human pose dataset © 2021 springer nature switzerland ag', 'unmanned aerial systems uass drones continue increase capabilities sophistication across wide range applications uass high mobility easily deployed capable realtime monitoring crowd behavior utilizing multisensorbased detection remote sensing objects capabilities make uass useful tool human autonomy teaming hat applications law enforcement le capitalizing human factors hf study examines concept leveraging drone technology together artificial intelligence ai machine learning ml produce uas system assist le monitoring assessment crowd behaviors peaceful nonpeaceful events le agencies increasingly tasked engaging dynamic environments exist public events utilized force multiplier autonomous tool would benefit aiuas platform utilizing artificial intelligence assisting identifying behavior peaceful people opposed malevolent participants instigators may attempt take control aiuass type would allow le leverage existing resources within organizational structures provide increased situation awareness via live virtual constructive lvc broadcast monitoring dynamic environments information provided aiuas systems would provide realtime information field forces well command control operations may remotely located aiuas sensors dynamically allocated needed monitoringdocumenting crowd behavior police actions video recordings would provide evidence court well counter truthbending recordings published professional protestors agenda driven mainstream media outlets benefits impact type le aiuas platform would profound traditional visible light sensors greatly influenced environmental factors preventing ability determine variations regarding abnormal crowd behaviors order overcome challenge project utilize four types collection multitask cascading cnn mccnn scatternet hybrid deep learning network multiscale infrared optical flow mirof event cameras eventbased vision event camera slam simultaneous localization mapping ai developed monitor crowd density average ground speed human pose estimations movement behaviors well identification primary violent instigators system detect violent individuals realtime leveraging onboard image processing well cloud processing fundamental research project inspired built upon recent drone surveillance system dss publications mdpi © 2021', 'neural architecture search proven highly effective design efficient convolutional neural networks better suited mobile deployment handdesigned networks hypothesizing neural architecture search holds great potential human pose estimation explore application neuroevolution form neural architecture search inspired biological evolution design 2d human pose networks first time additionally new weight transfer scheme enables us accelerate neuroevolution flexible manner produces network designs efficient accurate handdesigned networks fact generated networks process images higher resolutions less computation previous handdesigned networks lower resolutions allowing us push boundaries 2d human pose estimation network designed via neuroevolution refer evopose2ds achieves comparable accuracy simplebaseline 50 faster 127 × smaller terms file size largest network evopose2dl achieves new accuracy microsoft coco keypoints benchmark 43× smaller nearest competitor similar inference speed code publicly available httpsgithubcomwmcnallyevopose2d © 2013', 'objective 3d human pose estimation monocular videos become open research problem computer vision graphics community long time understanding human posture limb articulation important highlevel computer vision tasks humancomputer interaction augmented virtual reality human action activity recognition recent success deep networks led many 3d pose estimation train deep networks end end direct image prediction topperforming approaches shown effectiveness dividing task 3d pose estimation two steps follows 2d pose estimator estimate 2d poses images mapping 3d space results indicate large portion error modern deep 3d pose estimation systems stems 2d pose estimation error therefore mapping 2d pose containing error noise optimum reasonable 3d pose crucial 3d pose estimation jointly sparse representation depth model combine spatial geometric priori 3d poses temporal information improve 3d pose estimation accuracy first 3d variable shape model integrates sparse representation sr represent rich 3d human posture changes convex relaxation l12 regularization used transform nonconvex optimization problem singleframe image shapespace model convex programming problem provide reasonable initial values single frame image manner possibility ambiguous reconstructions considerably reduced second initial 3d poses obtained sr module regarded 3d data noise fed multichannel long short term memory mlstm denoising endecoder form pose sequences temporal dimension 3d data noise converted three components x z ensure spatial structure 3d pose component multilayer lstm cells used capture different frames time variation output lstm unit optimization result corresponding component time dependence two adjacent frames character posture input sequence implicitly encoded hidden layer lstm unit time information learned added initial value residual connection maintain time consistency 3d pose effectively alleviate problem sequence jitter moreover shaded joints corrected smoothing constraint two frames lastly obtain optimized 3d pose estimation results decoding last linear layer result comparative experiment conducted verify validity conducted human3 6m dataset results compared quantitative evaluation metrics common used align predicted 3d pose ground truth 3d pose similarity transformation average error per joint millimeters estimated ground truth 3d pose 2d joint ground truth 2d pose estimations convolutional network separately used inputs quantitative experimental results suggest remarkably improve 3d estimation accuracy input data 2d joint ground truth given human 3 6 dataset average reconstruction error decreased 126 optimization model compared individual frame estimation compared existing sparse model video average reconstruction error decreased 64 input data 2d pose estimations convolutional network average reconstruction error decreased 13 optimization model compared single frame estimation compared existing depth model average reconstruction error decreased 128 compared existing sparse model video average reconstruction error decreased 91 conclusion combining mlstm endecoder temporal information sparse model adequately exploit 3d pose prior knowledge temporal spatial dependence continuous human pose changes achieve remarkable improvement monocular video 3d pose estimation accuracy © 2020 editorial publishing board journal image graphics right reserved', 'present movnect lightweight deep neural network capture 3d human pose single rgb camera improve overall performance model apply teacherstudent learning knowledge distillation 3d human pose estimation realtime postprocessing makes cnn output yield temporally stable 3d skeletal information used applications directly implement 3d avatar application running mobile realtime demonstrate network achieves high accuracy fast inference time extensive evaluations show advantages lightweight model training previous 3d pose estimation human36m dataset mobile devices © 2020', 'existing human pose estimation hpe models show good performance also reflect pair contradictions computational complexity prediction performance address dilemma paper novel hpe training via selfguided learning sgl specifically dual model training designed get modeltemporal ensemble learning fuse knowledge new guidance model moreover selfguided joint loss considered keypoint attention enhancement selfguided compensation experimental results show sgl lower computation cost also achieves higher prediction precision © 2021 springer nature switzerland ag', '44 special focus conference humancomputer interaction topics realtime estimation eye movement condition deep learning model development spidarhmd standalone hmd human pose estimation uavhuman workspace current status user experience keyboard smartphones overall questionnaire analysis extended reality xr applications architectural practice towards development costeffective immersive telexistence platform generic telemanipulation tasks motion primitive segmentation cognitive model vriadl virtual reality simulation evaluation technology acceptance models automotive consumer electronics modeling viewpoint forklift operators contextbased clustering gaze fixations vibrothermal haptic display socioemotional communication study virtual reality performance user ’ individual characteristics training tool structured knowledge risk management vr technology industryadapted ar training manual assembly operations interacting fem simulated tubes ar improving learnability capabilities desktop vr medical applications towards improving situation awareness maritime field operators augmented reality semiautomatic reply avatar vr training system adapted scenario trainee ’ status pilot study progress driving skills immersive vr driving simulator ethical ai social good change perspective artificial intelligence interactive systems design human centric yes limited haptic finger glove vr keyboard input explain facility managers qualitative industrial user research study explainability toward aibased scenario management cyber range training', 'present new 3d human pose estimation single image 3d pose estimation focused predicting fullbody pose single person given enough attention challenges application incompleteness body pose existence multiple persons image paper introduce depth maps solve problems predicts depths human pose spatial grids supports 3d poses estimation incomplete full bodies multiple persons depth maps encode depths limbs rather joints informative reversibly convertible depths joints unified network trained end end mixed 2d 3d annotated samples experiments reveal algorithm achieves state art human36m largest publicly available 3d pose estimation benchmark moreover qualitative results reported demonstrate effectiveness 3d pose estimation incomplete human bodies multiple persons © 2019 springerverlag gmbh germany part springer nature', 'recent advances deep learning algorithms hardware led rapid development computer vision development computer vision field many deep learning algorithms applied augmented realityar technology however order increase accessibility users majority ar services provided mobile devices addition many deep learning algorithm techniques require high computing resources applied directly ar services therefore order apply deep learning technology mobile ar services necessary implement lightweight network paper lightweighted network model estimate single human pose mobile ar service contribution paper following lightweighted network applied commercial mobile devices ii integrating unity 3d rendering tool tensorflow lite library iii body skeleton human extracted analyzed real time results suggest useful provide way new interactions mobile ar services © 2020', 'anterior cruciate ligament acl injury cause serious burden especially athletes participating relatively risky sports raises growing incentive designing injuryprevention programs purpose analysis drop jump landing test example provide useful asset recognizing likely sustain knee injuries knee flexion angle plays key role within test scenarios multiple research efforts conducted engaging existing technologies microsoft kinect sensor motion capture mocap investigate connection lower limb angle ranges jump tests injury risk associated even though technologies provide sufficient capabilities researchers clinicians need certain levels knowledge enable utilize facilities moreover systems demand special requirements setup procedures make limiting due recent advances area deep learning numerous powerful 3d pose estimation algorithms developed last years access relatively reliable accurate 3d body keypoint information lead successful detection prevention injury idea combining temporal convolutions video sequences deep convolutional neural networks cnns offer substantial opportunity tackle challenging task accurate 3d human pose estimation microsoft kinect sensor ground truth analyze performance cnnbased 3d human pose estimation everyday settings qualitative quantitative results convincing enough give incentive pursue improvements especially task lower extremity kinematics estimation addition performance comparison kinect cnn also verified highmargin consistency two kinect sensors © 2020', 'recent advancements deep learning significantly improved accuracy multiperson pose estimation rgb images however deep learning typically rely large number deep refinement modules refine features body joints limbs hugely reduce runtime speed therefore limit application domain paper feature transfer capture concurrent correlations body joint limb features concurrent correlations features form complementary structural relationship mutually strengthens networks inferences reduces needs refinement modules transfer subnetwork implemented multiple convolutional layers merged body part detection network form endtoend system transfer relationship automatically learned groundtruth data instead manually encoded resulting general efficient design validated multiple popular multiperson pose estimation benchmarks mpii coco 2018 posetrack 2017 2018 experimental results show significantly increases inference speed 738 frame per second fps also attains comparable performance © 2020 elsevier inc', 'human pose estimation crucial step towards understanding characterizing people ’ behavior images videos current state art results human pose estimation achieved large deep learning models restricted cloud computing real time applications however development edge computing deep learning moving cloud edge work present deeprehab deep learning 2d pose estimator optimized edge tpu processing first improve existing edge tpu compatible model named posenet refining predictions filtering subsequently performance filters limited model ’ inaccuracies specifically lower body parts developed deeprehab trained 23 keypoints cocowholebody dataset achieve 065 ap deeprehab quantize losing 3 ap runs speed 15 fps coral usb accelerator suits real time evaluations © 2021 springer nature switzerland ag', 'estimating human pose difficult task due high flexibility joints possible occlusion paper architecture efficiently predicts human pose solves preceding problem three main consecutive parts 1 deep convolutional neural network dcnn feature extraction 2 feature refinement 3 fusion detection context information feature extraction phase fusion two dcnn modules inspired vgg19 inceptionv4 deep learning architectures feature refinement cascaded feature integration technique stacked hourglass make system efficiently locate challenging joints last fusion context information detected prediction performed makes system accurate towards occlusion way poses difficult joint coordinates reliably estimated even presence occlusion severe distracting factors successful testing done popular mpii lsp datasets experimental results analysis selected datasets found accurate compared terms pck metric © 2021 informa uk limited trading taylor francis group', 'study explores usefulness human pose estimation technique sport since expansion deep learning techniques human pose estimation became important field computer vision used many applications like pose analysis correction training session etc used estimate whether baseball hitter performs good swing hitter ’ limb coordinates detected openpose model real time multiperson detection system coordinates used calculate hip distance limb angles distance angles applied custom rules custom rules made researches coaching experience order evaluate swing baseball hitter rule awarded differ points importance assumed goal study technology assistance sport coaching scenario ©2021', 'humanobject interaction hoi detection finds relationships humans objects important research area current hoi detection performance unsatisfactory one main problems cnnbased hoi detection algorithms fail predict correct outputs unseen test data limited number available training examples herein novel hoi detection called onthefly stacked generalization deep neural network osgnet osgnet consists three main components 1 feature extraction modules 2 hoi relationship detection networks 3 metalearner combining outputs submodels components 1 2 considered submodels taskbased feature extraction modules classification human pose estimation modules used submodels achieve onthefly stacked generalization submodels metalearner trained simultaneously submodels trained provide complementary information metalearner improves generalization performance unseen test data extensive experiments demonstrate achieves accuracy particularly cases involving rare classes © 2013', '35 special focus conference structural syntactic statistical techniques pattern recognition topics learning highresolution domainspecific representations gan generator predicting polypharmacy side effects relationwise graph attention network lglgnn learning global local information graph neural networks graph transformer learning better representations graph neural networks weighted network analysis debye model estimating manifold dimension complex network weyl ’ law efficient partitioning partial correlation networks alzheimer ’ brain network analysis sparse learning feature selection entropy graph embeddings proxy potential mobility covid19 outbreaks complexvalued embeddings generic proximity data novel data set information retrieval basis subgraph matching graph preimage graph edit distances multivalent graph matching problem solved maxmin ant system metric learning graph edit costs regression parallel subgraph isomorphism multicore architectures comparison four strategies tree search multipleimage superresolution deep learning statistical features unsupervised semantic discovery visual patterns detection deep residual neural network child ’ spontaneous facial expressions recognition multilayer pca network image classification multimodal fusion model hybrid attention mechanism gesture recognition metric learning multilabel classification stegcolnet steganalysis ensemble colorspace residual multiscale full convolutional network rmfcn high resolution semantic segmentation retinal vasculature practical hybrid active learning human pose estimation', 'paper realtime human pose recognition bidirectional long shortterm memory lstm openpose utilized human pose estimation module obtain twodimensional joint point data human body according condition missing data judge whether human body occlusion state nonocclusion cases classifier formulated bidirectional lstm initial twodimensional joint point information sent classifier human pose recognition occlusion state depth camera internal parameters used 3d mapping torso vector joint angle constructed aforementioned highdimension features processed principal component analysis sent classifier human pose recognition kth dataset laboratory dataset containing five human poses utilized evaluate nonocclusion conditions experimental results show accuracy values algorithm 263 108 higher traditional models deep learning models occlusion conditions accuracy rate improved 56 compared traditional model human pose recognition complex environments achieved © 2020 science press right reserved', 'research support sports instruction displaying visualized comparison model motion player human pose estimation openpose deep learning technologies first applied model user motions motion difference model user visualized trajectories superimposed two motions help users improve motions forms system displays specific body part example elbow throwing arm located specific time computed location timing model motion system also presents changes velocity position body parts time line graphs supposing users better understand motion current implementation designed pitching motions several motions included future work conducted user experiments four participants used system improve pitching forms recording pitching motion four participants divided two groups first two participants given visualizations system two presented pair motions model participants performed improved pitching motions front system evaluate two groups showed refinements experimental results shown forms improved groups usability system successfully tested future work includes conducting largerscale experiments automatic selection model motions user well extending motions different sports © 2021 spie', 'human pose estimation process recognizing human keypoints given image one important tasks computer vision wide range applications including movement diagnostics surveillance selfdriving vehicle accuracy human keypoint prediction increasingly improved thanks burgeoning development deep learning existing solved human pose estimation generating heatmaps ith heatmap indicates location confidence ith keypoint paper introduce novel network structures referred multiresolution representation learning human keypoint prediction different resolutions learning process networks branch extra layers learn heatmap generation firstly consider architectures generating multiresolution heatmaps obtaining lowestresolution feature maps second allows learning process feature extraction heatmaps generated resolution feature extractor first second approaches referred multiresolution heatmap learning multiresolution feature map learning respectively architectures simple yet effective achieving good performance conducted experiments two common benchmarks human pose estimation mscoco mpii dataset code made publicly available httpsgithubcomtqtrunghnvnsimmrpose © 2020', 'welllabeled datasets available melody extraction scarce limits advancement deep learning overcome problem pitch refinement refine semitonelevel pitch sequences decoded massive melody midi files generate large number fundamental frequency f0 values model training since refined pitch values used first round training errors small set welllabeled data used second round training highresolution network hrnet initially developed human pose estimation introduced melody extraction considers multiresolution feature learning making resulting representation semantically richer subsequently bidirectional long shortterm memory blstm layer used exploit temporal information melody addition new loss function unvoiced frames contribute voicing detection pitch classification also alleviate class imbalance problem experiment results three public datasets show system outperforms four algorithms cases © 2021', 'paper presents novel 3d human pose shape estimation images sparse views joint points silhouettes parametric model firstly parametric model fitted joint points estimated deep learningbased human pose estimation extract correspondence parametric model pose fitting silhouettes 2d 3d space novel energy function correspondence built minimized fit parametric model silhouettes comprehensive shape information energy function silhouettes built 2d 3d space also means needs images sparse views balances data used required prior information results synthetic data real data demonstrate competitive performance pose shape estimation human body © 2021 springer nature switzerland ag', 'precise orientationestimation humans relative pose monocular camera system challenging task due general aspects camera calibration deformable nature human body motion thus novel approaches deep learning precise object poseestimation robotics hard adapt human body analysis work hybrid accurate estimation human body rotation relative camera system presented thereby significantly improving results derived posenet applying analysis optical flow frame frame comparison human body inplace rotating tpose thereby aligned center applying object tracking compensate translations body movement 2d skeleton extraction optical flow calculated region interest roi area aligned relative vertical skeleton joint representing spine compared frame frame evaluate eligibility clothing fundament good feature local pixel homogeneity taken consideration restrict optical flow heterogeneous regions distinctive features like imprint patterns buttons buckles besides local illumination changes mean optical flow coarse approximation axial body shape ellipsis accuracy 01° 20° target rotation 10° orientationestimation achieved frametoframe comparison evaluated validated computer generated imagery cgi renderings realworld videos people wearing clothing varying feature appropriateness © 2020 university west bohemia rights reserved', '19 special focus conference wireless sensor networks topics uachr accurate csibased human behavior recognition gaussian goodness fast general image encryption deep learning compressed sensing compound chaotic system cascading failure mitigation strategy urban road traffic networks analysis countermeasure design adversarial patch attacks online offloading delaysensitive tasks fog computing data download scheme vehicle mobile information internet vehicles novel random matrix theory mean shift clustering spectrum sensing consensus algorithm leadership transferltraft load balancing heterogeneous network sdn survey novel distance estimation model node localization uav task allocation swarm intelligence optimization algorithm opraw raw grouping algorithm outage probability industrial internet things acousticpose acousticbased human pose estimation smartphonesbased noncontact children ’ posture evaluation research military application operating system internet things application intelligent traffic scene recognition computer vision errge balancing accuracy efficiency respiratory monitoring smart watch combining peak detection regression model', 'paper novel algorithm relationship detection task involves tracking target object human pose target object tracked visual object tracker human poses estimated via keypoint detector person identities preserved simple yet effective iou tracker finally possessing relationship inference made position information tracked target humans algorithm meets realtime requirement running 20 fps give application illustration sports analytics © 2020 technical committee control theory chinese association automation', 'closedcircuit television cctv systems essential nowadays prevent security threats dangerous situations early detection crucial novel deep learningbased allowed develop automatic weapon detectors promising results however approaches mainly visual weapon appearance handguns body pose may useful cue especially cases gun barely visible work novel combine single architecture weapon appearance human pose information first pose keypoints estimated extract hand regions generate binary pose images model inputs input processed different subnetworks combined produce handgun bounding box results obtained show combined model improves handgun detection state art achieving 423 189 ap points best previous © 2013', 'field construction safety construction workers construction sites major problem years according statistic previous study approximately 88 accidents caused unsafe behaviors recent years development application deep learning attracted considerable research interest object detection human pose estimation etc meanwhile object detection techniques applied detect whether workers wear proper equipment however proper equipment doesnt mean performing correct operation therefore paper deep learning deepsafty detect unsafe behaviors construction workers postures reduce mortality rate deepsafty object detection model yolov3 used locate construction workers precisely human pose estimation technology used determine various postures construction workers 17 joints poses words 51dimensional vector produced neural network unsafe behaviors continual movements field construction finally model deals time series deep learning ie long shortterm memory used solve classification problems timedependent joint vectors finally conduct comprehensive experimental study dataset collected cctv real construction site results demonstrate effectiveness deep learning show strength taking object detection human pose estimation account unsafe behaviors construction workers © 2020', 'selflearning integral part yoga incorrect posture performing yoga lead serious harm muscles ligaments body thus prevent present intuitive machine learning techniques correct practitioner ’ pose performing various yoga asanas system aimed providing concise feedback practitioner able perform yoga poses correctly assist identifying incorrect poses suggest proper feedback improvement order prevent injuries well increase knowledge particular yoga pose dataset five yoga pose ie natarajasana trikonasana vrikshasana virbhadrasana 1 2 utkatasana created collecting images internet well different individuals took part development system deep learning model convolutional neural networks cnn yoga pose identification along human joints localization model followed process identification errors pose developing system system able achieve classification accuracy 95 pose identification obtaining information pose user system gives feedback improve correct posture user © 2021', 'person retrieval aims effectively matching pedestrian images extensive database given specified identity extracting effective features crucial highperformance retrieval system recent significant progress achieved partbased models constructed robust local representations top vertically striped part features however kind models predefined partitioning strategies making number size partition identical even input images vary lot unchangeable setting usually leads less flexibility robustness capturing visual variance primary reason negative effect fixed partitioning strategy unable deal significant variance pose illumination viewpoint common pedestrian image dataset b also inference error misalignment human bodies introduced prepositive pedestrian detection module human pose estimation module paper tackle problem via introducing novel adaptive partition network apn apn utilizes deep reinforcement learning applies agent generate optimal partitioning strategies dynamically different input images agent inside apn optimized policy gradient algorithm maximizes reward choosing best partition setting leveraging supervision cues objective partitioning strategies generated set heldout training images agent trained jointly parts apn ensures apns robustness generalization ability extensive experimental results multiple datasets including cuhk03 dukemtmc market1501 demonstrate superiority apn models © 19992012', 'human behavior recognition nature environment needs special effort accuracy human pose estimation infrared spectral signal illustrated key role fields human behavior recognition slow spectral imaging data band overlap random noise limited applications work develop deep learning network architecture extract human body parts links skeleton extract human features precisely develop rapid blind restoration model linear canonical transform lct regularization recover weak infrared spectral signals firstly apply lct transform weak ir spectral signal highresolution one analyze essential difference reveal sparsity distributions weak ir spectra imaging data sparser highresolution one inspired findings infrared spectral restoration developed constraint sparsity distribution observed ir spectra l0norm experimental results illustrate lct ir spectrum restoration well save ir spectral band structures remove existed poisson noises human behavior tracking furthermore recovered ir spectral signals also applied estimate human pose nature classroom © 2019 elsevier bv', 'human pose estimation problem continues one greatest challenges field computer vision stacked structure hourglass network enabled substantial progress human pose estimation keypoint detection areas largely used backbone network however also requires relatively large number parameters high computational capacity due characteristics stacked structure accordingly present work lightweight version hourglass network also improves human pose estimation performance new hourglass network architecture utilizes several additional skip connections improve performance minimal modifications still maintaining number parameters network additionally size convolutional receptive field decisive effect learning detect features full human body therefore multidilated light residual block expands convolutional receptive field also reducing computational load residual block also invariant scale multiple dilations wellknown mpii lsp human pose datasets used evaluate performance variety experiments conducted confirm efficient compared current hourglass weightreduction © 2020 authors', 'deep learning learns datadriven temporal priors perform 3d human pose estimation six body worn magnetic inertial measurement units sensors work estimates 3d human pose associated uncertainty sparse body worn sensors derive implement 3d angle representation eliminates yaw angle magnetometer dependence show 3d human pose still obtained reduced representation enhanced uncertainty kinematic acceleration input show improves generalization real sensor data different subjects well accuracy bidirectional recurrent autoencoder sliding window used inference time instead full sequence offline mode major contribution research 3d human pose predicted sparse sensors well calibrated uncertainty correlated ambiguity actual errors demonstrated results two real sensor datasets dipimu total capture come stateofart accuracy work confirms main limitation sparse sensor 3d human pose prediction lack temporal priors therefore finetuning small synthetic training set target domain improves accuracy © 2013', 'deep neural networks human pose estimation sample fixed position feature map therefore difficult model geometric transformation human pose generalization ability network poor variation size pose shooting angle human instance solve problem multiperson human pose estimation deformable convolution proposedbased strong ability deformable convolution modeling geometric transformation targets feature extraction module designed ensure detection accuracy geometric changes human key points improve performance network prediction value model truth value generated twodimensional gaussian model employed calculate loss model trained iteratively human key points detected effectively model complex conditions shooting angle attachment character scale changes experiment shows model effectively improves accuracy human key point detection © 2020 science press right reserved', 'although significant progress achieved monocular 3d human pose estimation correlation body parts crossview geometry consistency well studied work fully explore priors body structure viewrelationship 3d human pose estimation anatomy geometry constrained onestage first define kinematic structure model deep learning represents joint positions treestructure model bonelength bonesymmetry losses anatomy prior encode body structure information explore crossview geometry information introduce novel training mechanism multiview consistency constraints effectively reduces unnatural implausible estimation results achieves results human36m mpiinf3dhp data sets © 2021 springer nature switzerland ag', 'modelbased gait recognition considered promising due robustness variations clothing baggage carried although modelbased gait recognition fully explored due difficulty human body model fitting lack largescale gait database recent progress deep learningbased approaches human body model fitting human pose estimation mitigating difficulty paper therefore address remaining issue presenting largescale human posebased gait database oumvlppose publicly available multiview largescale gait database oumvlp oumvlppose many unique advantages compared public databases first oumvlppose first gait database provides two datasets human pose sequences extracted two standard deep learningbased pose estimation algorithms openpose alphapose second contains multiview largescale data ie 10000 subjects 14 views subject addition also provide benchmarks different kinds gait recognition including modelbased appearancebased evaluated comprehensively modelbased gait recognition shown promising performances believe database oumvlppose greatly promote modelbased gait recognition next years © 2019', 'availability large labeled dataset key requirement applying deep learning solve various computer vision tasks context understanding human activities existing public datasets large size often limited single rgb camera provide perframe perclip action annotations enable richer analysis understanding human activities introduce ikea asm three million frame multiview furniture assembly video dataset includes depth atomic actions object segmentation human poses additionally benchmark prominent video action recognition object segmentation human pose estimation tasks challenging dataset dataset enables development holistic integrate multimodal multiview data better perform tasks © 2021', 'paper aim develop automatic system monitor evaluate workers efficiency smart manufacturing human pose tracking temporal action localization first explore generative adversarial networks gans achieve significantly improved estimation human body joints second formulate automated worker efficiency analysis temporal action localization problem action video performed worker matched reference video performed teacher extract invariant spatiotemporal features human body pose sequences perform crossvideo matching dynamic time warping human pose estimation achieves performance benchmark dataset automated work efficiency analysis able achieve action localization average iou intersection union score large 09 represents one first systems provide automated worker efficiency evaluation © 2020', '2d pose estimation plays important role many activity recognition applications however pose estimation computationally expensive paper dcd dynamicity cropdrop contextbased pose refreshing algorithm controllable information loss much better processing speed main idea utilize correlation consecutive frames understand dynamicity individual pose decide pose refreshing rate minimize pose information loss full pose estimation applied needed test results show twice speed compared openpose acceptable information loss tested multiperson multidynamicity activities like sitting idle practicing thaiqi dancing © 2020', 'aiming impact unstructured rotational variability threedimensional information point cloud prediction results featuresupervised threedimensional information encoding decoding convolution deep learning network proposedthe network composed feature monitoring coding decoding modules seriesin first part module threedimensional convolution module used form hourglass structure realize coding decoding feature mapin second part residual blocks different parameters connected parallel realize monitoring fusion feature mapsfeature monitored coding decoding modules build networks different depths series according size data setsat time according data resolution modules parameters set realize feature learning rough fine ultimately obtain best networkthe experiment itop database shows network achieves endtoend deep learning threedimensional information significantly improves system performance higher precision accuracy © 2020 chinese institute electronics right reserved', 'leverage recent advances reliable 2d pose estimation convolutional neural networks cnn estimate 3d pose people depth images multiperson humanrobot interaction hri scenarios observation depth information obtain 3d lifted points 2d body landmark detections provides rough estimate true 3d human pose thus requiring refinement step line contributions threefold perform 3d pose estimation depth images decoupling 2d pose estimation 3d pose refinement ii deeplearning regresses residual pose lifted 3d pose true 3d pose iii show despite simplicity achieves competitive results accuracy speed two public datasets therefore appealing multiperson hri compared recent © 2020', 'human pose estimation fundamental yet challenging task computer vision although deep learning techniques made great progress area difficult scenarios eg invisible keypoints occlusions complex multiperson scenarios abnormal poses still wellhandled alleviate issues novel spatial preserve contentaware network spcnet includes two effective modules dilated hourglass module dhm selective information module sim dilated hourglass module preserve spatial resolution along large receptive field similar hourglass network stack dhms get multistage multiscale information selective information module designed select relatively important features different levels sufficient consideration spatial contentaware mechanism thus considerably improves performance extensive experiments mpii lsp flic human pose estimation benchmarks demonstrate effectiveness network particular exceed previous achieve performance three aforementioned benchmark datasets © 2020 authors ios press', 'recent advances generalized image understanding seen surge deep convolutional neural networks cnn across broad range imagebased detection classification prediction tasks whilst reported performance approaches impressive study investigates hitherto unapproached question impact commonplace image video compression techniques performance deep learning architectures focusing jpeg h264 mpeg4 avc representative proxy contemporary lossy imagevideo compression techniques common within networkconnected imagevideo devices infrastructure examine impact performance across five discrete tasks human pose estimation semantic segmentation object detection action recognition monocular depth estimation within study variety network architectures domains spanning endtoend convolution encoderdecoder regionbased cnn rcnn dualstream generative adversarial networks gan results show nonlinear nonuniform relationship network performance level lossy compression applied notably performance decreases significantly jpeg quality quantization level 15 h264 constant rate factor crf 40 however retraining said architectures precompressed imagery conversely recovers network performance 784 cases furthermore correlation architectures employing encoderdecoder pipeline demonstrate resilience lossy image compression characteristics relationship input compression output task performance used inform design decisions within future imagevideo devices infrastructure © 2021', 'active learning al received much attention deep learning dl human pose estimation paper practical hybrid active learning strategy training human pose estimation model tested industrial online environment conducted experiments show active learning strategy select diverse samples annotated outperforms baseline random sampling result strategy enables significant improvement performance pose estimation © 2021 springer nature switzerland ag', 'human pose estimation also machine vision tasks eg object recognition semantic segmentation image classification convolution neural networks cnns obtained highest performance today besides performance traditional networks shown attention module hence paper focuses valuable feedforward cnns first feed feature map attention module stage backbone network divided two different dimensions channel spatial multiplication combines two feature maps gives next stage backbone longrange dependencies channel spatial data network capture information gain better precision efficiency experimental findings would also demonstrate disparity attention module current result change make spatial better expected joint heatmap retains accuracy decreasing number parameters comparison architecture benefits baseline 13 points ap addition network trained benchmarks coco 2017 open dataset © 2021 springer nature switzerland ag', 'letter present conditional generative adversarial networkbased model realtime underwater image enhancement supervise adversarial training formulate objective function evaluates perceptual image quality global content color local texture style information also present euvp largescale dataset paired unpaired collection underwater images poor good quality captured seven different cameras various visibility conditions oceanic explorations humanrobot collaborative experiments addition perform several qualitative quantitative evaluations suggest model learn enhance underwater image quality paired unpaired training importantly enhanced images provide improved performances standard models underwater object detection human pose estimation saliency prediction results validate suitable realtime preprocessing autonomy pipeline visuallyguided underwater robots model associated training pipelines available httpsgithubcomxahidbuffonfuniegan © 2016', 'intelligent image processing deep learning recently emerging artificial intelligence critical next generation computing systems keyspeaker detection image processing aims find speaker plays important role complex multiperson scenes however existing speaker detection judge whether somebody talking overcome problem paper novel keyspeaker detection named speaker poseattention detection model space extracts space information analyze peoples attention gathering find keyspeaker consists two main parts first part importance detection model idm human pose estimation effective evaluation strategy find candidates important speaker second part consists poseattention graph pag link analysis algorithm pag represents peoples attention graph link analysis algorithm analyzes connectivity graph find keyspeaker due lack available keyspeaker detection datasets selfcollected dataset containing speech meeting scenes built estimate space model experimental results show obtain best detection accuracy compared existing', 'multimodal 3d human pose estimation combines 2d human pose estimation network utilizing rgb data 3d human pose estimation network utilizing 2d pose estimation results depth information order predict 3d human poses improve upon proposing accurate 2d human pose estimation network well introducing squeezeexcite blocks architecture 3d pose estimation network importantly focused challenging application 3d human pose estimation collaborative tasks direction selected appropriate subsets address collaborative tasks largescale multiview rgbd dataset generated novel oneview rgbd dataset training testing respectively achieved performance among rgbd approaches tested novel benchmark rgbd dataset collaborative assembly created made publicly available © 2021 springer nature switzerland ag', '3d human pose estimation plays important roles various humanmachine interactive applications lacking diversity existing labeled 3d human posture dataset restricts generalization ability deep learning models data augmentation therefore important solve problem however data augmentation pose estimation network training usually treated two isolated processes limiting performance pose estimation network paper developed improved data augmentation jointly performs pose network estimation data augmentation designing rewardpenalty strategy effective joint training making model training data augmentation improve particular improved evolutionary data augmentation generate distribution nodes crossover rotation angles mutation process evolution extensive experiments show significantly improves models without additional data efforts also extremely competitive advanced © 2021 springer nature switzerland ag', 'human pose estimation hpe relatively new significant computer vision field applications hpe process estimating location human body joints image video correct estimate human body joints used track peoples minimal activities realtime applications hpe extensive research area relies many individuals monitored hpe categorized two ways eg number humans whose pose estimated ie singleperson multiperson pose estimation b environment used ie 2dimension 2d 3dimension 3d initially paper presents traditional brief later paper focus recent advancement hpe deep learning approaches rigorous review deep learning approaches topdown bottomup approaches expressed compared various evaluation matrices models accuracy observed models succeed perform well mpii dataset comparison coco dataset distributed aware architecture gives best performance among models providing 97 percentage correct key pck mpii dataset 789 average precision ap coco dataset multiperson hpe ap limited 786 alphapose model © 2021 university bahrain rights reserved', 'paper address problem 3d human mesh reconstruction single 2d human pose deep learning meshlifter network estimates 3d human mesh input 2d human pose unlike existing 3d human mesh reconstruction studies train models paired 2d 3d data weakly supervised learning loop structure train meshlifter alleviates difficulty obtaining groundtruth 3d data ensure meshlifter trained successfully 2d human pose dataset unpaired 3d motion capture dataset compare recent studies various experiments show achieves effective 3d human mesh reconstruction performance notably achieves reconstruction error 591 mm without 3d groundtruth data human36m standard dataset 3d human mesh reconstruction © 2020 authors licensee mdpi basel switzerland', 'novel attentionbased 3d human pose estimation monocular video despite general success endtoend deep learning paradigms two key observations 1 temporal incoherence jitter often yielded single frame prediction 2 error rate remarkably reduced increasing receptive field video therefore design attentional mechanism adaptively identify significant frames tensor outputs deep neural net layer leading optimal estimation achieve large temporal receptive fields multiscale dilated convolutions employed model longrange dependencies among frames architecture straightforward implement flexibly adopted realtime applications offtheshelf 2d pose estimation system eg mocap libraries easily integrated adhoc fashion quantitatively qualitatively evaluate various standard benchmark datasets eg human36m humaneva considerably outperforms algorithms 8 error reduction average mean per joint position error 347 compared bestreported results code available httpsgithubcomlrxjasonattention3dhumanpose © 2020', 'sports pose unique challenge highspeed unobtrusive uninterrupted motion tracking due speed movement player occlusion especially fast competitive sport squash objective study video tracking techniques quantify kinematics elitelevel squash increasing availability quality elite tournament matches filmed entertainment purposes new methodology multiplayer tracking squash requires broadcast video input paper introduces evaluates markerless motion capture technique autonomous deep learning human pose estimation algorithm computer vision detect identify players inverse perspective mapping utilized convert pixel coordinates court coordinates distance traveled court position ‘ ’ dominance average speeds elite players squash determined validated results previous study manual tracking filtered coordinates displayed average absolute percent error manual 373 total distance traveled 352 126 average speeds 9 ms without speeds 1 ms respectively proven effective collecting kinematic data elite players squash timely manner special camera setup limited manual intervention © 2020 authors licensee mdpi basel switzerland', 'alberta infant motor scale aims questionnaire tracks infants motor function infants mental development evaluated recording poses baby achieve therefore meaningful systematic imagebased pose classifier classify infant actions aims provide early diagnosis potential developmental disorder autism paper presents hierarchical pose classifier given baby image frame combines benefits 3d human pose estimation scene context information due privacy policies collect enough real infant imagesvideos experiments instead generate synthetic baby images help skinned multiinfant linear smil model images first fed resnet50 coarselevel pose classification stacked hourglass cnn hierarchical 3d pose estimation scheme used 2d3d pose estimation finally innovative hierarchical infant pose classifier hipc takes estimated 3d keypoints coarselevel pose classification confidence scores give finelevel baby pose classification results experimental results show hierarchical pose classifier achieves accurate stable performance infant pose recognition © 2021', 'paper study problem monocular 3d human pose estimation deep learning due single view limitations monocular human pose estimation avoid inherent occlusion problem commonmethods themultiview 3d pose estimationmethod solve problem however singleview images used directly multiview greatly limits practical applications address abovementioned issues novel endtoend 3d pose estimation network monocular 3d human pose estimation first multiview pose generator predict multiview 2d poses 2d poses single view secondly simple effective data augmentation generating multiview 2d pose annotations account existing datasets eg human36m etc containing large number 2d pose annotations different views thirdly employ graph convolutional network infer 3d pose multiview 2d poses experiments conducted public datasets results verified effectiveness furthermore ablation studies show improved performance existing 3d pose estimation networks © 2020 authors', 'human pose estimation computer vision problem tries estimate human body joints location decide connected long studied still frontier research field nowadays paper comprehensive survey human pose estimation 2d monocular images given including classical representative works recent deeplearningbased goal paper let reader get brief understanding human pose estimation work see developed different time share common ideas paper inherits one admitted way categorize human pose estimation dividing topdown bottomup pipelines milestone works introduced comparison discussion among ideas made also new jump restriction purely top bottomup paper includes well later sections paper collects datasets used frequently ways error measurement given finally overall discussion made including unsolved problems currently challenging tasks © 2020', 'crowd counting problem counts number people image extensively studied recent years paper introduce new variant crowd counting problem namely categorized crowd counting counts number people sitting standing given image categorized crowd counting many realworld applications crowd monitoring customer service resource management major challenges categorized crowd counting come high occlusion perspective distortion seemingly identical upper body posture sitting standing persons existing density map approaches perform well approximate large crowd lose important local information necessary categorization hand traditional detectionbased approaches perform poorly occluded environments especially crowd size gets bigger hence solve categorized crowd counting problem develop novel attentionbased deep learning addresses limitations particular works three phases first generate basic detection sitting standing density maps capture local information ii generate crowd counting density map global counting feature iii finally crossbranch segregating refinement phase splits crowd density map final sitting standing density maps attention mechanism extensive experiments show efficacy solving categorized crowd counting problem © 2020', 'endtoend deep representation learning achieved remarkable accuracy monocular 3d human pose estimation yet models may fail unseen poses limited fixed training data paper novel data augmentation 1 scalable synthesizing massive amount training data 8 million valid 3d human poses corresponding 2d projections training 2dto3d networks 2 effectively reduce dataset bias evolves limited dataset synthesize unseen 3d human skeletons hierarchical human representation heuristics inspired prior knowledge extensive experiments show achieves accuracy largest public benchmark also generalizes significantly better unseen rare poses relevant files tools available project website © 2020', 'real time human posture estimation reduced number sensors challenging highly sought problem various modelbased developed years direction utilize optical andor inertial sensor data although proven effective laboratory settings applicability real world limited due difficulty information gathering high intrusiveness higher cost nonposition paper deals hybrid involving fullbody inverse kinematics ik deep learning order estimate physiologically feasible joint angles real time orientation information 6 inertial measurement units imus ik performed kinematically constrained 3d human body model obtain joint angles body model given orientation data 17 sensors attached different bone segments body bidirectional recurrent neural network birnn trained newly collected imu dataset regress orientation data 6 sensors joint angles obtained ik training converged mean squared error mse 598 degrees © 2021 copyright paper authors permitted creative commons license attribution 40 international cc 40 ceur workshop ceurwsorg', 'geometric features topological manifold properties utilized extract geometric properties geometric exploit applications geometrics eg geometric features widely used computer graphics computer vision problems review presents literature review geometric concepts geometric applications humanrelated analysis eg human shape analysis human pose analysis human action analysis review categorize geometric scope geometric properties extracted objectoriented geometric featureoriented geometric routinebased geometric considering broad applications deep learning review also studies geometric deep learning recently become popular topic research validation datasets collected performances collected compared finally research trends possible research topics discussed © 2019 authors licensee mdpi basel switzerland', 'human pose estimation always challenging problem holds great attention widespread extensive variety classification images activity acknowledgment main challenge detection localization key points variation several body poses resolve issue substantial research work done area paper discusses issues human pose estimation gives overview considerable research work pose estimation including deep learning customary imagebased techniques analyzing several results detecting restrictions author reconstructed simple model convolutional neural network estimates poses demonstrates potential cnns author concludes promising bearings directions explored future research © 2019', 'purpose inbed motion monitoring become great interest variety clinical applications imagebased approaches could seen natural nonintrusive purpose however video devices require special challenging settings clinical environment estimate patient ’ posture pressure sensors ’ data mapped images introduce deep learning retrieve human poses pressure sensors data addition present second hashing contentretrieval results results show good performance presented even poses subject minimal contact sensors moreover show deep learning approaches could used medical application despite limited amount available training data convnet provides overall posture even patient less contact mattress surface addition show could used realtime patient monitoring conclusions provided two successfully perform realtime inbed patient pose estimation robust different sizes patient activities furthermore provide overall posture even patient less contact mattress surface © 2018 cars', 'human pose estimation defined process locating joints person crowd given image video currently estimation widely used evaluation athletes workers monitoring patients clinical settings however human pose estimation easy task requires experts manually assess person ’ position specialized equipment ehealth devices watches bands handles markers highcost cameras monitor limited scenario main goal article evaluate markerless lowcost computer vision system get automatic estimation poses fall detection video calculating person ’ joint angle high level adaptability space model first step construction tool allows monitoring generating alerts prevent falls home clinical settings © 2020 spie', 'camerabased pose estimation necessity flexible applications robotics especially interaction robots mobile entities inspired recent advancements human pose estimation convolutional neural networks aspired substitute usage artificial marker automatically detecting inherent keypoints robot representing 2d skeleton model addition current encoder readings robot utilized establishing corresponding 3d skeleton model forward kinematics help 2d 3d point correspondences estimation translation orientation deviation robot camera derived solving perspectivenpoint problem adequate markerless keypoint detection ur5 robot presented evaluated terms precision pose dispersion considering dynamically moving robot promising results show novel works robustly reliably fewshot copes false positives well partly occlusions nondetected keypoints potential identified regarding enhancing accuracy synthetic data © 2019', '3d human pose estimation single image challenging problem due occlusion viewpoint variance illposed nature back projection follow standard twostep pipeline first detects 2d joint locations infer 3d pose first step recent deep learningbased detector second step novel exemplarbased algorithm implicitly augment exemplar set 3d human pose estimation motivation algorithm well represent various poses real world finite real exemplars achieve strategy synthesizing virtual candidate poses ensures augmented exemplar set much variety moreover also present effective select best exemplar candidate set well match detected 2d pose experimental results show achieves competitive performance human36m dataset © 2019 elsevier inc', 'paper revises main advances assistive computer vision recently fostered deep learning aim first discuss application deep learning computer vision contributed development assistive techinologies analyze recent advances assistive technologies achieved five main areas namely object classification localization scene understanding human pose estimation tracking actionevent recognition anticipation paper concluded discussion insights future directions © springer nature switzerland ag 2019', 'recent studies shown remarkable advances 3d human pose estimation monocular images help largescale indoor 3d datasets sophisticated network architectures however generalizability different environments remains elusive goal work geometryaware 3d representation human pose address limitation multiple views simple autoencoder model training stage 2d keypoint information supervision view synthesis learn shared 3d representation viewpoints synthesizing human pose one viewpoint one instead performing direct transfer raw imagelevel skeletonbased encoderdecoder mechanism distil poserelated representation latent space learningbased representation consistency constraint introduced facilitate robustness latent 3d representation since learnt representation encodes 3d geometry information mapping 3d pose much easier conventional image 2d coordinates input 3d pose estimator demonstrate task 3d human pose estimation comprehensive experiments three popular benchmarks show model significantly improve performance simply injecting representation robust 3d prior © 2019', 'human pose estimation received significant attention recently due various applications real world performance human pose estimation improved deep learning paper presents comprehensive survey deep learning human pose estimation analyzes methodologies employed summarize discuss recent works methodologybased taxonomy singleperson multiperson pipelines first reviewed separately deep learning techniques applied pipelines compared analyzed datasets metrics used task also discussed compared aim survey make every step estimation pipelines interpretable provide readers readily comprehensible explanation moreover unsolved problems challenges future research discussed © 19962012 tsinghua university press', 'paper present intelligent interaction system capable autonomous mobility athome workouts whereas current interaction systems athome workouts much difficulty mobility presented system able assist people work athome advanced deep learning technologies action recognition human pose estimation biosignal recognition also autonomous mobility utilization mobile robot verify systems feasibility regard intelligent interaction capable autonomous mobility conducted individual experiments module system mobile robot navigation interfacing unity contents visualization action recognition human pose estimation biosignal recognition human data generation © 2019', 'fall risk high older adults dementia gait impairment contributes increased fall risk gait changes common people dementia although reliable assessment gait challenging population study aimed develop automated performing gait assessments gait data collected frequently unobtrusively analysed computer vision recent developments computer vision led availability open source human pose estimation algorithms automatically estimate joint locations person image study preexisting pose estimation model applied 1066 walking videos collected 31 older adults dementia walked naturally corridor specialized dementia unit two week period tracked pose information gait features extracted video recordings gait bouts association clinical mobility assessment scores future falls data examined significant association found extracted gait features clinical mobility assessment number future falls providing concurrent predictive validation © 2013', 'estimating 3d human pose monocular images challenging problem due variety complexity human poses inherent ambiguity recovering depth single view recent deep learning show promising results supervised learning 3d pose annotated datasets however lack largescale 3d annotated training data captured inthewild settings makes 3d pose estimation difficult inthewild poses approaches utilized training images 3d 2d pose datasets weaklysupervised manner learning 3d poses unconstrained settings paper effectively predict 3d human pose 2d pose deep neural network trained weaklysupervised manner combination groundtruth 3d pose groundtruth 2d pose reprojection error minimization constraint predict 3d locations body joints crucial training data 3d groundtruth present since minimizing reprojection error alone may guarantee accurate 3d pose also additional geometric constraints skeleton pose regularize pose 3d demonstrate superior generalization ability crossdataset validation challenging 3d benchmark dataset mpiinf3dhp containing wild 3d poses © 2019', 'human pose estimation hpe basis human action recognition analysis better locate key points human body accurately achieve desired results certain applications paper discuss explore overall pipeline refers alphapose among single person pose estimation network improved hourglass networkthe calls vector hourglass network main process vector hourglass network follows first heatmap values graph obtained firstlevel secondlevel hourglass network saved respectively heatmap values obtained firstlevel network geometric center gravity obtained joint points heatmap taken joint coordinates calculating paf weight calculate paf values weighting heatmap value getted firstlevel network thermal value obtained secondlevel network new thermal value secondlevel hourglass network obtained test phase since project detect pose single person lsp leeds sports pose dataset single person sports dataset selected test set existing open dataset verify validity project calculates mean square error mse mean absolute error mae values heatmap weighted paf weights results show positioning accuracy key joint points improved 143 pixels image 64×80 pixel size applies idea intermediate supervision part affinity fields paf two levels hourglass network improve positioning accuracy joints makes obtained human posture contour obvious © 2019', 'work addresses problem 3d human body shape pose estimation single depth image 3d human pose estimation deep learning utilize rgb images instead depth images traditional optimizationbased depth images aim establish point correspondences depth images template model paper novel estimate 3d pose shape human body depth images specifically joints features original depth features spatial attention feature extractor capture spatial local features depth images 3d joints learning dynamic weights features addition generalize real depth data weaklysupervised conduct extensive experiments surreal human36m dfaust real depth images human bodies experimental results demonstrate 3d human pose estimation yield good performance © 2020 springer nature switzerland ag', 'vibration serviceability footbridges important terms fitness purpose humaninduced dynamic loading primary excitation footbridges researched traditional sensors inertial sensors force plates along development computer hardware algorithms eg machine learning especially deep learning computer vision technology improves rapidly potential application problem high precision pedestrian detection realized various computer vision corresponding different situations demands paper two widely recognized computer vision approaches used detecting body center mass ankle movement explore potential humaninduced vibration research consumergrade cameras used without artificial markers take videos processing wearable inertial sensors used validate evaluate computer vision measurements © copyright © 2019 wang brownjohn dai patel', 'introduce pose estimation sensor array data utilizing inverse graphics general unsupervised manipulating 3d model discover jointspace parameters produce observed image sensor reading novel architecture discuss design model performing human pose estimation data obtained mattress pressure sensor © 2019', 'due trade model complexity estimation accuracy current human pose estimators either low accuracy requires long running time dilemma especially severe real time multiperson pose estimation address issue design deep network reduced parameter size high estimation accuracy via introducing deconvolution layers instead widely used fullyconnected configuration specifically model consists two successive parts detection network matching network former outputs keypoint heatmap person locations latter produces final pose estimation multiple deconvolutional layers benefiting simple structure explicit utilization previously neglected spatial structure heatmap matching network specially high efficiency high precision experiments challenging coco dataset demonstrate almost cut running parameters matching network achieving higher accuracy existing © 2019', 'noninvasive behavioral tracking animals experiments critical many scientific pursuits extracting poses animals without markers often essential measuring behavioral effects biomechanics genetics ethology neuroscience however extracting detailed poses without markers dynamically changing backgrounds challenging recently introduced opensource toolbox called deeplabcut builds human poseestimation algorithm allow user train deep neural network limited training data precisely track userdefined features match human labeling accuracy provide updated toolbox developed python package includes new features graphical user interfaces guis performance improvements activelearningbased network refinement provide stepbystep procedure deeplabcut guides user creating tailored reusable analysis pipeline graphical processing unit gpu 1–12 h depending frame size additionally provide docker environments jupyter notebooks run cloud resources google colaboratory © 2019 authors exclusive licence springer nature limited', '66 special focus conference international conference computational collective intelligence topics studying accuracy improvement gm 1 1 model evaluation imagebased malware classification machine learning automatic container code recognition multideep pipeline efficient solution people tracking profiling video streams lowpower compute simple pose network skipconnections single human pose estimation simple finetuning attention modules human pose estimation human eye detector lightweight efficient convolutional neural network robust contentbased recommendation distribution system gaussian mixture model incremental svdbased collaborative filtering enhanced diversity personalized recommendation collaborative filtering recommendation statistical implicative analysis deep learning multilingual pos tagging object searching video orb descriptor support vector machine improved joint reversible data hiding encrypted remote sensing satellite images 3d kinematics upper limb functional assessment htc vive unreal engine 4 2dcnn segmentation ischemic stroke lesions mri scans melanoma skin cancer classification transfer learning design neural controller control rescue quadcopter hang status multidimensional analysis scada stream data estimating energy efficiency mining transport simple haulage cycles detection lhd machine haul truck cycle identification support vector machine dbscan models data quality management erp systems – accounting case study machine learning techniques accident data identification determinants prerevenue young enterprises value', 'struckby accidents potential safety concerns construction sites require robust machine pose estimation development deep learning enhanced human pose estimation adapted articulated machines require abundant dataset training challenging timeconsuming obtain onsite paper fast data collection build dataset excavator pose estimation two industrial robot arms excavator camera monopod collect different excavator pose data 3d annotation obtained robots embedded encoders 2d pose annotated manually evaluation 2500 pose images collected trained stacked hourglass network results showed dataset suitable excavator pose estimation network training controlled environment leads potential dataset augmenting real construction site images © 2019 american society civil engineers', 'flexible mixture model solve problems human pose estimation model composed joint appearance innerjoint relationship models trained deep convolutional neural network dcnn firstly graphical model constructed represent joints limbs human body secondly images decomposed several image blocks centered joints used training input data finally multiple classification dcnn network obtained perform human pose estimationthe flexible human body representation detection rate joint points correct detection rate effectively improved © 2019 science press right reserved', 'key part unmanned retail judge bought product solve problems combines human pose estimation commodity detection algorithm deep learning process videos realtime apply depth separable convolution modify human pose estimation algorithm reduce size convolution kernel fuse multistage information detect commodities construct commodity detection dataset train object detection model modified pose estimation algorithm used identify key points left right wrists human body object detection algorithm recognizes products existing current image calculate distance key points products value determine whether customer purchased certain product result tested pipeline real scenarios determine whether user purchased product computer vision judgment convenient deploy development unmanned retail research results applied brickandmortar stores © 2020 springer nature singapore pte ltd', '3d human pose estimation assume input images scene collected one several viewpoints video given consequently focus estimates leveraging prior knowledge measurement fusing information spatially andor temporally whenever available paper address problem active observer freedom move explore scene spatially timefreeze mode andor temporally selecting informative viewpoints improve estimation accuracy towards end introduce posedrl fully trainable deep reinforcement learningbased active pose estimation architecture learns select appropriate views space time feed underlying monocular pose estimator evaluate model single multitarget estimators strong result settings system learns automatic stopping conditions time transition functions next temporal processing step videos extensive experiments panoptic multiview setup complex scenes containing multiple people show model learns select viewpoints yield significantly accurate pose estimates compared strong multiview baselines © aaai 2020 34th aaai conference artificial intelligence rights reserved', 'new bottomup multiperson 2d human pose estimation particularly well suited urban mobility selfdriving cars delivery robots new pifpaf part intensity field pif localize body parts part association field paf associate body parts form full human poses outperforms previous low resolution crowded cluttered occluded scenes thanks new composite field paf encoding finegrained information ii choice laplace loss regressions incorporates notion uncertainty architecture fully convolutional singleshot boxfree design perform par existing bottomup standard coco keypoint task produce results modified coco keypoint task transportation domain © 2019', 'existing deep learning approaches 3d human pose estimation videos either recurrent convolutional neural networks rnns cnns however rnnbased tackle sequences limited frames sequential models sensitive bad frames tend drift long sequences although existing cnnbased temporal attempt address sensitivity drift problems concurrently processing input frames sequence existing cnnbased limited 3d pose estimation single frame sequential input paper deep learningbased utilizes matrix factorization sequential 3d human poses estimation processes input frames concurrently avoid sensitivity drift problems yet outputs 3d pose estimates every frame input sequence specifically 3d poses frames represented motion matrix factorized trajectory matrix trajectory coefficient matrix trajectory matrix precomputed matrix factorization approaches singular value decomposition svd discrete cosine transform dct problem sequential 3d pose estimation reduced training deep network regress trajectory coefficient matrix demonstrate effectiveness long sequences achieving performances multiple benchmark datasets source code available httpsgithubcomjiahaoljhtrajectorypose3d © 2019 copyright document resides authors', 'paper presents robust human posture body parts detection specific application scenario known inbed pose estimation although human pose estimation various computer vision cv applications studied extensively last decades inbed pose estimation camerabased vision ignored cv community assumed identical general purpose pose estimation problems however inbed pose estimation specialized aspects comes specific challenges including notable differences lighting conditions throughout day pose distribution different common human surveillance viewpoint paper demonstrate challenges significantly reduce effectiveness existing general purpose pose estimation models order address lighting variation challenge infrared selective irs image acquisition technique provide uniform quality data various lighting conditions addition deal unconventional pose perspective 2end histogram oriented gradient hog rectification presented deep learning proves effective model human pose estimation however lack large public dataset inbed poses prevents us large network scratch paper explored idea employing pretrained convolutional neural network cnn model trained large public datasets general human poses finetuning model shallow limited size different perspective color inbed irs dataset developed irs imaging system collected irs image data several realistic lifesize mannequins simulated hospital room environment pretrained cnn called convolutional pose machine cpm finetuned inbed pose estimation retraining specific intermediate layers hog rectification pose estimation performance cpm improved significantly 264 probability correct keypoint pck criteria pck01 compared model without rectification even testing well aligned inbed pose images finetuned model still surpassed traditionally tuned cnn another 166 increase pose estimation accuracy © 2013', 'inferring 3d scene information 2d observations open problem computer vision deeplearning energy minimization learn consistency measure 2d observations world model demonstrate trained endtoend produce consistent realistic inferences evaluate human pose estimation voxelbased object reconstruction benchmarks show competitive results achieved relatively shallow networks drastically fewer learned parameters floating point operations conventional deeplearning approaches © 2019', 'paper new geometric model mixture markov random fields mrfs human pose estimation build previous work expresses global constraints relative locations body joints autoencoder convnet performs dimensionality reduction heat maps recovers manner low dimensional manifold global pose human body lies address shortcomings architecture obtain meaningful vectors span low dimensional pose space replace autoencoder network layer layer implements gaussian mixture model gmm provides soft clustering human pose predictions online fashion show large number meaningful global poses feasible way preserving underlying structure informative body poses b clustering helps properly initialize mrf filters different global pose c body joint masking data augmentation procedure better exploited system stability significantly improved best knowledge first time clustering algorithm like gmm used online fashion problem 2d human pose estimation efficacy demonstrated extensive experiments widely used public benchmarks © 2019 springer nature switzerland ag', 'detection abnormal postures reclining person crucial part visual surveillance even regular poses appear rotated incongruity image angle preinstalled camera however existing human pose estimation focus small rotational changes ie less 50 degrees seldom consider robust human pose estimation drastic rotational changes best knowledge reports robustness human pose estimation rotational changes large angles study robust human pose estimation creating path learning new rotational changes selfsupervised combining results obtained path supervised furthermore combination module composed convolutional layer trained complementarily paths network produce robust results various rotational changes demonstrate robustness extensive experiments images generated rotating elements standard benchmark datasets fully analyze rotational characteristics human pose estimators coco keypoint detection dataset attains 15 improvement mean average precision compared standard deviation performance improved 47 times © 2013', 'recent advances deep learning computer vision offer excellent opportunity investigate highlevel visual analysis tasks human localization human pose estimation although performances human localization human pose estimation significantly improved recent reports perfect erroneous estimation position pose expected among video frames studies integration techniques generic pipeline robust errors still lacking paper fills missing study explored developed two working pipelines suited visualbased positioning pose estimation tasks analyses pipelines conducted badminton game showed concept tracking detection could work well errors position pose could effectively handled linear interpolation information nearby frames results showed visualbased positioning pose estimation could deliver position pose estimations good spatial temporal resolutions © 2020 springer nature switzerland ag', 'estimation athletes pose video footage enables automation athletic performance assessment prediction motion kinematics dynamics sports videos possibility technologyassisted direct training feedback despite remarkable progress field deep learning assisted human pose estimation performance systems decreases noise errors increase complexity scene paper focus aquatic training scenarios even novel pose estimators produce several types orthogonal errors including joint swaps prediction outliers order improve estimation athletes pose swimming graph partitioning problem connects pose estimates time explicitly allows joints switch labels location better fits others trajectory optimize problem integer linear programming partitions graph probable joint trajectories show experimentally joint rectification improves joint detection precision swimmers swimming channel 0848 pck antisymmetrical motion 18 pck symmetrical styles © 2019', 'recently several deep learning models 3d human pose estimation nevertheless approaches focus singleperson case estimate 3d pose people high resolution furthermore many applications autonomous driving crowd analysis require pose estimation large number people possibly lowresolution work present pandanet pose estimation dectection anchorbased network new singleshot anchorbased multiperson 3d pose estimation model performs bounding box detection detected person 2d 3d pose regression single forward pass need postprocessing regroup joints since network predicts full 3d pose bounding box allows pose estimation possibly large number people low resolution manage people overlapping introduce poseaware anchor selection strategy moreover imbalance exists different people sizes image joints coordinates different uncertainties depending sizes automatically optimize weights associated different people scales joints efficient training pandanet surpasses previous singleshot several challenging datasets multiperson urban virtual realistic dataset jta dataset two real world 3d multiperson datasets cmu panoptic mupots3d © 2020', 'describe 3d human pose estimation transient images ie 3d spatiotemporal histogram photons acquired optical nonlineofsight nlos imaging system perceive 3d human pose looking around corners light indirectly reflected environment bring together diverse set technologies nlos imaging human pose estimation deep reinforcement learning construct endtoend data processing pipeline converts raw stream photon measurements full 3d human pose sequence estimate contributions design data representation process includes 1 learnable inverse point spread function psf convert raw transient images deep feature vector 2 neural humanoid control policy conditioned transient image feature learned interactions physics simulator 3 data synthesis augmentation strategy depth data transferred realworld nlos imaging system preliminary experiments suggest able generalize realworld nlos measurement estimate physicallyvalid 3d human poses1 © 2020', 'paper studies task 3d human pose estimation single rgb image challenging without depth information recently many deep learning achieve great improvements due strong representation learning however existing ignore relationship joint features paper joint relationship aware neural network take global local joint relationship consideration first whole feature block representing human body joints extracted convolutional neural network dual attention module dam applied whole feature block generate attention weights exploiting attention module global relationship whole joints encoded second weighted whole feature block divided individual joint features capture salient joint feature individual joint features refined individual dams finally joint angle prediction constraint consider local joint relationship quantitative qualitative experiments 3d human pose estimation benchmarks demonstrate effectiveness © 19922012', 'research 3d human pose estimation deep neural networks recently witnessed substantial progress accuracy execution efficiency many combine deep neural networkbased 2d pose estimation 3d pose matching however 1 multiscale analysis potential improve inference accuracy 2 isometric constraint considered 3d matching stage paper new module named densely connected attentional pyramid residual module dcaprm bottomup mapping stage presented effectively increase inference accuracy new isometric regularization term also punish limb extension shrinkage topdown fitting phase performance 3d human pose datasets evaluated experimental results show provides better results approaches terms accuracy 3d human pose estimation © 2019 elsevier bv', 'purpose many years deep convolutional neural networks achieved results wide variety computer vision tasks 3d human pose estimation makes exception results public benchmarks impressive however specialized domains operating rooms pose additional challenges clinical settings severe occlusions clutter difficult lighting conditions privacy concerns patients staff make necessary unidentifiable data work aim bring robust human pose estimation clinical domain 2d–3d information fusion makes network multiple depth cameras strong pose priors first step probabilities 2d joints predicted single depth images information fused shared voxel space yielding rough estimate 3d pose final joint positions obtained regressing latent pose space pretrained convolutional autoencoder results evaluate several baselines challenging mvor dataset best results obtained fusing 2d information multiple views constraining predictions learned pose priors conclusions present robust 3d human pose estimation multidepth camera network operating room depth images input modalities make especially interesting clinical applications due given anonymity patients staff © 2019 cars', 'designed human motion statistics system used count users ’ physical activity system recognition human action depends 3d human pose estimation combine probability model 3d human pose multilayer cnn architecture obtain 3d human pose estimation different actions defined proportion distance joint amount action counted total calories burned exercise obtained calculating calories consumed action system simple convenient used many situations experimental results show system achieved desired results © 2019 springer nature switzerland ag', '3d cnnbased architectures found application variety 3d vision tasks significantly outperforming earlier approaches increase accuracy however come cost computational complexity deep learning models becoming complex requiring significant computational resources especially case 3d data meanwhile growing adoption low power devices various technology fields shifted research focus towards implementation deep learning systems limited resources plenty approaches achieved promising results terms reducing computational complexity 2d tasks applicability 3dcnn designs thoroughly researched current work aims filling void investigating series efficient cnn design techniques within scope 3dcnns order produce guidelines 3dcnn design applied already established architectures reducing computational complexity following guidelines computationally efficient 3dcnn architecture human pose estimation 3d data achieving comparable accuracy design guidelines validated within scope 3d object classification achieving high accuracy results low computational cost © 2019 copyright document resides authors', 'recent advancements human motion behaviors camera images made human motion tracking much robust past however still computationally expensive allow online reconstruction human pose context work presents tracking 2d human pose high frequency keeping robustness achieved result combining means kalman filtering recent success openpose robust deep learningbased 2d human pose estimation technique fast lucaskanade features matching allows processing images different framerates thus multirate measurement updates frequency estimation kept high setting kalman filter time step tested videos several activities fast slow motion results show improvement reconstruction error increased speed reconstruction better tracking fast motion capability cover loss tracking two measurements achieved intraframe two available measurements trajectory estimation frequency high 1 khz © 2019', 'multiperson pose estimation 2d image essential technique human behavior understanding paper human pose refinement network estimates refined pose tuple input image input pose pose refinement performed mainly endtoend trainable multistage architecture previous however highly dependent pose estimation models require careful model design contrast modelagnostic pose refinement according recent study 2d human pose estimation similar error distributions error statistics prior information generate synthetic poses synthesized poses train model testing stage pose estimation results input moreover model require code knowledge allows easily used postprocessing step show achieves better performance conventional multistage refinement models consistently improves performance various pose estimation commonly used benchmark code available footnoteurlhttpsgithubcommks0601posefix release © 2019', 'human pose estimation hpe convolutional neural networks cnn demonstrated significant progress achieved results human pose datasets study aimed assess performance cnnbased hpe measuring anthropometric data vicon motion analysis system reference system stereo vision system recorded ten asymptomatic subjects standing front stereo vision system static posture eight hpe estimated 2d poses transformed 3d poses stereo vision system percentage correct keypoints 3d error absolute error body segment lengths evaluation measures used assess results percentage correct keypoints – standard metric 2d pose estimation – showed hpe could estimate 2d body joints minimum accuracy 99 meanwhile average 3d error absolute error body segment lengths 5 cm © springer nature switzerland ag 2019', 'paper deep learning exploits multiperson pose estimation image sequence predict individual actions well collective activity group scene first apply multiperson pose estimation extract pose information image sequence novel representation called pose motion history pmh aggregates spatiotemporal dynamics multiperson human joints whole scene single stack feature maps individual pose motion history stacks indipmh cropped whole scene stack sent cnn model obtain individual action predictions individual predictions construct collective map encodes positions actions individuals group scene feature map stack final group activity prediction determined fusing results two classification cnns one takes whole scene pose motion history stack input takes collective map stack input evaluate challenging volleyball dataset provides competitive performance compared © 2020 springer nature switzerland ag', 'human body posture recognition important concern research many fields field human pose estimation algorithm called openpose widely used efficiency low used deep learning tensorflow recognize human body postures applied judgment teachers teaching states order choose algorithm works best scenario designed eight sets experimental schemes combining classification model detection algorithm used groups schemes detect classify teachers teaching states including standing sitting etc last analyzed experimental results depth selected suitable algorithm scenario © 2019', 'performance deep neural networks improves annotated data problem budget annotation limited one solution active learning model asks human annotate data perceived uncertain variety recent apply active learning deep networks either designed specific target tasks computationally inefficient large networks paper novel active learning simple taskagnostic works efficiently deep networks attach small parametric module named loss prediction module target network learn predict target losses unlabeled inputs module suggest data target model likely produce wrong prediction taskagnostic networks learned single loss regardless target tasks rigorously validate image classification object detection human pose estimation recent network architectures results demonstrate consistently outperforms previous tasks © 2019', 'multicamera deep learning vision applications subscribe edge computing paradigm due stringent latency requirements however guaranteeing latency wireless communication links cameras nodes edge server challenging especially cheap easily available unlicensed bands due interference camera nodes system external sources paper show approximate computation techniques used design latency controller multiple video frame image quality control knobs simultaneously satisfy latency accuracy requirements machine vision applications involving object detection human pose estimation experimental results edge test bed indicate controller able correct 164 degradation latency due interference within settling time 115 © springer nature switzerland ag 2019', 'recent years due deep learning development computer vision great progress made 3d human pose estimation rgb images however due lack depth information rgb images task still faces great challenges paper adversarial learning estimate 3d pose human body consists two parts pose generator discriminator 3d pose descriptor designed adversarial learning effectively increase accuracy visual effect 3d pose estimation results performed ablation experiments public dataset good improvement compared baseline © 2019', 'paper estimation human pose making tof time flight cameras yolo object detection used develop topdown first stage network developed detect people image second stage network developed estimate joints person image result first stage show deep learning network trained scratch tof images yields better results taking deep neural network pretrained rgb data retraining tof data also show topdown detector person detector joint detector works better detecting body joints entire image copyright © 2019 scitepress – science technology publications lda rights reserved', 'paper presents human tracking 3d pose estimation algorithm pair 360◦ cameras identify track individual throughout complex multiperson scenes indoor outdoor environments appearance models positional data produce temporally consistent 3d skeleton optimising skeleton realistic joint lengths joint positions produce convolutional pose machines cpms results show average improvement 2267 state art deep learning approaches tracking well reasonable estimates pose two cameras © 2019 computer society rights reserved', 'existing rgb cnnbased video action recognition mostly distinguish human body environment thus easily overfit scenes objects training sets work present conceptually simple general highperformance action recognition videos aiming personcentric modeling called action machine person bounding boxes instancelevel action analysis extends inflated 3d convnet i3d adding branch human pose estimation 2d cnn posebased action recognition action machine benefit multitask training action recognition pose estimation fusion predictions rgb images poses experiments results provided trimmed video action datasets ntu rgbd northwestern ucla multiview action3d msr daily activity3d action machine achieves superior performance generalizes well across datasets © 19942012', 'human pose estimation fundamental challenging task computer vision although batch normalization widely used deep learning feature extraction deep convolutional neural networks dcnn still well explored work pose encoding module pem enhance learning ability generalization ability feature extraction given input images pem integration instance normalization batch normalization combining appropriate way learn capture eliminate appearance changes maintaining distinction learning features seen integration adjustment global information addition simple efficient upsampling strategy recover highresolution representations predicting accurate human keypoint heatmaps achieved better performance average network recent missions studied standard benchmark human pose estimation © 2019 published licence iop publishing ltd', 'leveraging powerful deep convolutional networks 2d human pose estimation achieved great success hand 3d human pose estimation still challenging task attracts great attention due inherent depth ambiguity 2d 3d mapping conventional typically able predict 3d locations precisely especially joints far torso paper coarsetofine model predict 3d joint locations progressively observe joints like shoulders hips relatively easy get precise 3d locations utilized facilitate prediction hard joints far torso make happen set constraints human limb length ratio prior guide model generate reasonable predictions conduct experiments human36m dataset comparison experimental results benchmark dataset turns outperforms baseline © 2019 springer nature switzerland ag', 'leveraging power deep neural networks singleperson pose estimation made substantial progress throughout last years recently multiperson pose estimation also become growing importance mainly driven high demand reliable video surveillance systems public security keep demands certain efforts made improve performance systems yet limited insufficient amount available training data work addresses lack labeled data diminishing often faced problem domain shift synthetic images computer game graphics engines real world data annotated training data shall provided zero labelingcost end generative adversarial networks applied domain adaption adapting data novel synthetic pose estimation dataset several real world target domains domain adaption extended meet important requirement exact content preservation synthetic adapted images experiments subsequently conducted indicate improved suitability adapted data human pose estimators trained data outperform trained purely synthetic images © 2020 spie', 'distributed camera network human pose estimation solve problem limited view occlusion single view great potential wide area surveillance applications fuse different field view information distributed human pose estimation combining interactive multiple model imm algorithm distributed information fusion human skeleton joints compared works often single motion model depict motion human skeleton joints eg constant velocity novelty work maneuvering property human action handled imm ie motion model human skeleton joints filter approximated constant velocity constant acceleration singer motion models advantages imm algorithm maneuvering target tracking solve singleview occlusion problem also solve problem joint point fluctuation caused estimation error sensor node distributed information fusion final human action recognition experimental results show improve action recognition rate datasets captured kinect v2 addition built distributed camera network embedded machine learning boards deep learningbased human pose estimation employed handle limitations original kinect sdk © 2019', 'sporadic falls due lack balance factors complications elderly people might experience frequently others accordingly high probability events causing major health casualties bone breaking head clots studies monitoring falls rapidly assist victim work evaluate multistream learning model convolutional neural networks highlevel handcrafted features input order cope situation therefore consists extracting highlevel handcrafted features instance human pose estimation optical flow one input distinct vgg16 classifier addition experiments able showcase features used fall detection results shown assembling directed input learners outperforms terms accuracy sensitivity rates similar tested found literature © 2019', 'nonvolitional discontinuation motion namely bradykinesia common motor symptom among patients parkinsons disease pd evaluating bradykinesia severity important part clinical examinations pd patients diagnosis monitoring phases however subjective evaluations different clinicians often show low consistency research works explore objective quantification bradykinesia mostly highlyintegrated sensors although sensorbased demonstrate applaudable performance unrealistic promote wide special devices require far popularized daily lives paper take advantage computer vision machine learning technologies proposing visionbased automatically objectively quantify bradykinesia severity three bradykinesiarelated items investigated study finger tapping hand clasping hand prosupination human pose estimation technology utilized extract kinematic characteristics supervisedlearningbased classifiers employed generate score ratings clinical experiment 60 patients shows scoring accuracy 360 examination videos 897 competitive related works devices requires camera instrumentation laptop data processing therefore produce reliable assessment results parkinsonian bradykinesia minimal device requirement showing great potential realizing longterm remote monitoring patients condition © 20012011', 'accuracy monocular 3d human pose estimation depends viewpoint image captured freely moving cameras drones provide control viewpoint automatically positioning location yield highest accuracy remains open problem problem address paper specifically given short video sequence introduce algorithm predicts viewpoints chosen capture future frames maximize 3d human pose estimation accuracy key idea underlying estimate uncertainty 3d body pose estimates integrate several sources uncertainty originating deep learning regressors temporal smoothness motion planner yields improved 3d body pose estimates outperforms matches existing ones person following orbiting © 2020', 'computer vision cv attempts mimic human eyes image processing identifications detailed visual information object positions features appearances even human emotions behaviors research one hundred literatures relating applying deep learning dl methodologies advanced computer visions 2010∼2018 reviewed analyzed objective discover dl topics trends cv practical applications dl algorithms aim representing multilevels distributed neural networks enhancement high speed computational power dl modeling accumulated big data analytics found practical applications nonsupervised intelligent decision supports detection product defects prognosis machine malfunctions realtime signal feature data analyses vast number literature describing dl related researches developments implementations problem solving comprehensive mining related literature integrate latent dirichlet allocation lda kmeans clustering normalized term frequencyinverse document frequency ntfidf approaches discover called technology mining major trends dl computer visions specifically key applications object detection semantic segmentation image retrieval human pose estimation © 2019', 'convolutional neural network approaches monocular 3d human pose estimation usually require large amount training images 3d pose annotations feasible provide 2d joint annotations large corpora inthewild images humans providing accurate 3d annotations inthewild corpora hardly feasible practice existing 3d labelled data sets either synthetically created feature instudio images 3d pose estimation algorithms trained data often limited ability generalize real world scene diversity therefore new deep learning monocular 3d human pose estimation shows high accuracy generalizes better inthewild scenes network architecture comprises new disentangled hidden space encoding explicit 2d 3d features supervision new learned projection model predicted 3d pose algorithm jointly trained image data 3d labels image data 2d labels achieves accuracy challenging inthewild data © 2019', 'studying human motion images videos turned interesting topic research given recent advances computer vision deep learning algorithms focusing automatic procedure tracking physical exercises cameras used full human pose estimation relation worn sensors work workout repetition counting validation set skeletonbased deep semantic features obtained 2d human pose estimation network given individuals ’ body parts might occluded throughout physical exercises also perform multiview analysis supporting cameras improve recognition rates nevertheless obtained results singleview show able count valid repetitions 90 precision scores 4 5 considered exercises recognizing 50 invalid ones © springer nature switzerland ag 2020', '70 special focus conference soft computing signal processing topics human pose estimation activity classification machine learning implementation interpolation credit card fraud detection statisticalsemantic pso model customer reviewsbased question answering systems lowcost iot spirometer device silicon pressure sensor recommending research article user queries latent dirichlet allocation applied time series forecasting model yield prediction agricultural crop design implementation protocol secure softwarebased remote attestation iot devices label assignment sentimental analysis product review twitter data preserving privacy audio file ideal secret sharing scheme cloud storage hadoop performance acceleration effective data job placement modified quick fuzzy hypersphere neural network pattern classification supervised clustering third eye assistance reading disability enhanced attach procedure prevention authentication synchronisation failure attack static load balancing algorithms cloud computing challenges solutions survey text question responsive systems english indian languages vehicle type classification deep learning deep learning detection replay attack asv systems acoustic scene classification convolutional neural network classification microscopic algae observational study alexnet innovative secure authentication interface hadoop cluster dna cryptography practical study novel true colour image bit modification technique image steganography grubased neural machine translation followed proper noun transliteration realtime suspicious activity detection', '58 special focus conference intelligent data engineering automated learning topics modelling survival machine learning liver transplantation application unos dataset design development automatic blood detection system capsule endoscopy images comparative analysis computerbased decision support case study knee osteoarthritis clusteringbased patient grouper burn care comparative assessment feedforward convolutional neural networks classification prostate lesions filter bank common spatial pattern multiclass motor imagery bci safe deep neural networkdriven autonomous vehicles software safety cages wave viscous resistance estimation nn neural controller uavs inertia variations unsupervised initialization archetypal analysis proportional membership fuzzy clustering metric quantifying data concentration adaptive machine learningbased stock prediction financial time series technical indicators exploiting online newspaper articles metadata profiling city areas modelling social interactions ant colony optimization innovative deeplearning algorithm supporting approximate classification workloads big data environments controlflow business process summarization via activity contraction classifying flies reconstructed audio signals studying evolution ‘ circular economy ’ concept topic modelling mining frequent distributions time series time series display knowledge discovery selective laser melting machines multimodal web video annotator realtime human pose estimation prior knowledge facilitate computational reading arabic calligraphy smote algorithm variations balancing data streams', 'work presented paper dedicated determining evaluating efficient neural network architecture applied multiple regression network localizing human body joints 3d space single low resolution depth image main challenge deal noisy coarse representation human body observed depth sensor large distance achieve high localization precision regression network expected reason relations body parts depth image extract locations joints provide coordinates defining body pose involved creation dataset 200000 realistic depth images 3d body model training testing numerous architectures including feedforward multilayer perceptron network deep convolutional neural networks results training evaluation included discussed accurate dnn network trained evaluated augmented depth images dataset achieved accuracy similar reference kinect algorithm results great benefit fast processing speed significantly lower requirements sensor resolution used 100 times less pixels kinect depth sensor robust sensor noise allowing imprecision depth measurements finally results compared vgg mobilenet resnet architectures © 2019 authors', 'recovering persons height single image important virtual garment fitting autonomous driving surveillance however also challenging without absolute scale information examine rarely addressed case camera parameters scene geometry unknown circumstances scale inherently ambiguous height inferred statistics intrinsic human anatomy estimated images directly articulated pose bonelength proportions facial features contribution twofold first create new humanheight dataset three magnitudes larger existing ones mining explicit height labels propagating additional images face recognition assignment consistency second test wide range machine learning models linear shallow deep models capture relation image content human height also show performance predominantly limited dataset size central finding height estimated large uncertainty remaining high variance demonstrates geometrically motivated scale ambiguity persists age deep learning important implications pose monocular reconstruction 3d human pose estimation scale invariant way © 2019', 'paper interested human pose estimation problem focus learning reliable highresolution representations existing recover highresolution representations lowresolution representations produced hightolow resolution network instead network maintains highresolution representations whole process start highresolution subnetwork first stage gradually add hightolow resolution subnetworks one one form stages connect mutliresolution subnetworks parallel conduct repeated multiscale fusions hightolow resolution representations receives information parallel representations leading rich highresolution representations result predicted keypoint heatmap potentially accurate spatially precise empirically demonstrate effectiveness network superior pose estimation results two benchmark datasets coco keypoint detection dataset mpii human pose dataset addition show superiority network pose tracking posetrack dataset code models publicly available httpsgithubcomleoxiaobindeephighresolutionnetpytorch © 2019', 'occlusion key problem 3d human pose estimation monocular video address problem introduce occlusionaware deeplearning employing estimated 2d confidence heatmaps keypoints opticalflow consistency constraint filter unreliable estimations occluded keypoints occlusion occurs incomplete 2d keypoints feed 2d 3d temporal convolutional networks 2d 3d tcns enforce temporal smoothness produce complete 3d pose incomplete 2d keypoints instead complete incorrect ones networks less affected errorprone estimations occluded keypoints training occlusionaware 3d tcn requires pairs 3d pose 2d pose occlusion labels dataset available introduce cylinder man model approximate occupation body parts 3d space projecting model onto 2d plane different viewing angles obtain label occluded keypoints providing us plenty training data addition model create pose regularization constraint preferring 2d estimations unreliable keypoints occluded outperforms human 36m humanevai datasets © 2019', 'human pose estimation longstanding challenging problem computer vision problem involves high freedom articulation body limbs different occlusions selfocclusion occlusion objects persons various clothing various background natural image foreshortening due different capturing angle camera work present 1 densenet module used improve original resnet hourglass model 2 intermediate points derived ground truth joint segments used output augmentation convolutional neural network convnet improve prediction accuracy improvement also made via intermediate points voting optimizing joint probability distribution human joints intermediate points experimental results effects intermediate point optimization scheme presented able achieve competitive results © 2019', '47 special focus conference advanced concepts intelligent vision systems topics bayesian feature pyramid networks automatic multilabel segmentation chest xrays assessment cardiothoratic ratio deeplearning tidemark segmentation human osteochondral tissues imaged microcomputed tomography quadratic tensor anisotropy measures reliable curvilinear pattern detection exposing presentation attacks combination multiintrinsic image properties convolutional networks transfer learning multiview 3d markerless human pose estimation openpose skeletons cliplevel feature aggregation key factor videobased person reidentification towards approximating personality cues simple daily activities person identification walking gesture skeleton sequences verifying kinship rgbd face data epnet deep neural network ear detection 3d point clouds vastargan continuous affect generation fast iris segmentation algorithm visible wavelength images multicolor space local flow phase stretch transform robust retinal vessel detection evaluation unconditioned deep generative synthesis retinal images dynamic texture representation hierarchical local patterns temporalclustering technique identifying thermal regions buildings distance weighted loss forest trail detection semantic line localization map changes exploiting slam residuals initial pose estimation 3d object severe occlusion deep learning automatic focal blur segmentation difference blur feature theoretical thresholding graphcuts fire segmentation still images feature map augmentation improve rotation invariance convolutional neural networks svmbased zerowatermarking technique 3d videos traitor tracing', 'paper new system estimating 3d human pose tof sensor system single lowcost tof sensor captures depth data human pose new clean imaging converter tof sensor transforms depth data corrupted sensor errors zero values clean 3d image data finally deep learning predictor convolutional neural networks pretrained millions 2d image data human pose 3d ones estimates 3d human pose clean 3d image data experimental results show system shows good performance comparable commercial system terms 3d human pose estimation accuracy © 2019', 'despite large improvements performance attained deep learning computer vision one often improve results additional postprocessing exploits geometric nature underlying task commonly involves displacing posterior distribution cnn way makes appropriate task hand eg better aligned local image features compact work integrate geometric postprocessing within deep architecture introducing differentiable probabilistically sound counterpart common geometric voting technique used evidence accumulation vision refer resulting neural models mass displacement networks mdns apply human pose estimation two distinct setups landmark localization collapse distribution point allowing precise localization body keypoints b communication across body parts transfer evidence one part allowing globally consistent pose estimate evaluate largescale pose estimation benchmarks mpii human pose coco datasets report systematic improvements © 2018 copyright document resides authors', 'direct control humanoid robot human motion important aspect current research additional equipments kinect usually equipped robot order avoid external equipments explored robot controlling lowresolution camera robot firstly stacked hourglass network employed obtain accurate 2d heatmap containing positions human joints rgb image captured camera robot 3d human poses including coordinates human body joints estimated 2d heatmaps aiming reconstruct 3d human poses 2d poses finally rotation angles robot computed according 3d coordinates transmitted robot reconstruct original human pose nao robot example experimental results show humanoid robot imitate motions different human actors different scenes well applying © 2019 springer nature switzerland ag', 'video annotation expensive time consuming consequently datasets multiperson pose estimation tracking less diverse sparse annotations compared large scale image datasets human pose estimation makes challenging learn deep learning models associating keypoints across frames robust nuisance factors motion blur occlusions task multiperson pose tracking address issue relies keypoint correspondences associating persons videos instead training network estimating keypoint correspondences video data trained large scale image dataset human pose estimation selfsupervision combined topdown human pose estimation keypoint correspondences recover missed pose detections ii associate pose detections across video frames achieves results multiframe pose estimation multiperson pose tracking posetrack 2017 2018 datasets © 2020 springer nature switzerland ag', 'road centerline extraction key step road network extraction modeling handcraft feature engineering traditional road extraction unstable makes extracted road centerline deviated road center complex cases even results overall extracting errors recently road centerline extraction semantic segmentation employing deep neural network greatly outperformed traditional nevertheless pixelwise labels training deep learning models expensive postprocess road segmentation errorprone inspired work human pose estimation deepwindow novel automatically extract road network remote sensing images deepwindow sliding window guided cnnbased decision function track road network directly images without prior road segmentation first design train cnn model estimate road center points inside patch road seeds automatically searched patch patch employing cnn model finally starting seeds first estimates road direction fourier spectrum analysis algorithm iteratively tracks road centerline along road direction guided cnn model cnn model trained point annotations greatly reduces training costs comparing semantic model training achieves comparable performance road extraction extensive experiments indicate robust point deviation © 20082012', 'existing deep learningbased markless human pose estimation single depth map common takes 2d depth map directly regresses 3d coordinates human body joints via 2d convolutional neural networks cnns depth map intrinsically 3d data treat 2d images distort shape actual object projection 3d 2d space compels network perform perspective distortioninvariant estimation moreover directly regressing 3d coordinates 2d image highly nonlinear mapping causes difficulty learning procedure overcome problems module called supervised endecoder process 3d convolution data also stacked series connection adapt different size dataset module network called supervised high dimension endecoder network designed used predict key points markless human single depth map 3d space experiments show improved prediction accuracy compared approaches © 2019', 'recognize object human easy task machines perform task efficiency complex task computer systems images sets numeric values meaning make numbers useful diverse techniques comparative others deep learning approaches achieved performance many computer vision applications object detection image classification image retrieval human pose estimation detect object interest convolutional neural network cnn observed widely successful factors get better accuracy performance instance efficient model larger datasets hardware support study aims review cnn object detection highlighting contribution challenges recent research also well cnn techniques combination best suited results better performance increased accuracy fast processing reduce error rates also introduced new concerns issues parallel regarding discussed time consumption anonymous behavior neural network address issues conceptual model presented cnn lease square support vector machine lssvm © 2019', 'view significant effects deep learning graphics image processing research human pose estimation deep learning attracted much attention many models produced one another basis tracking indepth study domestic foreign research results paper concentrates 3d single person pose estimation contrasts analyzes three endtoend staged hybrid network models summarizes characteristics evaluating performance set experimental environment utilize human36m data set test several mainstream test results indicate hybrid network model better performance field human pose estimation © 2020 authors ios press', 'paper present exercisecheck exercisecheck interactive computer vision system sufficiently modular work different sources human pose estimates ie estimates deep traditional models interpret rgb rgbd camera input pilot study first compare pose estimates produced four deep models rgb input ms kinect rgbd data results indicate performance gap required us choose ms kinect tested exercisecheck parkinsons disease patients homes exercisecheck capable customizing exercises capturing exercise information evaluating patient performance providing therapeutic feedback patient therapist checking progress user course physical therapy supporting patient throughout period conclude exercisecheck userfriendly computer vision application assist patients providing motivation guidance ensure correct execution required exercises results also suggest considerable progress field pose estimation deep learning current deep learning models fully ready replace rgbd sensors especially exercises involved complex patient population accounted carefully tracked active range motion © 2019', 'deep learning approaches rapidly adopted across wide range fields accuracy flexibility require large labeled training datasets presents fundamental problem applications limited expensive private data ie small data human pose behavior estimationtracking could highly personalized paper present semisupervised data augmentation synthesize large scale labeled training datasets 3d graphical engines physicallyvalid low dimensional pose descriptor evaluate performance synthesized datasets training deep learningbased models generated large synthetic human pose dataset called scanava 3d scans 7 individuals augmentation human pose estimation deep learning model trained scratch scanava dataset could achieve pose estimation accuracy 912 pck05 criteria applying efficient domain adaptation synthetic images pose estimation accuracy comparable model trained large scale pose data real humans mpii dataset much higher model trained synthetic human dataset surreal © 2019 springer nature switzerland ag', 'present unsupervised learning recover 3d human pose 2d skeletal joints extracted single image require multiview image data 3d skeletons correspondences 2d3d points previously learned 3d priors training lifting network accepts 2d landmarks inputs generates corresponding 3d skeleton estimate training recovered 3d skeleton reprojected random camera viewpoints generate new synthetic 2d poses lifting synthetic 2d poses back 3d reprojecting original camera view define selfconsistency loss 3d 2d training thus self supervised exploiting geometric selfconsistency liftreprojectlift process show selfconsistency alone sufficient generate realistic skeletons however adding 2d pose discriminator enables lifter output valid 3d poses additionally learn 2d poses wild train unsupervised 2d domain adapter network allow expansion 2d data improves results demonstrates usefulness 2d pose data unsupervised 3d lifting results human36m dataset 3d human pose estimation demonstrate improves upon previous unsupervised 30 outperforms many weakly supervised approaches explicitly 3d data © 2019', 'major challenge 3d human pose estimation ambiguity process regressing 3d poses 2d ambiguity introduced poor exploiting image cues especially spatial relations previous works try weaklysupervised constrain illegal spatial relations instead leverage image cues directly follow weaklysupervised train endtoend network first detecting 2d body joints heatmaps constraining 3d regression 2d heatmaps utilize inherent spatial relations multiscale recalibrated regress 3d pose recalibrated integrated network independent module scale factor altered capture information different resolutions additional multiscale recalibration modules spatial information pose better exploited regression process whole network finetuned extra parameters quantitative result human36m dataset demonstrates performance surpasses qualitative evaluation results human36m inthewild mpii datasets show effectiveness robustness handle complex situations selfocclusions © springer nature switzerland ag 2019', 'human pose estimation dramatically improved thanks continuous developments deep learning however markerfree human pose estimation standard framebased cameras still slow power hungry realtime feedback interaction huge number operations necessary large convolutional neural network cnn inference eventbased cameras dynamic vision sensor dvs quickly output sparse movingedge information sparse rapid output ideal driving lowlatency cnns thus potentially allowing realtime interaction human pose estimators although application cnns standard framebased cameras human pose estimation well established application eventbased cameras still study paper novel benchmark dataset human body movements dynamic vision sensor human pose dataset dhp19 consists recordings 4 synchronized 346x260 pixel dvs cameras set 33 movements 17 subjects dhp19 also includes 3d pose estimation model achieves average 3d pose estimation error 8 cm despite sparse reduced input data dvs © 2019', '164 special focus conference computer science online topics process automation scenario intelligence investigation units experience development elements intelligent highperformance platform distributed decision support system monitoring diagnostics technological objects developing efficient automatic threshold detection hybrid feature selection rewardtovariability ratio key performance indicator financial manager efficiency assessment civic engagement analysis open social media data human pose estimation applying ann rgbd cameras video handling remote sensing image processing modified fuzzy algorithm fog robotics distributed computing monitoring task study evaluation novel chaotic system applied image encryption security statistical analyses model adaptive system neurofuzzy inference pid pidfuzzycontrollers impact advanced technologies cyber attacks surface multilayer global tracing bioinspired deep learning model long shortterm memory dlstm prediction currency exchange rates energy consumption reduction real time multiprocessor embedded systems uncertain data neural network optimization algorithms controlled switching systems studies big data processing linear accelerator sources machine learning experimental study fogcomputingbased systems reliability recurrent neural network hardware implementation theoretical domains applied cybersecurity behaviour correlationextreme systems defect search pipeline networks spatial analysis management inconsistent data sources preface', '73 special focus conference image analysis recognition topics slicing dicing soccer automatic detection complex events spatiotemporal data rnvid feature fusion architecture video object detection color inference semantic labeling person search videos 2d bidirectional gated recurrent unit convolutional neural networks endtoend violence detection videos video live tracking fishes tanks external knowledge improve zeroshot action recognition egocentric videos semanticsguided warping semisupervised video object instance segmentation twostream activity recognition 2d human pose estimation video object segmentation convex optimization foreground background distributions flowchroma deep recurrent neural network video colorization deep learning partial fingerprint inpainting recognition visual perception analyse neonatal pain face images combining asynchronous events traditional frames steering angle prediction survey preprocessing techniques classification approaches online signature verification ssim signature facial microexpressions learning search objects images human gaze sequences detecting defects materials deep convolutional neural networks visual perception ranking chess players video tampering detection decentralized video transcoding networks generalized subspace learning roweis discriminant analysis benchmark generic product detection low data baseline dense object detection understanding public speakers ’ performance first contributions support computational open source multipurpose multimedia annotation tool slambased multistate tracking system mobile humanrobot interaction', 'preserving maintaining teaching traditional martial arts important activities social life helps individuals preserve national culture exercise selfdefense people however traditional martial arts many different postures activities body body parts problem estimating actions human body still many challenges accuracy obscurity forth especially 3d human pose estimation paper convolutional neural network cnn estimating key points joints actions traditional martial postures 2d space projecting results 3d space apply measurements length joints deviation angle joints deviation distance key points evaluating pose estimation cnn model cpm convolutional pose machine since comparative study cpm trained classic mscoco keypoints challenge dataset 1 human36m 2 results evaluated martial arts dancing sports dataset zhang et al 3 quantitatively results evaluated published © 2019', 'popularization health concept demand fitness trainer system increased however existent trainer systems provide motion demonstration lack users ’ motion feedback paper designs implements intelligent fitness trainer system human pose estimation shows fitness training courses also provides motion correction system obtains users ’ motion data optical camera applies human pose estimation finally providing motion correction advice paper present system design hardware software introduce applied human pose estimation algorithm detail field trail results show system exerts good influence fitness training © springer nature singapore pte ltd 2019', 'paper serves survey empirical evaluation activity recognition still rgb images andor videos understanding human activities videos still images challenging task computer vision domain identifying action activity accomplished automatically recognizing represents prime goal intelligent video system human activity recognition arises various application domains varying human computer interfaces health care monitoring surveillance security despite ongoing efforts domain tasks remained unsolved unconstrained environments face many challenges occlusions variations clothing background clutter recently numerous deep learning algorithms solve traditional artificial intelligence problems shown great advances particular pose estimation task since extract appropriate features jointly performing discrimination paper provide detailed review recent research advances field human activity recognition categorization human activity methodologies discuss advantages limitations particular divide feature representation global local body modeling human activity classification approaches arranged three categories reflect model human activities templatebased generative discriminative moreover provide comprehensive analysis posebased human activity recognition conventional deep learningbased human pose estimation approaches reported finally discuss openchallenges field endeavor provide possible solutions © 2019', 'work introduces novel convolutional feature task human pose estimation fusing convolutional neural network multilevel graph structure model improve pose estimation results bodypart detection human spatial structure stage part detection probability vectors corresponding human body parts whole image analyzed convolutional neural network novel multilevel graph structure model designed contains whole body human body parts joints realizes coarsetofine establishment constrained human spatital constraint model levels structures edges pixels obtained probability vectors put multilevel graph structure model compute location coordinates joint successfully achieving combining deep learning network multilevel graph structure model large number qualitative quantitative experimental results show compared stateofart integration deep learning network multilevel pictorial structure model improve accuracy human pose estimation greater extent © 2019 springerverlag london ltd part springer nature', 'driver pose estimation key component driver monitoring systems helpful driver anomaly detection compared traditional human pose estimation driver pose estimation required fast compact embedded systems fast compact driver pose estimation composed shufflenet v2 integral regression shufflenet v2 reduce computational expense integral regression reduce quantization error heat maps driver suddenly gets seriously ill head driver view therefore addition localizing body parts classifying whether body part view also crucial driver anomaly detection also novel model localize detect body part driver extensive experiments conducted driver pose estimation dataset recorded near infrared camera capture driver night achieves large improvement compared human pose estimation limited computation resources futhermore perform ablation study composed shufflenet v2 integral regression driver body parts detection finally show experimental results driver action driver monitoring systems © 2020 japanese society artificial intelligence rights reserved', 'paper present deep learning estimate human pose 3d multiple 2d views available system composed cascade specialized systems firstly 2d poses obtained deep neural network detection skeleton keypoints available view 3d coordinates keypoint reconstructed least squares optimization analyzes quality 2d detections decide either consider reject 3d poses obtained time step full body pose estimation performed long shortterm memory lstm neural network takes advantage process history refine final pose estimation provide evidence suitability contributions extensive experimental study finally able prove experimentally obtains competitive results compared recent representative works literature © 2018 elsevier bv', 'human pose estimation important problem computer vision dominated deep learning techniques recent years paper novel model named mixedscale dense block exploits dilation convolution layers dense concatenation connections maximise information flow block consequently captures feature representation different scales effectively efficiently comparing baseline hourglass models model employs fewer learning parameters nevertheless experiments demonstrate model produces accurate predictions meanwhile achieves comparable accuracy techniques especially indicators better performance addition model easy implement could improved existing techniques adopted promote hourglass models © springer nature switzerland ag 2019', '13 special focus conference cyberspace data intelligence topics research human pose estimation object detection field unmanned retail edge computingbased solution softwaredefined industrial intelligent control industrial internet things wisector novel sectorbased wifi scheduling iterative optimization edge federated learning machinery health prognostics dust removal fan data deep neural networks numeric cnns cnns reading numeric characters meters deeptsw urban traffic safety warning bayesian deep learning multimodal semantic model packaging sorting field deep learning classification autism fmri data featurefused convolutional neural network topic logistics node resource status summarization coal mine accident reports naturallanguageprocessingbased', 'convolution neural networks cnns achieved best performance human pose estimation also computer vision tasks eg object detection semantic segmentation image classification paper focuses useful attention module feedforward cnns firstly feed feature map block backbone network attention module split two separate dimensions channel spatial combines two feature maps multiplication gives next block backbone network capture information longrange dependencies channel spatial data gain better performance accuracy therefore experimental results illustrate different attention module existing result predicted joint heatmap maintains accuracy spatially better simple baseline besides architecture gains 10 points ap higher baseline moreover network trained coco 2017 benchmarks accessible dataset nowadays © 2020 springer nature switzerland ag', 'human pose estimation hpe computer vision problem become increasingly popular last years multiple applications medical field therapy virtual augmented reality robot caregivers virtual physical therapy kinematic analysis nevertheless machine learning algorithms developed applications trained small datasets images captured constrained scenarios information given sensors bounding applicability developed simple yet useful deep learning algorithm human pose estimation input image scene people estimated position joints body parts used retrieve basic kinematic information people image applied aforementioned medical applications focus overcoming limit human pose estimation algorithms due jittering aiming preserve precise pixel location thus explore different novel approaches improve precision existing algorithms keypoint estimation evaluate coco keypoint dataset outperforming current top hope algorithm encourages academic community develop simpler precise hpe algorithms medical applications rgb images © copyright spie downloading abstract permitted personal', 'human pose estimation formulated joint heatmap regression problem deep learning existing convolutional neural networks usually adopt fixed kernel size generating joint heatmaps without regard size human shapes paper novel address issue adapting kernel size joint heatmaps human scale input images training stage present normalization strategy perform adaption kernel size human scale beyond introduce novel limb region representation learn human pose structural information adaptive joint heatmaps well limb region representation combined together construct novel neural network named multiscale adaptive structure network masn effectiveness network evaluated two widely used human pose estimation benchmarks experiments demonstrate could obtain results outperform existing body parts © 2019 springer nature switzerland ag', 'paper estimating 3d human pose single rgb image compared either provide point estimates coordinate regression unimodal predictions joint locations predicts joint locations multimodal distributions addition apply datadriven learn conditional dependencies relative positions joints endtoend takes input images either 2d 3d labels performs par better human36m mpii datasets © 2019', '3d human pose estimation monocular image 2d joints illposed problem depth ambiguity occluded joints argue 3d human pose estimation monocular input inverse problem multiple feasible solutions exist paper novel generate multiple feasible hypotheses 3d pose 2d joints contrast existing deep learning approaches minimize mean square error unimodal gaussian distribution able generate multiple feasible hypotheses 3d pose multimodal mixture density networks experiments show 3d poses estimated input 2d joints consistent 2d reprojections supports argument multiple solutions exist 2dto3d inverse problem furthermore show performance human36m dataset best hypothesis multiview settings demonstrate generalization capacity model testing mpii mpiinf3dhp datasets code available project website © 2019', 'recent findings indicate overparametrization crucial successfully training deep neural networks also introduces large amounts redundancy tensor potential efficiently parametrize overcomplete representations leveraging redundancy paper fully parametrize convolutional neural networks cnns single highorder lowrank tensor previous works network tensorization focused parametrizing individual layers convolutional fully connected perform tensorization layerbylayer separately contrast jointly capture full structure neural network parametrizing single highorder tensor modes represent architectural design parameters network eg number convolutional blocks depth number stacks input features etc parametrization allows regularize whole network drastically reduce number parameters model endtoend trainable lowrank structure imposed weight tensor acts implicit regularization study case networks rich structure namely fully convolutional networks fcns parametrize single 8thorder tensor show achieve superior performance small compression rates attain high compression rates negligible drop accuracy challenging task human pose estimation © 2019', 'precise orientationestimation humans relative pose monocular camera system challenging task due general aspects camera calibration deformable nature human body motion thus novel approaches deep learning precise object poseestimation robotics hard adapt human body analysis work hybrid accurate estimation human body rotation relative camera system presented thereby significantly improving results derived posenet applying analysis optical flow frame frame comparison human body inplace rotating tpose thereby aligned center applying object tracking compensate translations body movement 2d skeleton extraction optical flow calculated region interest roi area aligned relative vertical skeleton joint representing spine compared frame frame evaluate eligibility clothing fundament good feature local pixel homogeneity taken consideration restrict optical flow heterogeneous regions distinctive features like imprint patterns buttons buckles besides local illumination changes mean optical flow coarse approximation axial body shape ellipsis accuracy 01° 20° target rotation 10° orientationestimation achieved frametoframe comparison evaluated validated computer generated imagery cgi renderings realworld videos people wearing clothing varying feature appropriateness © 2020 vaclav skala union agency rights reserved', 'computer vision research nowadays largely datadriven due prevalence deep learning one reason depth data become less popular datasets exist comparable common color datasets terms size quality however depth data advantages practical applications involve people case utilizing cameras raises privacy concerns consider one application namely 3d human pose estimation health care application study whether lack large depth datasets represent problem overcome via synthetic data aspects must considered ensure generalization compares alternative approaches obtaining training data furthermore compare pose estimation performance depth data color images show depth data suitable alternative color images regard copyright © 2020 scitepress – science technology publications lda rights reserved', '84 special focus conference image analysis recognition topics sequential image synthesis human activity video generation deep learningbased noiseresilient keyword spotting engine embedded platforms compact representation histopathology images digital stain separation frequencybased encoded local projections computeraided tumor segmentation t2weighted mr images patientderived tumor xenografts sittostand analysis wild silhouettes longitudinal health monitoring target aware visual object tracking design endtoend dual mode driver distraction detection system keytrack lightweight scalable lstmbased pedestrian tracker surveillance systems kptransfer improved performance faster convergence keypoint subsetwise domain transfer human pose estimation netscore towards universal metrics largescale performance analysis deep neural networks practical ondevice edge usage deep learning model skin lesion segmentation fully convolutional network deep learning bayesian optimization facial age estimation female facial beauty analysis transfer learning stacking ensemble model investigating automatic classification algae spectral morphological characteristics via deep residual learning random field computational adaptive optics optical coherence microscopy deep learning approaches gynaecological ultrasound image segmentation radiofrequency vs bmode comparison discovery radiomics detection severely atypical melanocytic lesions saml skin imaging via deep residual group convolutional radiomic sequencer identifying diagnostically complex cases ensemble learning tchexnet detecting pneumothorax chest xray images deep transfer learning improving lesion segmentation diabetic retinopathy adversarial learning', 'purpose work develop computational intelligence models neural networks nn fuzzy models fm support vector machines svm long shortterm memory networks lstm predict human pose activity image sequences computer vision approaches gather required features obtain human pose semantics output classes set 3d points describe human body model input variables predictive model prediction models obtained acquired data example video images way predict semantics atomic activities compose activity human body model extracted video frame prediction models learned lstm networks cases best learned models implemented application test systems svm model obtained 9597 correct classification six different human poses tackled work tests different situations training phase implemented lstm learned model achieved overall accuracy 88 tests different situations training phase results demonstrate validity approaches predict human pose activity image sequences moreover system capable obtaining atomic activities quantifying time interval activity takes place © 2020 authors licensee mdpi basel switzerland', 'human pose estimation deep neural networks dnns pose estimation formulated dnnbased regression problem towards body joints present cascade dnn regres sors results high precision pose estimates advantage reasoning pose holistic fashion simple yet powerful formula tion capitalizes recent advances deep learn ing present detailed empirical analysis stateof art better performance four academic benchmarks diverse realworld images © 2014', 'recovery 3d human pose monocular camera inherently illposed problem due large number possible projections 2d image 3d space aimed improving accuracy 3d motion reconstruction introduce additional builtin knowledge namely heightmap algorithmic scheme reconstructing 3d posemotion singleview calibrated camera novel consists two major contributions firstly rgb image calculated heightmap combined detect landmarks 2d joints dualstream deep convolution network secondly formulate new objective function estimate 3d motion detected 2d joints monocular image sequence reinforces temporal coherence constraints camera 3d poses experiments human eva human36m mcad dataset validate outperforms algorithms 2d joints localization 3d motion recovery moreover evaluation results human eva indicates performance singleview comparable multi view deep learning counterpart © springer international publishing ag 2016', 'availability cheap effective depth sensors resulted recent advances human pose estimation tracking detailed estimation hand pose however remains challenge since fingers often occluded may represent pixels moreover labelled data difficult obtain deep learning basedapproach hand pose estimation targeting gesture recognition requires little labelled data leverages unlabeled data synthetic data renderings key making work integrate structural information model architecture would slow inference training objective show adding unlabelled realworld samples significantly improves results compared purely supervised setting © springer international publishing switzerland 2015', 'intelligent vehicle community devoted considerable efforts model driver behavior particular detect overcome driver distraction effort reduce accidents caused driver negligence however domain increasingly shifts toward autonomous semiautonomous solutions driver longer integral decisionmaking process indicating need refocus efforts elsewhere end study pedestrian distraction instead particular focus detecting pedestrians engaged secondary activities involving cellphones similar handheld multimedia devices purely visionbased standpoint achieve objective pipeline incorporating articulated human pose estimation followed soft object label transfer ensemble exemplar support vector machines trained nearest neighbors pose feature space additionally incorporate head gaze features prior pose information carry cellphone related pedestrian activity recognition finally offer reliably track articulated pose pedestrian sequence images particle filter gaussian process dynamical model used estimate sequentially varying activity scores low computational cost entire fast especially sequential data accurate easily extensible secondary activities sources distraction © 2016', 'human pose estimation deep learning achieved high accuracy various difficult poses however computationally expensive often suitable mobile systems paper investigate mobilenets wellknown lightweight efficient cnn architecture mobile embedded vision applications adapt mobilenets pose estimation inspired hourglass network introduce novel split stream architecture final two layers mobilenets reduces overfitting resulting improvement accuracy reduction parameter size also show maintaining part original network able improve accuracy transferring learned features imagenet pretrained mobilenets adapted model evaluated flic dataset network outperformed default mobilenets pose estimation well achieved performance comparable state art results reducing inference time significantly © 2018', '63 special focus conference computer vision vehicle technology spontaneous facial behavior analysis consumer depth cameras computer vision video event categorization tagging retrieval towards big data topics aesthetic judgment intersection art science classification artistic styles binarized features derived deep neural network artistic image analysis composition human figures improving ancient roman coin recognition alignment spatial encoding visionbased vehicle localization visual street map embedded surf scale approximated relative pose solvers efficient camera motion estimation autonomous landing lowcost quadrotor monocular cameras nature conservation drones automatic localization counting animals statistically learned deformable eye models quantifying microexpressions constraint local model local binary pattern analysing user visual implicit feedback enhanced tv scenarios exploiting pose information gait recognition depth streams assessing suitability microsoft kinect calculating person specific body segment parameters active patch model real world appearance reconstruction multiscale deep learning gesture detection localization nonparametric gesture labeling multimodal data action gesture temporal spotting super vector representation action detection improved dense trajectories sliding window sign language recognition convolutional neural networks multimodal gesture recognition skeletal joints motion trail model easy minimax estimation random forests human pose estimation', 'h150 pose estimation important research topic field computer vision artificial intelligence paper focuses stateofart progress 2d human pose estimation deep learning according neural network structure classified single cnn multistage cnn multibranch cnn recurrent neural network rnn generative adversarial networks gan summarize analyze attributes performance future development direction also prospected © 2018', '46 special focus conference computer vision topics multiview geometry compression camera calibration common selfpolar triangle sphere images multiscale tetrahedral fusion similarity reconstruction noisy positional measurements depth estimation parameter transfer single still images representation learning smooth autoencoder single image smoke detection adaptive sparse coding painting style analysis efficient image detail mining multiview point cloud registration affine shape distributions part detector discovery deep convolutional neural networks performance evaluation 3d local feature descriptors scene text detection robust stroke width transform deep belief network regularity guaranteed human pose correction accelerated kmeans clustering binary random projection efficient largescale structure motion graph partitioning deep learning motion features human pose estimation accelerating cost volume filtering salient subvolumes robust occlusion handling 3d human pose estimation monocular images deep convolutional neural network plant leaf identification via growing convolution neural network progressive sample learning understanding convolutional neural networks terms categorylevel attributes graphical model rapid obstacle imagemap estimation unmanned surface vehicles elastic shape analysis boundaries planar objects multiple components arbitrary topologies minimal solution relative pose unknown focal length radial distortion simultaneous entire shape registration multiple depth images depth difference shape silhouette', 'motion capture systems conventionally used motion analysis however deep learning technique recently made significant progress human pose estimation twodimensional image purpose study develop mobile application evaluating running form deep learning twodimensional images conducted biomechanical experimentto investigate accuracy application comparison motion capture system showed excellent consistency data addition collected running form data 642 runners developed evaluation standards running form six metrics thus application pose estimation technique combined evaluation standards could easy useful tool running form analysis © 2018', 'present novel human performance capture technique capable robustly estimating pose articulated joint positions performer observed passively via multiple viewpoint video mvv affine invariant pose descriptor learned convolutional neural network cnn trained volumetric data extracted mvv dataset diverse human pose appearance manifold embedding learned via gaussian processes cnn descriptor articulated pose spaces enabling regression estimation human pose mvv input learned descriptor manifold shown generalise wide range human poses providing efficient performance capture solution requires fiducials markers worn system evaluated ground truth joint configuration data commercial markerbased pose estimation system © springer international publishing switzerland 2016', 'deep learning directly estimate human joint positions 3d space 2d fisheye images captured egocentric manner core novel network architecture inceptionv3 4 featuring asymmtric convolutional filter size long shortterm memory module anthropomorphic weights training loss demonstrate outperform different tasks helpful develop useful deep learning network humanmachine interaction vrar applications © 2018 copyright held ownerauthors', 'recent years video surveillance research able recognize various behaviors pedestrians analyze overall situation objects combining image analysis technology deep learning human activity recognition har important issue video surveillance research field detect abnormal behavior pedestrians cctv environment order recognize human behavior necessary detect human image estimate pose detected human paper novel 2d human pose estimation object detection rgbd information adding depth information rgb information limitation detecting object due lack topological information improve detecting accuracy subsequently rescaled region detected object applied convolutional pose machines cpm sequential prediction structure convolutional neural network utilize cpm generate belief maps predict positions keypoint representing human body parts estimate human pose detecting 14 key body points experimental results prove detects target objects robustly occlusion also possible perform 2d human pose estimation providing accurately detected region input cpm future work estimate 3d human pose mapping 2d coordinate information body part onto 3d space consequently provide useful human behavior information research har © 2018 ksii', 'human pose estimation wellknown computer vision problem receives intensive research interest reason interest wide range applications successful estimation human pose offers articulated pose estimation includes real time acquisition analysis processing understanding high dimensional visual information ensemble learning operating handengineered features commonly used addressing task deep learning exploits representation learning learn multiple levels representations raw input data alleviating need handcrafted features deep convolutional neural networks achieving visual object recognition localization detection paper pose estimation task formulated offset joint regression problem 3d joints positions accurately detected single raw depth image deep convolutional neural networks model presented relies utilization data generation pipeline generate large realistic highly varied synthetic set training images analysis experimental results demonstrate generalization performance real time successful application © 2016', 'paper presents localize human body joints 3d coordinates single low resolution depth image first generate database 80k realistic depth images 3d body model described data preprocessing normalization procedure dnn mlp artificial neural networks architectures training presented robustness camera distance image noise analysed localization accuracy joint reported application low resolution large distance pose estimation fast regression body joints locations 3d space achieved even case sensor noise large distance reaching screen © 2017 division signal processing electronic systems poznan university technology', 'paper addresses problems graphicalbased human pose estimation still images including diversity appearances confounding background clutter present new architecture estimating human pose convolutional neural network cnn firstly relative mixture deformable model rmdm defined pair connected parts compute relative spatial information graphical model secondly local multiresolution convolutional neural network lmrcnn train learn multiscale representation body parts combining different levels part context thirdly lmrcnn hierarchical model defined explore context information limb parts finally experimental results demonstrate effectiveness deep learning human pose estimation © 2018 authors', 'paper presents deep learning problem human pose estimation employ generative adversarial networks learning paradigm set two stacked hourglass networks architecture one generator discriminator generator used human pose estimator training done discriminator distinguishes groundtruth heatmaps generated ones backpropagates adversarial loss generator process enables generator learn plausible human body configurations shown useful improving prediction accuracy © 2018 apsipa organization', 'heterogeneous multitask learning human pose estimation monocular images deep convolutional neural network particular simultaneously learn human pose regressor slidingwindow bodypart jointpoint detectors deep network architecture show including detection tasks helps regularize network directing converge good solution report competitive stateofart results several datasets also empirically show learned neurons middle layer network tuned localized body parts © 2014 springer sciencebusiness media new york', 'toward realization autonomous driving deep learning attracted attention seen indispensable technology alexnet consists eight layers incorporating ideas improving generalization able accomplish substantial improvement accuracy image recognition task since image recognition also various tasks applications various fields dramatically advanced even autonomous driving field many manufacturers taken aggressive efforts pushing ahead practical application paper introduce tasks tackled autonomous driving tasks introduced object detection human pose estimation semantic segmentation images sensors combining possible realize safer automatic operation system © springer international publishing ag part springer nature 2018', 'estimating threedimensional 3d human poses single camera usually implemented searching pose candidates image descriptors existing usually suppose mapping feature space pose space linear fact mapping relationship highly nonlinear heavily degrades performance 3d pose estimation recover 3d pose silhouette image multiview feature embedding mfe localitysensitive autoencoders lsaes one hand first depict manifold regularized sparse lowrank approximation mfe input image characterized fused feature descriptor hand fused feature corresponding 3d pose separately encoded lsaes twolayer backpropagation neural network trained parameter finetuning used map encoded 2d features encoded 3d poses lsae ensures good preservation local topology data points experimental results demonstrate effectiveness © 2017 spie ist', 'teaching robot predict mimic human moves acts near future observing series historical human movements crucial first step humanrobot interaction collaboration paper instrument robot prediction ability leveraging recent deep learning computer vision techniques first system takes images robot camera input produce corresponding human skeleton realtime human pose estimation obtained openpose library conditioning historical sequence robot forecasts plausible motion motion predictor generating corresponding demonstration lack highlevel fidelity validation existing forecasting algorithms suffer error accumulation inaccurate prediction inspired generative adversarial networks gans introduce global discriminator examines whether predicted sequence smooth realistic resulting motion gan model achieves superior prediction performance approaches evaluated standard h36m dataset motion gan model robot demonstrates ability replay predicted motion humanlike manner interacting person © 2018', 'existing deep learningbased 3d hand human pose estimation single depth map common takes 2d depth map directly regresses 3d coordinates keypoints hand human body joints via 2d convolutional neural networks cnns first weakness presence perspective distortion 2d depth map depth map intrinsically 3d data many previous treat depth maps 2d images distort shape actual object projection 3d 2d space compels network perform perspective distortioninvariant estimation second weakness conventional directly regressing 3d coordinates 2d image highly nonlinear mapping causes difficulty learning procedure overcome weaknesses firstly cast 3d hand human pose estimation problem single depth map voxeltovoxel prediction 3d voxelized grid estimates pervoxel likelihood keypoint design model 3d cnn provides accurate estimates running realtime system outperforms previous almost publicly available 3d hand human pose estimation datasets placed first hands 2017 framebased 3d hand pose estimation challenge code available in1 © 2018', 'deep learning architectures obtained significant results human pose estimation last years studies state art usually focus attention estimation human pose adults people depicted images estimation pose child infants toddlers children sparsely studied despite useful different application domains assistive computer vision eg early detection autism spectrum disorder monitoring pose child time could reveal important information especially clinical trials human pose estimation benchmarked variety challenging conditions studies highlight performance specifically children ’ poses still missing infants toddlers children smaller adults also significantly different anatomical proportions also assistive context unusual poses assumed children challenging infer objective study paper compare different state art approaches human pose estimation benchmark dataset useful understand performances subjects children results reveal accuracy state art drop significantly opening new challenges research community © 2017 springer international publishing ag', 'authors consider problem human pose estimation probabilistic convolutional neural networks explore ways improve human pose estimation accuracy standard pose estimation benchmarks mpii human pose leeds sports pose lsp datasets probabilistic deep learning transform deterministic neural network probabilistic one allow sampling independent equiprobable hypotheses different outputs given input overlapping body parts body joints hidden clothes obstacles make problem human pose estimation ambiguous context get accurate estimation joints position uncertainty networks predictions represented variance hypotheses provided probabilistic convolutional neural network confidence characterised mean work current cnn cascades pose estimation evaluate three probabilistic convolutional neural networks built top deterministic ones two probabilistic deep learning disco networks bayesian segnet authors evaluate models standard pose estimation benchmarks show probabilistic models outperform deterministic ones © institution engineering technology 2018', 'visual appearance score appearance mixture type deformation three important information sources human pose estimation paper build multisource deep model order extract nonlinear representation different aspects information sources deep model global highorder human body articulation patterns information sources extracted pose estimation task estimating body locations task human detection jointly learned unified deep model viewed postprocessing pose estimation results flexibly integrate existing taking information sources input extracting nonlinear representation multiple information sources deep model outperforms 86 percent three public benchmark datasets © 2014', 'present simultaneously estimating 3d human pose body shape sparse set widebaseline camera views train symmetric convolutional autoencoder dual loss enforces learning latent representation encodes skeletal joint positions time learns deep representation volumetric body shape harness latter upscale input volumetric data factor 4 × whilst recovering 3d estimate joint positions equal greater accuracy state art inference runs realtime 25 fps potential passive human behaviour monitoring requirement high fidelity estimation human body shape pose © springer nature switzerland ag 2018', 'paper presents grouplevel emotion recognition subchallenge emotiw 2018 task classify image one group emotions positive negative neutral mainly exploits three types visual cues task namely face body global image recent deep networks main contribution twofold first introduce body convolutional neural networks cnns task previous winner 18 specially crop bodies image human pose estimation train body cnns imagelevel labels group emotions body cue captures full view individual second cascade attention network face cue images network exploits importance face image generates global representation faces cascade attention network complementary models also improves naive average pooling 2 finally achieve second place subchallenge classification accuracies 869 6748 validation set testing set respectively © 2018 association computing machinery', 'paper discriminative human pose estimation system deep learning monocular videosequences combines simple efficient convolutional neural network directly regresses 3d pose estimation recurrent denoising autoencoder provides pose refinement temporal information contained sequence previous frames architecture also able provide integrated training parts order better model space activities noisy realistic poses produced partially trained cnn used enhance training autoencoder system evaluated two standard datasets humanevai human36m comprising 15 different activities show simple architecture provide state art results © springer international publishing ag part springer nature 2018', 'recent approaches monocular 3d pose estimation rely deep learning either train convolutional neural network directly regress image 3d pose ignores dependencies human joints model dependencies via maxmargin structured learning involves high computational cost inference time paper introduce deep learning regression architecture structured prediction 3d human pose monocular images 2d joint location heatmaps relies overcomplete autoencoder learn highdimensional latent pose representation accounts joint dependencies efficient long shortterm memory network enforce temporal consistency 3d pose predictions demonstrate achieves performance terms structure preservation prediction accuracy standard 3d human pose estimation benchmarks © 2018 springer sciencebusiness media llc part springer nature', 'present novel 3d pose estimation joint interdependency ji acquiring 3d joints human pose rgb image ji incorporates body part structural connectivity joints learn high spatial correlation human posture towards goal new long shortterm memory lstmbased deep learning architecture named propagating lstm networks plstms lstm connected sequentially reconstruct 3d depth centroid edge joints learning intrinsic ji first lstm seed joints 3d pose created reconstructed wholebody joints connected lstms utilizing plstms achieve higher accuracy 112 largest publicly available database importantly demonstrate ji drastically reduces structural errors body edges thereby leads significant improvement © springer nature switzerland ag 2018', 'study first time show formulate structured support vector machine ssvm two layers convolutional neural network top layer loss augmented inference layer bottom layer normal convolutional layer show deformable part model learned structured svm neural network backpropagating error deformable part model convolutional neural network forward propagation calculates loss augmented inference backpropagation calculates gradient loss augmented inference layer convolutional layer thus obtain new type convolutional neural network called structured svm convolutional neural network applied human pose estimation problem new neural network used final layers deep learning jointly learns structural model parameters appearance model parameters implemented new layer existing caffe library © 2017 elsevier ltd', 'last decade many studies focus modeling driver behavior particular detecting overcoming driver distraction effort reduce accidents caused driver negligence studies assume entire onus avoiding accidents driver alone study adopt different stance study behavior pedestrians instead particular focus detecting pedestrians engaged secondary activities involving cellphones similar handheld multimedia devices purely visionbased standpoint achieve objective pipeline incorporating articulated human pose estimation gradient image features detect presenceabsence device either hand pedestrian information different streams dependencies one another encoded belief network network used predict probability score suggesting involvement subject hisher device © 2016', 'action recognition human pose estimation closely related problems generally handled distinct tasks literature work multitask jointly 2d 3d pose estimation still images human action recognition video sequences show single architecture used solve two problems efficient way still achieves results additionally demonstrate optimization endtoend leads significantly higher accuracy separated learning architecture trained data different categories simultaneously seamlessly way reported results four datasets mpii human36m penn action ntu demonstrate effectiveness targeted tasks © 2018', 'human pose estimation process recognizing humans limb positions orientations video many important applications including surveillance diagnosis movement disorders computer animation deep learning lead great advances 2d 3d pose estimation single video sources problem estimating 3d human pose multiple video sensors overlapping fields view received less attention application allows multiple cameras 3d human pose estimates may greatly improved fusion multiview pose estimates observation limbs fully partially occluded views past approaches multiview 3d pose estimation used probabilistic graphical models reason constraints including perimage pose estimates temporal smoothness limb length paper present pipeline multiview 3d pose estimation multiple individuals combines stateofart 2d pose detector factor graph 3d limb constraints optimized belief propagation evaluate results tumcampus shelf datasets multiperson 3d pose estimation show system significantly outperforms previous simpler model limb dependency © 2018', 'human pose estimation heat map representation spite good performance representation issues nature nondifferentiable postprocessing quantization error work shows simple integral operation relates unifies heat map representation joint regression thus avoiding issues differentiable efficient compatible heat map effectiveness convincingly validated via comprehensive ablation experiments various settings specifically 3d pose estimation first time © springer nature switzerland ag 2018', 'paper addresses problem 3d human pose estimation body parts present input image ie body joints present joints fully absent exclude selfocclusion designed thus effective cases deep cnn regress human pose directly input image design train network work partial body presence parallel train detection network classify presence absence main body joints input image outputs detection regression networks joints present b joints absent outputs reconstructs full body skeleton evaluations human36m dataset yield promising results compared related work © 2018', 'paper1 mixture model combines joint appearancebased interjoint spatial relationshipbased models deep neural architecture called deep convolutional neural network dcnn applied tackle human pose estimation problem firstly construct graphical model human body secondly images decomposed several image patches used positive input samples finally dcnn network solve multiple classifications obtained perform human pose estimation quantitative results obtained experiments impressive show outperforms recent works used considered datasets © 2018 association computing machinery acm', 'human pose estimation refers estimation location body parts connected image human pose estimation monocular images wide applications eg image indexing several surveys human pose estimation found literature focus certain category example modelbased approaches human motion analysis etc far know overall review problem domain yet provided furthermore recent advancements deep learning brought novel algorithms problem paper comprehensive survey human pose estimation monocular images carried including milestone works recent advancements one standard pipeline solution computer vision problems survey splits problem several modules feature extraction description human body models modeling problem modeling approached two means categorization survey one way categorize includes topdown bottomup another way includes generative discriminative considering fact one direct application human pose estimation provide initialization automatic video surveillance additional sections motionrelated modules motion features motion models motionbased finally paper also collects 26 publicly available data sets validation provides error measurement frequently used © 2016 authors licensee mdpi basel switzerland', 'paper explore 3d human pose estimation rgbd images many researchers try directly predict 3d pose single rgb image simple could predict 3d pose predictions rgb image depth image two aspects one hand predicted accurate 2d joint locations rgb image applying stacked hourglass networks improved residual architecture hand view obtained 2d joint locations could estimate 3d pose depth calculating depth image patches general compared approaches model achieves signification improvement benchmark dataset copyright 2018 acm', 'paper introduces new architecture human pose estimation multilayer convolutional network architecture modified learning technique learns lowlevel features higherlevel weak spatial model unconstrained human pose estimation one hardest problems computer vision new architecture learning schema shows improvement current main contribution paper showing first time specific variation deep learning able meet performance many cases outperform existing traditional architectures task paper also discusses several lessons learned researching alternatives notably possible learn strong lowlevel feature detectors regions might cover pixels image higherlevel spatial models improve somewhat overall result much lesser extent expected many researchers previously argued kinematic structure topdown information crucial domain purely bottomup weak spatial model improve complicated architectures currently produce best results echos many researchers like speech recognition object recognition domains experienced 26 figure presented © 2014 international conference learning representations iclr rights reserved', 'paper novel recovering 3d human poses camera motions sequential images cnn lstm human pose estimation deep learning studied extensively recent years however existing aim classify 2d human motions images although recovering 3d human poses recently considered single frame poses sequential properties human actions used efficiently furthermore existing recover 3d poses relative viewpoints paper recovering 3d human poses 3d camera motions simultaneously sequential input images network cnn combined lstm network learn sequential properties 3d human poses camera motions efficiently efficiency evaluated real images well synthetic images © 2018 scitepress – science technology publications lda rights reserved', 'direct prediction 3d body pose shape parameters remains challenge even highly parameterized deep learning models representation prediction space difficult map plain 2d image space perspective ambiguities make loss function noisy training data scarce paper novel neural body fitting nbf integrates statistical body model layer within cnn leveraging reliable bottomup body part segmentation robust topdown body model constraints nbf fully differentiable trained endtoend 2d 3d annotations detailed experiments analyze components model improve model performance present robust easy endtoend trainable 3d human pose estimation single 2d images © 2018', 'paper presents efficient detect human pose monocular color imagery parallel architecture deep neural network network presented consists two sequentially connected stages 13 parallel cnn ensembles ensemble trained detect one specific kind linkage human skeleton structure detecting skeleton linkages voting scorebased postprocessing algorithm assembles individual linkages form complete human structure algorithm exploits human structural heuristics assembling skeleton links searches adjacent link pairs around expected common joint area structural heuristics presented heavily simplifies postprocessing computations furthermore parallel architecture presented network enables mutually independent computing nodes efficiently deployed parallel computing devices gpus computationally efficient training network trained tested coco 2017 personkeypoints dataset delivers pose estimation performance matching stateofart networks parallel ensembles architecture improves adaptability applications aimed identifying specific body parts saving computational resources copyright © 2018 asme', 'human body pose estimation hand detection two important tasks systems perform computer visionbased sign language recognitionslr however tasks challenging especially input color videos depth information many algorithms literature tasks successful recent algorithms deep learning paper introduce dataset human pose estimation slr domain evaluate performance two deep learning pose estimation performing userindependent experiments dataset also perform transfer learning obtain results demonstrate transfer learning improve pose estimation accuracy dataset results create useful baseline future works copyright 2016 held ownerauthors', 'many realworld applications require estimation human body joints higherlevel tasks example human behaviour understanding recent years depth sensors become popular obtain threedimensional information depth maps generated sensors provide information employed disambiguate poses observed twodimensional images work addresses problem 3d human pose estimation depth maps employing deep learning model named deep depth pose ddp receives depth map containing person set predefined 3d prototype poses returns 3d position body joints person particular ddp defined convnet computes specific weights needed linearly combine prototypes given input thoroughly evaluated ddp challenging ‘ itop ’ ‘ ubc3v ’ datasets respectively depict realistic synthetic samples defining new © 2018 elsevier inc', 'human activity recognition system detecting human estimating pose 2d 3d human correctly critical issue paper novel 3d human pose estimation rgbd cctv images deep learning rgbd model rather conventional rgb image limitation detecting object due lack topological information resolve selfocclusion problem improve object detection ratio efficiently subsequently position human joint localized convolutional neural network cnn detected person phase utilize cpm convolutional pose machine generate belief maps predict positions keypoint representing human body parts estimate 2d human pose detected keypoints final stage estimate 3d human pose 2d joint information deep neural network dnn experiment prove detects human objects robustly occlusion estimated 3d human pose accurate comparing previously introduced future work estimated 3d human pose used human activity recognition © 2018 association computing machinery', 'heterogeneous multitask learning human pose estimation monocular image deep convolutional neural network particular simultaneously learn posejoint regressor slidingwindow bodypart detector deep network architecture show including bodypart detection task helps regularize network directing converge good solution report competitive stateofart results several data sets also empirically show learned neurons middle layer network tuned localized body parts © 2014', 'paper presents capture human pose individual realworld rgb images deep learning technique current works estimating human pose deep learning designed detection regression partbased manner new perspective introduce classification scheme problem reasons pose holistically best knowledge first work holistic human pose classification task owes feasibility great power convolutional neural networks feature learning training convolutional neural network classify input image one keyposes final pose computed linear combination several keyposes new holistic classification attitude vast high degree freedom human pose space divided finite number subspaces convolutional neural network shows promising results learning features subspace empirical results pcp pck rates demonstrate scheme successfully able understand human pose ie predict valid true coarse pose realworld unconstrained images challenges like severe occlusion high articulation low quality cluttered background furthermore need defining complex model appearance model joints pairwise relations relieved also verified potential application semantic image retrieval human pose © 2018 springer sciencebusiness media llc part springer nature', '12 special focus conference articulated motion deformable objects topics optical recognition numerical characters digital images glucometers comparison text string similarity algorithms poi name harmonisation refining pose training deep recurrent autoencoders improving human pose estimation deep neural networks generated efficiently devices limited resources controlling smartphone braincomputer interfaces preliminary study capturing industrial machinery virtual reality leishmaniasis parasite segmentation classification deep learning robust pedestrian detection semiautomatic construction crowded person reidentification dataset shape appearance sequenced convnets detect realtime face attributes mobile devices image colorization generative adversarial networks', 'timed upandgo tug test widely accepted standard assessment measuring basic functional mobility patients parkinsons disease several basic mobility subtasks sit sittostand walk turn walkback sitback included tug test shown time costs subtasks useful clinical parameters assessment parkinsons disease several automatic segment time subtasks tug test however usually require either wellcontrolled environments tug video recording information special devices wearable inertial sensors ambient sensors depth cameras paper automatic tug subtask segmentation videobased activity classification validated study 24 parkinsons disease patients videos used paper recorded semicontrolled environments various backgrounds deep learningbase 2d human pose estimation technologies used feature extraction support vector machine long shortterm memory network used activity classification subtask segmentation used automatically acquire clinical parameters assessment parkinsons disease tug videosonly leading possibility remote monitoring patients condition © 2018', 'human pose estimation deep neural networks aims map input images large variations multiple body keypoints must satisfy set geometric constraints interdependence imposed human body model challenging nonlinear manifold learning process high dimensional feature space believe deep neural network inherently algebraic computation system efficient way capture highly sophisticated human knowledge example highly coupled geometric characteristics interdependence keypoints human poses work explore external knowledge effectively represented injected deep neural networks guide training process learned projections impose proper prior specifically stacked hourglass design inceptionresnet module construct fractal network regress human pose images heatmaps explicit graphical modeling encode external knowledge visual features able characterize constraints human body models evaluate fitness intermediate network output inject external features neural network projection matrix learned auxiliary cost function effectiveness inceptionresnet module benefit guided learning knowledge projection evaluated two widely used human pose estimation benchmarks achieves performance datasets © 19992012', 'paper provides predict 2d human pose image deep model resnet50 human pose estimation formulated regression problem towards body joints topdown first detect position humans holistic image take advantages multistages cascade resnet50 reason human body joints position challenging flic datasets large pose variation outperforms benchmarks © 2017 institution engineering technology rights reserved', 'deep learningbased models recently widely successful outperforming traditional approaches several computer vision applications image classification object recognition action recognition however models naturally designed learn structural information important tasks human pose estimation structured semantic interpretation video events paper demonstrate build structured semantic understanding audiovideo events reasoning multiplelabel decisions deep visual models auditory models grenanders structures imposing semantic consistency structured model require joint training structural semantic dependencies deep models instead independent components linked grenanders structures furthermore exploited grenanders structures means facilitate enrich model fusion multimodal sensory data particular auditory features visual features overall observed improvements quality semantic interpretations deep models auditory features combination grenanders structures reflecting numerical improvements 115 123 precision recall respectively © 2016', 'recent years human pose estimation greatly benefited deep learning huge gains performance achieved trend maximise accuracy benchmarks however resulted computationally expensive deep network architectures require expensive hardware pretraining large datasets makes difficult compare different reproduce existing results paper therefore efficient deep network architecture efficiently trained midrange gpus without need pretraining despite low computational requirements network par much complex models popular benchmarks human pose estimation © 2016 copyright document resides authors', 'learning articulated object pose inherently difficult pose high dimensional many structural constraints existing work model constraints guarantee geometric validity pose estimation therefore requiring postprocessing recover correct geometry desired cumbersome suboptimal work directly embed kinematic object model deep neutral network learning general articulated object pose estimation kinematic function defined appropriately parameterized object motion variables differentiable used gradient descent optimization network training prior knowledge object geometric model fully exploited structure guaranteed valid show convincing experiment results toy example 3d human pose estimation problem latter achieve result human36m dataset © springer international publishing switzerland 2016', 'human pose query modality alternative rich experience image video retrieval interesting retrieval applications domains sports dance databases work two novel ways representing image person striking pose one looking parts looking whole image representations used retrieval representations obtained deep learning first make following contributions introduce ‘ deep poselet posesensitive detection various body parts built convolutional neural network cnn features deep poselets significantly outperform previous instantiations berkeley poselets 6 b detector responses construct pose representation suitable pose search show pose retrieval performance par previous second make following contributions design optimized neural network maps input image low dimensional space similar poses close dissimilar poses farther away b show pose retrieval system low dimensional representation par deep poselet representation par previous previous works two compared bag visual words 44 berkeley poselets 6 human pose estimation algorithms 52 quantitatively evaluated large dataset images built number standard benchmarks together frames hollywood movies © 2016 elsevier bv', 'semantic segmentation currently mainstream addressing several remote sensing applications achieving recently remarkable performance employing deep learning techniques particular case pixelwise dense classification models high resolution remote sensing datasets paper exploit relatively deep architecture repetitive downscaleupscale processes previously employed human pose estimation tasks integrating model aiming capture extract lowlevel details small objects object boundaries edges experimental results quantitative evaluation performed publicly available isprs wgiii4 benchmark dataset indicating potential © 2018', 'human performance capture system employing convolutional neural networks cnn estimate human pose volumetric representation performer derived multiple viewpoint video mvv compare direct cnn pose regression performance affine invariant pose descriptor learned cnn classification task nonlinear manifold embedding learned descriptor articulated pose spaces enabling regression pose source mvv results evaluated ground truth pose data captured vicon markerbased system demonstrate good generalisation range human poses providing system requires special suit worn performer © 2016 acm', '61 special focus conference intelligence science big data engineering topics microphone array arrangement high order ambisonics recordings learning alternating bergman network nonconvex nonsmooth optimization problems sparse multimodal gaussian processes document analysis multiview intact space learning manifold regularization blind image quality assessment statistics color descriptors dct domain location dependent dirichlet processes frequency recognition optimized power spectral density analysis sssepbased bcis hybrid particle swarm optimization algorithm migration mechanism hybrid multiswarm pso algorithm shuffled frog leaping algorithm similarity degree multiattribute decision making incomplete dual hesitant fuzzy sets unified confidence measure auxiliary normalization graph image fusion pulse coupled neural network cnn cnnbased age classification via transfer learning deep attentive structured language model lstm classification motor imagery eeg signals deep learning models jointly deep model learned features traditional visual features stacked svm medical subfigure classification original face image virtual image face recognition novel representation abnormal crowd motion detection multitask learning person reidentification cascade errorcorrection mechanism human pose estimation videos index tracking sparse support vector regression online vehicle tracking aerial imagery robust variational autoencoder radar hrrp target recognition', 'spatial affordance defined functionality space place lends human activity different places afford different activity possibilities sleeping mostly done bedroom cooking mostly done kitchen semantic place labels like kitchen bedroom therefore provide context robot better infer human activity real rooms however often defy simple place labels multipurpose supporting many different types human activity solution identify spatial affordances associated current nexus human activity microlevel place labeling paper demonstrate estimate local spatial affordances integrating deep learning place estimator human pose estimation resulting affordances used improve activity recognition bayesian belief network © 2016', 'deep architectures convolution structure found highly effective commonly used computer vision introduction graphics processing unit gpu general purpose issues increasing attention towards exploiting gpu processing power deep learning algorithms also large amount data online made possible train deep neural networks efficiently aim paper perform systematic mapping study order investigate existing research implementations computer vision approaches deep learning algorithms convolutional neural networks cnn selected total 119 classified according field interest network type learning paradigm research contribution type study demonstrates field promising area research choose human pose estimation video frames possible computer vision task explore research careful studying three different research direction related improving existing cnn implementations recurrent neural networks rnns human pose estimation finally relying unsupervised learning paradigm train nns © 2017', 'paper focuses structuredoutput learning deep neural networks 3d human pose estimation monocular images network takes image 3d pose inputs outputs score value high imagepose pair matches low otherwise network structure consists convolutional neural network image feature extraction followed two subnetworks transforming image features pose joint embedding score function dotproduct image pose embeddings imagepose embedding score function jointly trained maximummargin cost function interpreted special form structured support vector machines joint feature space discriminatively learned deep neural networks also efficient recurrent neural network performing inference learned imageembedding test human36m dataset obtain results compared recent finally present visualizations imagepose embedding space demonstrating network learned highlevel embedding bodyorientation poseconfiguration © 2016 springer sciencebusiness media new york', 'drone systems deployed various law enforcement agencies monitor hostiles spy foreign drug cartels conduct border control operations etc paper introduces realtime drone surveillance system identify violent individuals public areas system first feature pyramid network detect humans aerial images image region human used scatternet hybrid deep learning shdl network human pose estimation orientations limbs estimated pose next used identify violent individuals deep network learn meaningful representations quickly scatternet structural priors relatively fewer labeled examples system detects violent individuals realtime processing drone images cloud research also introduces aerial violent individual dataset used training deep network hopefully may encourage researchers interested deep learning aerial surveillance pose estimation violent individuals identification performance compared techniques © 2018', 'human pose estimation important research topic computer vision community focus researchers mainly significant applications various important fields like human computer interaction action recognition surveillance picture understanding threat prediction etc difficult cover aspects domain diversity application areas therefore review focused significant contributions human pose estimation single twodimensional image start study traditional pictorial structure go discussion deep neural networks improved human pose estimation significantly recent famous namely stacked hourglass modern training evaluating comparing common datasets different architectures deep learning modules hence starting first practical models estimating human pose provide comprehensive study famous deep learning order provide concise analytical review influential © 2018', 'work novel efficient articulated human pose estimation videos convolutional network architecture incorporates color motion features new human body pose dataset flicmotion dataset downloaded httpcsnyuedu∼ajainaccv2014 extends flic dataset 1 additional motion features apply architecture dataset report significantly better performance current pose detection systems © springer international publishing switzerland 2015', 'object recognition human pose estimation scene recognition applications frequently solved decomposition collection parts resulting local representation significant advantages especially case occlusions subject nonrigid detection recognition require modelling appearance different object parts well spatial layout representation particularly successful body part estimation depth images integrating spatial layout parts may require minimization complex energy functions prohibitive real world applications therefore often omitted however ignoring spatial layout puts burden classifier whose available information local appearance new integrate spatial layout parts classification without costly pairwise terms testing spatial relationships exploited training algorithm testing competing classifies pixels independently makes realtime processing possible show training classifier spatial relationships increases generalization performance compared classical training minimizing classification error training set present application human body part estimation depth images © 2013 elsevier bv rights reserved', 'work new 3d human pose estimation single monocular rgb image deep convolutional neural network cnn depends reducing huge search space continuousvalued 3d human poses discretizing approximating continuous poses many discrete keyposes keyposes constitute restricted search space considered multipleclass candidates 3d human poses thus suitable classification technique trained set 3d keyposes corresponding rgb images build model predict 3d pose class input monocular rgb image deep cnn suitable classifier proven accurate technique rgb image classification proven achieve good accuracy comparable © springer nature switzerland ag 2018', 'recent approaches monocular 3d human pose estimation rely deep learning typically involve regressing image either 3d joint coordinates directly 2d joint locations 3d coordinates inferred approaches strengths weaknesses therefore novel architecture designed deliver best worlds performing simultaneously fusing information along way heart trainable fusion scheme learns fuse information optimally instead handdesigned yields significant improvements upon standard 3d human pose estimation benchmarks © 2017', 'deep learning algorithms subset machine learning algorithms aim discovering multiple levels distributed representations recently numerous deep learning algorithms solve traditional artificial intelligence problems work aims review deep learning algorithms computer vision highlighting contributions challenges 210 recent research first gives overview various deep learning approaches recent developments briefly describes applications diverse vision tasks image classification object detection image retrieval semantic segmentation human pose estimation finally paper summarizes future trends challenges designing training deep neural networks © 2015 elsevier bv', '75 special focus conference w04 brave new ideas motion representations w06 geometry meets deep learning w14 recovering 6d object pose w20 second international workshop video segmentation topics unsupervised learning optical flow via brightness constancy motion smoothness human action recognition without human motion representation acceleration images segmentation free object discovery video human pose estimation space time 3d cnn infer property dynamic object motion pattern temporal convolutional networks making case learning motion representations phase neural network library geometric computer vision learning covariant feature detectors scene segmentation driven deep learning surface fitting cnn cascade landmark guided semantic part segmentation overcoming occlusion inverse graphics deep kinematic pose regression learning structure objects web supervision deep volumetric shape learning without object labels deep shape low number silhouettes deep disentangled representations volumetric reconstruction monocular surface reconstruction 3d deformable part models deep bimodal regression apparent personality analysis rgbdepth database human head pose estimation audiovisual deep residual networks multimodal apparent personality trait recognition deep learning facial action unit detection large head poses first round challenge first impressions dataset results best practices finetuning visual classifiers new domains', 'paper provides predict 2d human pose image deep model resnet50 human pose estimation formulated regression problem towards body joints topdown first detect position humans holistic image take advantages multistages cascade resnet50 reason human body joints position challenging flic datasets large pose variation outperforms benchmarks © 2017', 'human pose estimation ie locating body partsjoints person fundamental problem humancomputer interaction multimedia applications significant progress made development depth sensors ie accessible human pose prediction still depth images 32 however existing approaches problem involve several componentsmodels independently designed optimized leading suboptimal performances paper novel inference embedded multitask learning predicting human pose still depth images implemented deep architecture neural networks specifically han dle two cascaded tasks generating heat confidence maps body parts via fully convolutional network fcn ii seeking optimal configuration body parts detected body part proposals via inference builtin matchnet 10 measures appearance geometric kinematic compatibility body parts embodies dynamic programming inference extra network layer two tasks jointly optimized extensive experiments show deep model significantly improves accuracy human pose estimation several sdks also release largescale dataset comparison includes 100k depth images challenging scenarios © 2016 acm', 'objective human pose estimation estimate locations keypoints human body single image convolutional pose machines one popular pose estimation techniques deep learning convolutional features paper semantic pose machines pose estimation technique enhances convolutional pose machines utilizing semantic segmentation heatmap addition convolutional features semantic segmentation leverage success object class recognition networks segmentation important object classes including people consider crf rnn semantic segmentation obtain heatmap incorporated pose estimation process additional channel results leeds dataset indicate improvements convolutional pose machines © 2018 society imaging science technology', 'human pose estimation refers estimation location body parts connected image human pose estimation monocular images wide applications eg image indexing several surveys human pose estimation found literature focus certain category example modelbased approaches human motion analysis etc far know overall review problem domain yet provided furthermore recent advancements deep learning brought novel algorithms problem paper comprehensive survey human pose estimation monocular images carried including milestone works recent advancements one standard pipeline solution computer vision problems survey splits problem several modules feature extraction description human body models modeling problem modeling approached two means categorization survey one way categorize includes topdown bottomup another way includes generative discriminative considering fact one direct application human pose estimation provide initialization automatic video surveillance additional sections motionrelated modules motion features motion models motionbased finally paper also collects 26 publicly available data sets validation provides error measurement frequently used', '2d multilevel appearance representation human body rgb images spatially modelled fullyconnected graphical model appearance model cnn body part detector shared features cascade architecture simultaneously detect body parts different levels granularity fullyconnected conditional random field crf spatial model approximate inference efficiently performed meanfield algorithm implemented recurrent neural network rnn stronger visual support body parts different levels granularity along fullyconnected pairwise spatial relations weights learnt model improve performance bottomup part detector adopt endtoend training strategy leverage potential appearance spatial models achieve competitive results mpii lsp datasets © 2018 r de bem arnab golodetz sapienza p torr', 'last years deep learning shown outperform previous machine learning techniques several fields computer vision one prominent cases review paper provides brief overview significant deep learning schemes used computer vision problems convolutional neural networks deep boltzmann machines deep belief networks stacked denoising autoencoders brief account history structure advantages limitations given followed description applications various computer vision tasks object detection face recognition action activity recognition human pose estimation finally brief overview given future directions designing deep learning schemes computer vision problems challenges involved therein © 2018 athanasios voulodimos et al', 'success deep learning vision attributed models high capacity b increased computational power c availability largescale labeled data since 2012 significant advances representation capabilities models computational capabilities gpus size biggest dataset surprisingly remained constant happen increase dataset size 10 × 100 ×', 'deep convolutional neural network 3dhuman pose camera estimation monocular imagesthat learns 2d joint annotations networkfollows typical architecture contains additionaloutput layer projects predicted 3d joints onto2d enforces constraints body part lengths 3dwe enforce pose constraints independentlytrained network learns prior distribution 3dposes evaluate several benchmarkdatasets compare approachesfor 3d human pose estimation achieving comparable performanceadditionally show significantlyoutperforms cases 3dground truth data unavailable network exhibitsgood generalization properties © 2016', 'study considers 3d human pose estimation problem single rgb image proposing conditional random field crf model 2d poses 3d pose obtained byproduct inference process unary term crf model defined powerful heatmap regression network 2d human pose estimation study also presents regression network lifting 2d pose 3d pose prior term consistency estimated 3d pose 2d pose obtain approximate solution crf model nbest strategy adopted inference algorithm viewed sequential processes bottomup generation 2d 3d pose proposals input 2d image deep networks topdown verification proposals checking consistencies evaluate two largescale datasets human36m humaneva experimental results show achieves 3d human pose estimation performance © 2018 elsevier inc']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport gensim\\nimport gensim.corpora as corpora\\nimport spacy\\n\\nfrom pprint import pprint\\nfrom gensim.utils import simple_preprocess\\nfrom gensim.models import CoherenceModel\\n\\ndef tokenizer(sentences):\\n    for sentence in sentences:\\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\\n\\ndocs_tokenized = list(tokenizer(docs))\\n\\nnlp = spacy.load(\"en_core_web_sm\", disable=[\\'parser\\', \\'ner\\'])\\ndef lemmatizer(texts, allowed_postags=[\\'NOUN\\', \\'ADJ\\', \\'VERB\\', \\'ADV\\']):\\n    \"\"\"https://spacy.io/api/annotation\"\"\"\\n    texts_out = []\\n    for sent in texts:\\n        doc = nlp(\" \".join(sent))\\n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\\n    return texts_out\\n\\ndocs_lemmatized = lemmatizer(docs_tokenized,\\n                             allowed_postags=[\\'NOUN\\', \\'ADJ\\', \\'VERB\\', \\'ADV\\'])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Generation"
      ],
      "metadata": {
        "id": "l0noUVfOJOu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "login(\"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\")\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LSGIw7r-vv4",
        "outputId": "41e1a00b-8369-4166-e67d-4875162263e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # 4-bit quantization\n",
        "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
        "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
        "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
        ")\n",
        "\n",
        "base_model = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "model_quantized = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "model_quantized.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757,
          "referenced_widgets": [
            "f294bd738484453fb537cb00ac510e72",
            "67dd25cc034a4975879149028193e11c",
            "465633bbf2324a5d970f2c7fa940bee2",
            "91d738bd27a84610bedcb4220cfc2c39",
            "74bcf17f48a94254b30f8aa972a6e6ec",
            "a2e931ce885d4315ba256a59cdf0f464",
            "015cd096bca946a3b792a096add1f03d",
            "d15bcde485a2455881c48877432fc76a",
            "4a7231d89e574202a6df6a7f3f4291ac",
            "489356534b53493db10a318df050b53c",
            "e86234a823e840e798010fe469576df6",
            "a715ffeb585a44c6864fdc4fec0045b2",
            "e5188f3241ec469081f2012c7cf9b582",
            "55d1d2abab48430da3c4f82210018efa",
            "c5bf9dbe2b8c456d906e6f41a8586f7a",
            "16e449cc53274f71a5d7a000f4c91251",
            "87c212cab8c34fe6907a3769b5dd4145",
            "4b2aa9827ed54589a260b7b5b0168c9c",
            "6ec14a953ec34890a358daf0ed7ade16",
            "118afbc89a2e412aa3d5fc284f3d5d1a",
            "79127b5da06f4bbca0d2a8073e6e9adb",
            "88196f6fc13645e3b07c7098fc761913",
            "a416ef6255f64c948bf457b8e60ac08a",
            "e4073980fbd64aca8e9250efc272a7f3",
            "61d12479688447ebb4d83707b0a894d7",
            "bcd8bf7bf1d54d46a77302fd422672a7",
            "43ad86cf5de24a0bb028ee96afdd736f",
            "48a8f522febb4b08acac1bcfa6935d46",
            "35ea83e4466849809651a71d1b80abc6",
            "5204ff0c68d44f649e283f5656f001ec",
            "cc9e66bc999e4a5ea9182997320bdca9",
            "f0a6b140741b46de853aa6a54e1c9c5b",
            "152193c644df48558270388a0e62f43a",
            "ab7347627882458eb785c025a7388aa6",
            "8bd2060b47154e81a101aaaac3b061fc",
            "e4f04be7f659491687469d88b7cd914c",
            "0015e36e3dac4e7b86c0bb247ccaf8b2",
            "fbcc7aa912684e79861404190b117b95",
            "ddd33afbe72d49e5a12e9d32b1d96e10",
            "f246d28f78d34b25989f97ac393b2b96",
            "f6791e4f6a3e4d28830ad20ab3d5c471",
            "9aac0c18000c43f4a0ccdcc80e7d2135",
            "a6176f8cad894310b4274cf244903b48",
            "055d2091df0c4182a70a96ee955b2fae",
            "3ac99a9964e74811b292f9bc178e7a25",
            "0bd453d1e49344ca8adae2a8cabece50",
            "13f9846948d442379d1e44bbf7029cc6",
            "6b780c039b7e418a8b921aaf5b83e9a8",
            "28bce5e80f8443da8f5b098cecedfb42",
            "52082f29352d480eae7b94572c240f5f",
            "6528fcb559a34061aec6ef297bdd5553",
            "29f2805c247a47229d986cb8083da39b",
            "4b1d68f3cfd8422e83164e11861fca5d",
            "f50a7d7f2d384d559330aa3d7c9e2828",
            "fe3eec1e9d304f0ca2d9abefe6caf817",
            "3da364cb4c574d2dbba73d764a42ca4e",
            "a509e88b975642be867db9760840ffcb",
            "fed94cab700945919839540e90328c29",
            "034bd0dfbf064638baf302d59a32273b",
            "e2dadcc35e2d4f38b6302e74544f0f2e",
            "7ca7a994680d4a4a9f248659ca847c9b",
            "6e353ea809bf4152bb8596997fde9498",
            "b31f6e912553474497153c485ab5dfae",
            "23712799dd3347e3992f59be05f1017f",
            "7a1db28658e149f38242f06fcfb88055",
            "9434e6af13f44407ba94c4ad23061016",
            "20a646fa32794cc5b30343c2224c5755",
            "99d1cc97d2b241ee8b1650543823d685",
            "722f89946e5d4d3ebc92ca74cdbc79b8",
            "f54fac74c56f420a97c51fcd05d3ee75",
            "f34c1ecf4c3b484fbd84bbbdf6f20407",
            "faec76a91f5f49c1bea16094c6b6f405",
            "8e1d2e7f07684426af9bf6f7230de1c3",
            "4a9727c452184400b66552a9eed5fefc",
            "f3cc29a367fe4ef5bc7c1fda01437763",
            "cce9b1eb54d84f6289e29414bbf0df5b",
            "0a1ce4f54f9c4f44875d2c1af043b0da",
            "c3486f7a17a3463a9d422715f8f92781",
            "d02bb307d22745fb9d0f98a4abdf57c0",
            "c87d68446e8b41538e29dcd62cbd2d1e",
            "ed9b5cb688754f33b6c245e966aea3b5",
            "a13bb33011434dd0aeba17fe429b3e45",
            "74d8bcd1f9e241f3b43e798b0ebaa3c9",
            "c1c0dadaaaf24749abc56ea3fb511bc3",
            "59f1820055ab4062be0f90e7da3688d9",
            "bb9856fcc8a24c83908f5721460ab04f",
            "f05b530228b14252a1eb25a70193afec",
            "83fa95e8d5b04271a71e3b8d80873066"
          ]
        },
        "id": "nAGGWhs9_bgv",
        "outputId": "4448a9a4-0a7e-46c0-f977-e5d4e2662ddd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f294bd738484453fb537cb00ac510e72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a715ffeb585a44c6864fdc4fec0045b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a416ef6255f64c948bf457b8e60ac08a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab7347627882458eb785c025a7388aa6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ac99a9964e74811b292f9bc178e7a25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3da364cb4c574d2dbba73d764a42ca4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20a646fa32794cc5b30343c2224c5755"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3486f7a17a3463a9d422715f8f92781"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 5120)\n",
              "    (layers): ModuleList(\n",
              "      (0-39): 40 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "generator = pipeline(\n",
        "    model=model_quantized, tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.5\n",
        ")"
      ],
      "metadata": {
        "id": "DcvakuqG_ish",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "339aac4288da47159dbbcf312b03357d",
            "55ca4d8a3774468098c501881ea07914",
            "acdd156eebb44a6d968b16a7f64518ea",
            "6c9f1d6dc318405c80a94c0c1b2bf683",
            "57ca34c742fb4d0c88342e922a526b91",
            "0bfa86bc06c0403b90efb81766567ce0",
            "78250eb9e2bd45e9984ca122805bf4d5",
            "b53f0b36baa243f1adf8bb16ee68ff67",
            "fc7e639d24464a7f9d9ee831cbc25fc9",
            "92399b2945a649aeaf7ae6a6b590abc9",
            "1bb0bd874a454f7fab8cc334d2d807d9",
            "c013e2a33cc94e1894715fef05a953ba",
            "62e00e64c614409ab449003251a0904c",
            "7a01ea2e450246ee973b097eefd8546b",
            "6414beac3aaf421f81a8be47b2a6c608",
            "7e63a3eba14f47d6bb119f4ead53c7c7",
            "b2593e1cdae84d6c9bf83ddaeed77d8d",
            "898f1025a4ab4b93a1ea6dab5532b02a",
            "0c8695540dd24d66b3af6ecff41477d3",
            "3140ef25eda04d788ced30cf851d011c",
            "c9e2c1345751463a82b53728121f01bf",
            "521fdd2ee7e74e27a297d847c37f1769",
            "5fa56f932dc94bcc81967caa8d0ec40c",
            "b3daf19111ec46e7a8af9b660fdffa21",
            "f5ae5a7e806c499194e479eb09455ec8",
            "15b61c2d40e848f8b47539065b954cf5",
            "1c1d71af9a054f83855a91dc101ca29a",
            "56515666a041471bb52f3a4eb1602e10",
            "d455aba3a3ef442d9192bcda9cca3b34",
            "58ff0a319b6b4666b59b7795ba859557",
            "be83b9be1ea84671a8fe24b9f37278dd",
            "ec7cd655b16543498a7571b3aa6f2977",
            "80d9eef06fd2462c995c218fa8147476",
            "dfe998a36a8e43a09f67e75ce838dd22",
            "03f3482789b54b2989ab6904ed8b9c81",
            "51cf6ca9a64047629c4fbae6906ac69a",
            "c382cf0dde3e4678a3ce2fc88656f2e4",
            "937e64a9473d4db7b821c3a60d1f31c9",
            "dce4e0b8499b4a5bb9cfe3497c987ead",
            "d9e134f2968b4ff8b329e3a1d2948637",
            "8e48ff321c5b41509efd14c74a83e542",
            "566d83e3218543e8884667ed5f6eca0e",
            "478781c13876476b991e86e41c349ac1",
            "e6b071f179ac431aa8fbbdb7fb19af22"
          ]
        },
        "outputId": "af5f0285-11c2-48cf-c9d3-7517044b7adf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "339aac4288da47159dbbcf312b03357d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c013e2a33cc94e1894715fef05a953ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fa56f932dc94bcc81967caa8d0ec40c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfe998a36a8e43a09f67e75ce838dd22"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant for labeling topics.\n",
        "<</SYS>>\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = \"\"\"\n",
        "I have a topic that contains the following documents:\n",
        "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
        "- Meat, but especially beef, is the word food in terms of emissions.\n",
        "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
        "\n",
        "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
        "\n",
        "[/INST] Environmental impacts of eating meat\n",
        "\"\"\"\n",
        "\n",
        "main_prompt = \"\"\"\n",
        "[INST]\n",
        "I have a topic that contains the following documents:\n",
        "[DOCUMENTS]\n",
        "\n",
        "The topic is described by the following keywords: '[KEYWORDS]'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = system_prompt + example_prompt + main_prompt"
      ],
      "metadata": {
        "id": "jsuIfr4m_jWJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
        "from bertopic import BERTopic\n",
        "\n",
        "embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
        "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
        "# reduced_embeddings = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "representation_model = {\n",
        "    \"KeyBERT\": KeyBERTInspired(top_n_words=10),\n",
        "    \"MMR\": MaximalMarginalRelevance(diversity=0.1,top_n_words=10),\n",
        "    \"Llama2\": TextGeneration(model=generator, prompt=prompt)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a1c2ce4d5d0841378bb41ab111aa4432",
            "d7436e360b59418fae8906ce24296494",
            "6ebe4a24a7a24dfda044691eb5c7a2b0",
            "10ecbea4515248b6a0bb9ee249a39378",
            "f1417bc4a2a64dec991fbec280e9f6c7",
            "f0df6f288aae4bd8aa09626a3aab6f9e",
            "3c5c41fd77ac42d687ddd99fdeb05712",
            "151816d6dc9a4829bbd9d99227c45a89",
            "c4f93717b4fe459e874b4c915470a22c",
            "6efbf3267b7f46cebf92a14d95de951c",
            "15f22f8fdb8046839d8a06cc4c0baa85"
          ]
        },
        "id": "cPJUNiSlAAk-",
        "outputId": "626005df-df23-4a4c-b207-066b639978ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1c2ce4d5d0841378bb41ab111aa4432"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "  embedding_model=embedding_model,\n",
        "  umap_model=umap_model,\n",
        "  hdbscan_model=hdbscan_model,\n",
        "  representation_model=representation_model,\n",
        "  top_n_words=10,\n",
        "  calculate_probabilities=True,\n",
        "  min_topic_size=2,\n",
        "  verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(docs, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISEaqWI4UNkC",
        "outputId": "c393a141-3bdc-4c2a-c9de-db19a829c449"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-02-24 13:05:31,844 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-02-24 13:05:40,227 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-24 13:05:40,229 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-02-24 13:05:40,300 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-24 13:05:40,311 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "100%|██████████| 6/6 [01:35<00:00, 15.94s/it]\n",
            "2024-02-24 13:07:20,266 - BERTopic - Representation - Completed ✓\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualisation"
      ],
      "metadata": {
        "id": "ZS2yPpL8Ra8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_info = topic_model.get_topic_info()\n",
        "topic_info.to_csv(\"/content/topic_info.csv\")\n",
        "topic_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "3GIxt5tiOexQ",
        "outputId": "e350fe91-baad-4757-974a-5f35db8f0edd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Topic  Count                              Name  \\\n",
              "0     -1     11        -1_yoga_system_pose_events   \n",
              "1      0    588        0_pose_human_estimation_3d   \n",
              "2      1     36    1_learning_deep_detection_data   \n",
              "3      2     18          2_signals_rf_human_radar   \n",
              "4      3     13    3_fall_falls_elderly_detection   \n",
              "5      4     12  4_driver_pose_estimation_driving   \n",
              "\n",
              "                                      Representation  \\\n",
              "0  [yoga, system, pose, events, feedback, emotion...   \n",
              "1  [pose, human, estimation, 3d, deep, model, net...   \n",
              "2  [learning, deep, detection, data, analysis, to...   \n",
              "3  [signals, rf, human, radar, pose, 3d, estimati...   \n",
              "4  [fall, falls, elderly, detection, falling, hum...   \n",
              "5  [driver, pose, estimation, driving, vehicle, d...   \n",
              "\n",
              "                                             KeyBERT  \\\n",
              "0  [yoga, postures, poses, asanas, pose, posture,...   \n",
              "1  [pose, poses, convolutional, human, 3d, datase...   \n",
              "2  [convolutional, ai, segmentation, technologies...   \n",
              "3  [rf, rfbased, rfpose, multiperson, radar, rfid...   \n",
              "4  [falling, falls, elderly, human, fall, skeleto...   \n",
              "5  [driving, driver, distracted, distraction, det...   \n",
              "\n",
              "                                                 MMR  \\\n",
              "0  [yoga, system, pose, events, feedback, emotion...   \n",
              "1  [pose, human, estimation, 3d, deep, model, net...   \n",
              "2  [learning, deep, detection, data, analysis, to...   \n",
              "3  [signals, rf, human, radar, pose, 3d, estimati...   \n",
              "4  [fall, falls, elderly, detection, falling, hum...   \n",
              "5  [driver, pose, estimation, driving, vehicle, d...   \n",
              "\n",
              "                                              Llama2  \\\n",
              "0     [Yoga Pose Detection System, , , , , , , , , ]   \n",
              "1  [Human Pose Estimation using Deep Learning, , ...   \n",
              "2  [Artificial Intelligence Applications, , , , ,...   \n",
              "3  [\"RF Human Pose Estimation Systems\", , , , , ,...   \n",
              "4  [Fall Detection and Recognition in Elderly Per...   \n",
              "5  [Autonomous Vehicle Monitoring using Pose Esti...   \n",
              "\n",
              "                                 Representative_Docs  \n",
              "0  [yoga change way see things transforms person ...  \n",
              "1  [rise deep learning technology broadly promote...  \n",
              "2  [180 special focus conference future technolog...  \n",
              "3  [advanced human sensing technologies radio fre...  \n",
              "4  [increase age elderly often fall seriously thr...  \n",
              "5  [number traffic accidents increased steadily r...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7788bd0a-51e4-4286-b7a2-8b0047fb1cc9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>KeyBERT</th>\n",
              "      <th>MMR</th>\n",
              "      <th>Llama2</th>\n",
              "      <th>Representative_Docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>11</td>\n",
              "      <td>-1_yoga_system_pose_events</td>\n",
              "      <td>[yoga, system, pose, events, feedback, emotion...</td>\n",
              "      <td>[yoga, postures, poses, asanas, pose, posture,...</td>\n",
              "      <td>[yoga, system, pose, events, feedback, emotion...</td>\n",
              "      <td>[Yoga Pose Detection System, , , , , , , , , ]</td>\n",
              "      <td>[yoga change way see things transforms person ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>588</td>\n",
              "      <td>0_pose_human_estimation_3d</td>\n",
              "      <td>[pose, human, estimation, 3d, deep, model, net...</td>\n",
              "      <td>[pose, poses, convolutional, human, 3d, datase...</td>\n",
              "      <td>[pose, human, estimation, 3d, deep, model, net...</td>\n",
              "      <td>[Human Pose Estimation using Deep Learning, , ...</td>\n",
              "      <td>[rise deep learning technology broadly promote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>1_learning_deep_detection_data</td>\n",
              "      <td>[learning, deep, detection, data, analysis, to...</td>\n",
              "      <td>[convolutional, ai, segmentation, technologies...</td>\n",
              "      <td>[learning, deep, detection, data, analysis, to...</td>\n",
              "      <td>[Artificial Intelligence Applications, , , , ,...</td>\n",
              "      <td>[180 special focus conference future technolog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>2_signals_rf_human_radar</td>\n",
              "      <td>[signals, rf, human, radar, pose, 3d, estimati...</td>\n",
              "      <td>[rf, rfbased, rfpose, multiperson, radar, rfid...</td>\n",
              "      <td>[signals, rf, human, radar, pose, 3d, estimati...</td>\n",
              "      <td>[\"RF Human Pose Estimation Systems\", , , , , ,...</td>\n",
              "      <td>[advanced human sensing technologies radio fre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>3_fall_falls_elderly_detection</td>\n",
              "      <td>[fall, falls, elderly, detection, falling, hum...</td>\n",
              "      <td>[falling, falls, elderly, human, fall, skeleto...</td>\n",
              "      <td>[fall, falls, elderly, detection, falling, hum...</td>\n",
              "      <td>[Fall Detection and Recognition in Elderly Per...</td>\n",
              "      <td>[increase age elderly often fall seriously thr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>4_driver_pose_estimation_driving</td>\n",
              "      <td>[driver, pose, estimation, driving, vehicle, d...</td>\n",
              "      <td>[driving, driver, distracted, distraction, det...</td>\n",
              "      <td>[driver, pose, estimation, driving, vehicle, d...</td>\n",
              "      <td>[Autonomous Vehicle Monitoring using Pose Esti...</td>\n",
              "      <td>[number traffic accidents increased steadily r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7788bd0a-51e4-4286-b7a2-8b0047fb1cc9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7788bd0a-51e4-4286-b7a2-8b0047fb1cc9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7788bd0a-51e4-4286-b7a2-8b0047fb1cc9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6aba700f-957e-48b5-9e5e-f4a17c9252ad\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6aba700f-957e-48b5-9e5e-f4a17c9252ad')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6aba700f-957e-48b5-9e5e-f4a17c9252ad button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "topic_info",
              "summary": "{\n  \"name\": \"topic_info\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": -1,\n        \"max\": 4,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          -1,\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 232,\n        \"min\": 11,\n        \"max\": 588,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          11,\n          588,\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"-1_yoga_system_pose_events\",\n          \"0_pose_human_estimation_3d\",\n          \"4_driver_pose_estimation_driving\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"KeyBERT\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MMR\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Llama2\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(topics))\n",
        "print(probs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHweDwQKVXrg",
        "outputId": "f4157652-0c31-4f52-8624-fab7df7b4d05"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "678\n",
            "(678, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_topic = pd.DataFrame({\"Title\": title_data, \"Abstract\": docs, \"Topic\": topics, \"Probability\": probs.tolist()})\n",
        "docs_topic.to_csv(\"/content/docs_topic.csv\")\n",
        "docs_topic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Lr2iYXw1_V6K",
        "outputId": "49362d03-85a0-4096-f22b-86e7995f544b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Title  \\\n",
              "0    Lightweight 2D Human Pose Estimation Based on ...   \n",
              "1    Human Pose Estimation Using MediaPipe Pose and...   \n",
              "2    Design Space Exploration on Efficient and Accu...   \n",
              "3    JoyPose: Jointly learning evolutionary data au...   \n",
              "4    DUA: A Domain-Unified Approach for Cross-Datas...   \n",
              "..                                                 ...   \n",
              "673  Deep Fully-Connected Part-Based Models for Hum...   \n",
              "674  Deep Learning for Computer Vision: A Brief Review   \n",
              "675  Revisiting Unreasonable Effectiveness of Data ...   \n",
              "676  3D human pose estimation via deep learning fro...   \n",
              "677  2D–3D pose consistency-based conditional rando...   \n",
              "\n",
              "                                              Abstract  Topic  \\\n",
              "0    traditional human pose estimation typically re...      0   \n",
              "1    seniors live alone home risk falling injuring ...      3   \n",
              "2    human pose estimation hpe assess human motion ...      0   \n",
              "3    videobased 3d human pose estimation important ...      0   \n",
              "4    human pose estimation important computer visio...      0   \n",
              "..                                                 ...    ...   \n",
              "673  2d multilevel appearance representation human ...      0   \n",
              "674  last years deep learning shown outperform prev...      0   \n",
              "675  success deep learning vision attributed models...      0   \n",
              "676  deep convolutional neural network 3dhuman pose...      0   \n",
              "677  study considers 3d human pose estimation probl...      0   \n",
              "\n",
              "                                           Probability  \n",
              "0    [1.0, 4.23820377351879e-309, 5.72102923067147e...  \n",
              "1    [1.4599027479160164e-308, 3.91561320826836e-30...  \n",
              "2    [1.0, 4.644404132682087e-309, 5.20329303599486...  \n",
              "3    [0.7980529524108124, 0.04012789285407543, 0.03...  \n",
              "4    [0.5533676013138721, 0.0630834821922568, 0.065...  \n",
              "..                                                 ...  \n",
              "673  [0.8424686549701329, 0.025045081005808857, 0.0...  \n",
              "674  [0.3581820455794816, 0.1159108561599556, 0.137...  \n",
              "675  [0.47837174262304527, 0.09456999990074651, 0.1...  \n",
              "676  [1.0, 3.78634087847647e-309, 3.618224716217967...  \n",
              "677  [1.0, 3.97912572051024e-309, 3.66116459853082e...  \n",
              "\n",
              "[678 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f24604ec-bf71-4ed4-9423-601c7d333300\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lightweight 2D Human Pose Estimation Based on ...</td>\n",
              "      <td>traditional human pose estimation typically re...</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.0, 4.23820377351879e-309, 5.72102923067147e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Human Pose Estimation Using MediaPipe Pose and...</td>\n",
              "      <td>seniors live alone home risk falling injuring ...</td>\n",
              "      <td>3</td>\n",
              "      <td>[1.4599027479160164e-308, 3.91561320826836e-30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Design Space Exploration on Efficient and Accu...</td>\n",
              "      <td>human pose estimation hpe assess human motion ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.0, 4.644404132682087e-309, 5.20329303599486...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JoyPose: Jointly learning evolutionary data au...</td>\n",
              "      <td>videobased 3d human pose estimation important ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.7980529524108124, 0.04012789285407543, 0.03...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DUA: A Domain-Unified Approach for Cross-Datas...</td>\n",
              "      <td>human pose estimation important computer visio...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.5533676013138721, 0.0630834821922568, 0.065...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>Deep Fully-Connected Part-Based Models for Hum...</td>\n",
              "      <td>2d multilevel appearance representation human ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.8424686549701329, 0.025045081005808857, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>Deep Learning for Computer Vision: A Brief Review</td>\n",
              "      <td>last years deep learning shown outperform prev...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.3581820455794816, 0.1159108561599556, 0.137...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>Revisiting Unreasonable Effectiveness of Data ...</td>\n",
              "      <td>success deep learning vision attributed models...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.47837174262304527, 0.09456999990074651, 0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>676</th>\n",
              "      <td>3D human pose estimation via deep learning fro...</td>\n",
              "      <td>deep convolutional neural network 3dhuman pose...</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.0, 3.78634087847647e-309, 3.618224716217967...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>677</th>\n",
              "      <td>2D–3D pose consistency-based conditional rando...</td>\n",
              "      <td>study considers 3d human pose estimation probl...</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.0, 3.97912572051024e-309, 3.66116459853082e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>678 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f24604ec-bf71-4ed4-9423-601c7d333300')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f24604ec-bf71-4ed4-9423-601c7d333300 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f24604ec-bf71-4ed4-9423-601c7d333300');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-53556d94-b3fb-4f1d-a9b6-519b3486f250\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53556d94-b3fb-4f1d-a9b6-519b3486f250')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-53556d94-b3fb-4f1d-a9b6-519b3486f250 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "docs_topic",
              "summary": "{\n  \"name\": \"docs_topic\",\n  \"rows\": 678,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 676,\n        \"samples\": [\n          \"3D human pose estimation from depth maps using a deep combination of poses\",\n          \"In-cabin vehicle synthetic data to test deep learning based human pose estimation models\",\n          \"State-consistency loss for learning spatial perception tasks from partial labels\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 676,\n        \"samples\": [\n          \"many realworld applications require estimation human body joints higherlevel tasks example human behaviour understanding recent years depth sensors become popular obtain threedimensional information depth maps generated sensors provide information employed disambiguate poses observed twodimensional images work addresses problem 3d human pose estimation depth maps employing deep learning model named deep depth pose ddp receives depth map containing person set predefined 3d prototype poses returns 3d position body joints person particular ddp defined convnet computes specific weights needed linearly combine prototypes given input thoroughly evaluated ddp challenging \\u2018 itop \\u2019 \\u2018 ubc3v \\u2019 datasets respectively depict realistic synthetic samples defining new \\u00a9 2018 elsevier inc\",\n          \"vehicle incabin monitoring increasing fulfil specifications european safety regulations regulations present several requirements detecting driver distraction complex requirements soon expected higher automation levels todays restraint systems provide optimal protection standard frontal seat positions deviations might cause severe airbaginduced injuries makes incabin monitoring critical improve safety mitigate dangerous situations case crash especially high levels autonomous driving defining best sensor positioning inside vehicles cabin challenge due constraints limitations main aim work verify simulated 3d human models integrated 3d modelled vehicle interior environment used run deep learning human pose estimation models perform task utilized software makehuman combined blender build virtual environment create photorealistic scenes containing selected front occupants postures cases feed openpose mask rcnn models results showed 2d hpe human pose estimation network pretrained real data detect successfully photorealistic synthetic data humans complex scenarios also shown complex rare postures cause failure 2d hpe detections shown literature review work helps define suitable camera positions combination specific camera lenses deliver quality images robust pose detection \\u00a9 2021\",\n          \"learning models realworld robot spatial perception tasks one might access partial labels occurs example semisupervised scenarios labels available subset training instances types selfsupervised robot learning robot autonomously acquires labeled training set acquires labels subset output variables instance introduce general deal class problems auxiliary loss enforcing expectation perceived environment state abruptly change instantiate solve two robot perception problems simulated ground robot learning longrange obstacle mapping 400binarylabel classification task selfsupervised way static environment real nanoquadrotor learning human pose estimation 3variable regression task semisupervised way dynamic environment cases yields significant quantitative performance improvements average increase 6 auc percentage points former relative improvement r2 metric ranging 7 33 latter baselines \\u00a9 2016\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": 4,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llama2_labels = [label[0][0].split(\"\\n\")[0] for label in topic_model.get_topics(full=True)[\"Llama2\"].values()]\n",
        "topic_model.set_topic_labels(llama2_labels)\n",
        "topic_model.visualize_documents(title_data, embeddings=embeddings, reduced_embeddings=None,\n",
        "                                hide_annotations=False, hide_document_hover=False, custom_labels=True,\n",
        "                                title = \"\",topics=topics,width=1920,height=1080)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U4OgArOZO7Sv",
        "outputId": "67077e2d-fee9-4576-ccab-290bd034a438"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"14c2187d-6c87-4b8d-875a-3e50c7cbeb0a\" class=\"plotly-graph-div\" style=\"height:1080px; width:1920px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"14c2187d-6c87-4b8d-875a-3e50c7cbeb0a\")) {                    Plotly.newPlot(                        \"14c2187d-6c87-4b8d-875a-3e50c7cbeb0a\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[\"Smart Yoga Assistant: SVM-based Real-time Pose Detection and Correction System\",\"MoEmo Vision Transformer: Integrating Cross-Attention and Movement Vectors in 3D Pose Estimation for HRI Emotion Detection\",\"EventGAN: Leveraging Large Scale Image Datasets for Event Cameras\",\"An automatic tool for yoga pose grading using skeleton representation\",\"YoPose: Yoga Posture Recognition Using Deep Pose Estimation\",\"Deep learning of mobile service robots\",\"3D Keypoint Detection of Lying Human Body Using an RGB-D Camera\",\"Implementation of a Quality Evaluation System for Chest Compression based on OpenPose Model\",\"YOG-GURU: REAL-TIME YOGA POSE CORRECTION SYSTEM USING DEEP LEARNING METHODS\",\"A Real-Time Virtual Yoga Assistant Using Machine Learning\",\"Development of an AI Enabled Yoga Posture (Aasans) Prediction System Using Deep Neural Network Model\",null],\"marker\":{\"color\":\"#CFD8DC\",\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"other\",\"showlegend\":false,\"x\":[11.960779190063477,8.500235557556152,11.336835861206055,11.960835456848145,11.961588859558105,8.449962615966797,11.355125427246094,11.412080764770508,11.960926055908203,11.960909843444824,11.960892677307129,11.1654691696167],\"y\":[8.065747261047363,14.462918281555176,14.666638374328613,8.065380096435547,8.059934616088867,14.450835227966309,15.864891052246094,15.790445327758789,8.064908027648926,8.064881324768066,8.06529426574707,11.238351821899414],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Hand segmentation with structured convolutional learning\",\"A Design of Learning based Human Positioning Algorithm using Images\",\"Multimodal 3D Human Pose Estimation from a Single Image\",\"Fighting Deepfakes Using Body Language Analysis\",\"Marker-less Motion Capture Technology Based on Binocular Stereo Vision and Deep Learning\",\"Light-Weight High-Performance HRNet for Human Pose Estimation\",\"A mini-survey and feasibility study of deep-learning-based human activity recognition from slight feature signals obtained using privacy-aware environmental sensors\",\"3D Human Pose Estimation Based on Volumetric Joint Coordinates; [\\u57fa\\u4e8e\\u4f53\\u7d20\\u8054\\u5408\\u5750\\u6807\\u7684\\u5355\\u4eba\\u4e09\\u7ef4\\u59ff\\u6001\\u4f30\\u8ba1]\",\"2D\\u002f3D Pose Estimation and Action Recognition Using Multitask Deep Learning\",\"Human Pose Estimation Using Thermal Images\",\"3D Human Pose Estimation from Deep Multi-View 2D Pose\",\"Automatic spacing inspection of rebar spacers on reinforcement skeletons using vision-based deep learning and computational geometry\",\"A Novel Joint Points and Silhouette-Based Method to Estimate 3D Human Pose and Shape\",\"Generalizable Human Pose Triangulation\",\"A deep learning and computer vision based multi-player tracker for squash\",\"Structural topic model-based comparative review of human pose estimation research in the United States and China\",\"CHANNEL-POSITION SELF-ATTENTION WITH QUERY REFINEMENT SKELETON GRAPH NEURAL NETWORK IN HUMAN POSE ESTIMATION\",\"Video-based Person Re-identification Method Based on GAN and Pose Estimation; [\\u878d\\u5408\\u751f\\u6210\\u5bf9\\u6297\\u7f51\\u7edc\\u548c\\u59ff\\u6001\\u4f30\\u8ba1\\u7684\\u89c6\\u9891\\u884c\\u4eba\\u518d\\u8bc6\\u522b\\u65b9\\u6cd5]\",\"DHP19: Dynamic vision sensor 3D human pose dataset\",\"Human pose, hand and mesh estimation using deep learning: a survey\",\"CE-HigherHRNet: Enhancing Channel Information for Small Persons Bottom-Up Human Pose Estimation\",\"Intelligent garment detection using deep learning\",\"JoyPose: Jointly learning evolutionary data augmentation and anatomy-aware global\\u2013local representation for 3D human pose estimation\",\"Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network\",\"Towards Scalable scenarios Human Pose Estimation via two-stage hierarchical network\",\"Feasibility of 3D Body Tracking from Monocular 2D Video Feeds in Musculoskeletal Telerehabilitation\",\"Comparison of video-based algorithms for 2D human kinematics estimation: a preliminary study\",\"Deep Fully-Connected Part-Based Models for Human Pose Estimation\",\"DEEP ACTIVE LEARNING FOR HUMAN POSE ESTIMATION VIA CONSISTENCY WEIGHTED CORE-SET APPROACH\",\"Position constrained network for 3D human pose estimation\",\"Top-down human pose estimation with depth images and domain adaptation\",\"HEMlets PoSh: Learning Part-Centric Heatmap Triplets for 3D Human Pose and Shape Estimation\",\"Deep Learning for Computer Vision: A Brief Review\",\"Neural keypoint detection for visual gestures on micro-controllers\",\"L-HRNet: A Lightweight High-Resolution Network for Human Pose Estimation\",\"Single-shot 3D multi-person pose estimation in complex images\",\"Attention-based 3D human pose sequence refinement network\",\"Video-Based Pose Estimation for Gait Analysis in Stroke Survivors during Clinical Assessments: A Proof-of-Concept Study\",\"IGE-net: Inverse graphics energy networks for human pose estimation and single-view reconstruction\",\"3D human pose estimation from RGB+D images with convolutional neural networks\",\"GolfPose: Golf Swing Analyses with a Monocular Camera Based Human Pose Estimation\",\"Least-Squares Estimation of Keypoint Coordinate for Human Pose Estimation\",\"Human Pose Estimation: Current Trend and Related Improvements\",\"Human Pose Estimation with Combined Feature Maps and Joint Embeddings\",\"CCCNet: An Attention Based Deep Learning Framework for Categorized Counting of Crowd in Different Body States\",\"Learning markerless human pose estimation from multiple viewpoint video\",\"CROMOSim: A Deep Learning-Based Cross-Modality Inertial Measurement Simulator\",\"Occlusion-aware networks for 3D human pose estimation in video\",\"Improving Multiperson Pose Estimation by Mask-aware Deep Reinforcement Learning\",\"Multi-Task Deep Learning for Real-Time 3D Human Pose Estimation and Action Recognition\",\"Methods of Separable RGB-D 3D Human Pose Estimation for Different Scenes\",\"Human pose estimation from monocular images: A comprehensive survey\",\"A Survey on Deep Learning-Based 2D Human Pose Estimation Models\",\"Trajectory space factorization for deep video-based 3D human pose estimation\",\"Recent Advances in 3D Human Pose Estimation: From Optimization to Implementation and beyond\",\"Hierarchical pose classification for infant action analysis and mental development assessment\",\"3D Human Pose Estimation in\\u00a0Video for\\u00a0Human-Computer\\u002fRobot Interaction\",\"Global joint information extraction convolution neural network for Parkinson's disease diagnosis\",\"Lightweight 3d human pose estimation network training using teacher-student learning\",\"Estimation of Human Posture Using Convolutional Neural Network Using Web Architecture\",\"Camera- and Viewpoint-Agnostic Evaluation of Axial Postural Abnormalities in People with Parkinson\\u2019s Disease through Augmented Human Pose Estimation\",\"Body joints regression using deep convolutional neural networks\",\"Deep convolutional networks for marker-less human pose estimation from multiple views\",\"YOLO-Rlepose: Improved YOLO Based on Swin Transformer and Rle-Oks Loss for Multi-Person Pose Estimation\",\"TransNet: Parallel encoder architecture for human pose estimation\",\"Occlusion-Aware Pedestrian Detection and Tracking\",\"Real-time multiple spatiotemporal action localization and prediction approach using deep learning\",\"Meshlifter: Weakly supervised approach for 3D human mesh reconstruction from a single 2D pose based on loop structure\",\"Eye in the sky: Real-time drone surveillance system (DSS) for violent individuals identification using scatternet hybrid deep learning network\",\"THPoseLite, a Lightweight Neural Network for Detecting Pose in Thermal Images\",\"Dynamicity-based Crop-Drop: A Context-based 2D Pose Refreshing Algorithm\",\"Segmentation-Based Background-Inference and Small-Person Pose Estimation\",\"Parallel deep learning ensembles for human pose estimation\",\"Motion Projection Consistency-Based 3-D Human Pose Estimation with Virtual Bones from Monocular Videos\",\"Tiny-Hourglassnet: An Efficient Design for 3d Human Pose Estimation\",\"A semi-supervised data augmentation approach using 3D graphical engines\",\"Beyond Perspectives: Enhancing Pose Estimation via Viewpoint Transformation\",\"UCO Physical Rehabilitation: New Dataset and Study of Human Pose Estimation Methods on Physical Rehabilitation Exercises\",\"Dangerous Behavior Recognition Based on Pose Estimation and Action Analysis\",\"A novel dataset and deep learning-based approach for marker-less motion capture during gait\",\"Design of a Diagnostic System for Patient Recovery Based on Deep Learning Image Processing: For the Prevention of Bedsores and Leg Rehabilitation\",\"Modeling Noisy Annotations for Point-Wise Supervision\",\"Deep 3D pose dictionary: 3D human pose estimation from single RGB image using deep convolutional neural network\",\"Comparative Performance Analysis of Deep Neural Networks for Human Activity Recognition\",\"Pose estimation of hurdles athletes using openpose\",\"Robust Few-Shot Pose Estimation of Articulated Robots using Monocular Cameras and Deep-Learning-based Keypoint Detection\",\"Deep-Learing-Based 2D Human Pose Estimation: Present and Future; [\\u57fa\\u4e8e\\u6df1\\u5ea6\\u5b66\\u4e60\\u7684\\u4e8c\\u7ef4\\u1109\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1\\uff1a\\u73b0\\u72b6\\u53ca\\u5c55\\u671b]\",\"Deep Human Pose Estimation Method Based on Mixture Articulated Limb Model; [\\u57fa\\u4e8e\\u6df7\\u5408\\u5173\\u8282\\u80a2\\u4f53\\u6a21\\u578b\\u7684\\u6df1\\u5ea6\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1\\u65b9\\u6cd5]\",\"Video-based detection of hemiparetic weakness side in post-stroke patient\",\"Real-time human pose estimation on a smart walker using convolutional neural networks\",\"Human Pose Estimation Based on Feature Fusion and Graph Encoding Optimization\",\"On the Estimation of Children\\u2019s Poses\",\"On human behavior recognition with deep learning and IR spectral signal restoration technologies in a natural classroom\",\"Deep 3D human pose estimation: A review\",\"3D human pose sequence estimation from point clouds combing temporal feature and joint learning strategy; [\\u878d\\u5408\\u65f6\\u5e8f\\u7279\\u5f81\\u7ea6\\u675f\\u4e0e\\u8054\\u5408\\u4f18\\u5316\\u7684\\u70b9\\u4e91 3 \\u7ef4\\u4eba\\u4f53\\u59ff\\u6001\\u5e8f\\u5217\\u4f30\\u8ba1]\",\"Proceedings - 2021 International Conference on 3D Vision, 3DV 2021\",\"Applications of Deep Learning for Top-View Omnidirectional Imaging: A Survey\",\"RepEPnP: Weakly Supervised 3D Human Pose Estimation with EPnP Algorithm\",\"3D human pose estimation with adversarial learning\",\"3D human pose and shape estimation via de-occlusion multi-task learning\",\"BlanketSet - A Clinical Real-World In-Bed Action Recognition and Qualitative Semi-Synchronised Motion Capture Dataset\",\"Privacy-Oriented Successive Approximation Image Position Follower Processing\",\"Intelligent Interaction System Capable of Autonomous Mobility for At-Home Workouts\",\"Multi-agent deep reinforcement learning for online 3d human poses estimation\",\"MoDeep: A deep learning framework using motion features for human pose estimation\",\"Human body part estimation from depth images via spatially-constrained deep learning\",\"A Comprehensive Defense Approach Targeting The Computer Vision Based Cheating Tools in FPS Video Games\",\"Human pose estimation based on lightweight basicblock\",\"ASHN for Multi-Human Pose Estimation\",\"Computer vision intelligent approaches to extract human pose and its activity from image sequences\",\"Real-Time End-to-End 3D Human Pose Prediction on AI Edge Devices\",\"Multi-view pose generator based on deep learning for monocular 3D human pose estimation\",\"Use of Keypoint-RCNN and YOLOv7 for Capturing Biomechanics and Barbell Trajectory in Weightlifting\",\"Human pose estimation method based on flexible model and deep learning\",\"3D Human Pose Estimation Using Blazepose and Direct Linear Transform (DLT) for Joint Angle Measurement\",\"3D human pose and shape estimation through collaborative learning and multi-view model-fitting\",\"Review on Human Pose Estimation & Human Body Joints Localization\",\"Follower: A novel self-deployable action recognition framework\",\"Fusion with Temporal and Spatial Attention for Video Human Pose Estimation\",\"Part-Level Occlusion-Aware Human Pose Estimation; [\\u90e8\\u4f4d\\u7ea7\\u906e\\u6321\\u611f\\u77e5\\u7684\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1]\",\"AECA-PRNetCC: Adaptive Efficient Channel Attention-based PoseResNet for Coordinate Classification in 2D Human Pose\",\"Deep learning based 2D human pose estimation: A survey\",\"A comprehensive survey on 2D multi-person pose estimation methods\",\"Fast Underwater Image Enhancement for Improved Visual Perception\",\"Efficient Human Pose Estimation via 3D Event Point Cloud\",\"GAN-Uplift: 2D to 3D Uplift with Generative Adversarial Networks\",\"Enabling Gait Analysis in the Telemedicine Practice through Portable and Accurate 3D Human Pose Estimation\",\"3D Human Pose Machines with Self-Supervised Learning\",\"Constraints on Optimising Encoder-Only Transformers for Modelling Sign Language with Human Pose Estimation Keypoint Data\",\"Short: Deep Learning Approach to Skeletal Performance Evaluation of Physical Therapy Exercises\",\"EfficientPose: Scalable single-person pose estimation\",\"Performance benchmark of deep learning human pose estimation for UAVs\",\"3D human pose estimation with 2D raw data\",\"Anatomy and Geometry Constrained One-Stage Framework for 3D Human Pose Estimation\",\"Relationship between the Results of Arm Swing Data from the OpenPose-Based Gait Analysis System and MDS-UPDRS Scores\",\"Overview of 3D Human Pose Estimation\",\"Vision-Based Method for Automatic Quantification of Parkinsonian Bradykinesia\",\"RefinePose: Towards More Refined Human Pose Estimation\",\"Learning Dynamical Human-Joint Affinity for 3D Pose Estimation in Videos\",\"A Critical Review of Object Detection using Convolution Neural Network\",\"Modelling Proper and Improper Sitting Posture of Computer Users Using Machine Vision for a Human\\u2013Computer Intelligent Interactive System during COVID-19\",\"Lightweight 2D Human Pose Estimation Based on Joint Channel Coordinate Attention Mechanism\",\"Semantic pose machines\",\"A Comparative Analysis for Single Person and Multi Person Pose Estimation Using Deep Learning Algorithms\",\"Multi-initialization Optimization Network for Accurate 3D Human Pose and Shape Estimation\",\"Predicted Seamless Human Positioning Algorithm based on M-RCNN in Obstacle Environment for Indoor Localization\",\"LOWER BODY REHABILITATION DATASET AND MODEL OPTIMIZATION\",\"LHPE-nets: A lightweight 2D and 3D human pose estimation model with well-structural deep networks and multi-view pose sample simplification method\",\"Neuromorphic high-frequency 3D dancing pose estimation in dynamic environment\",\"2D Human Pose Estimation from Monocular Images: A Survey\",\"Towards improvement of baseline performance for regression based human pose estimation\",\"A Novel Model for Intelligent Pull-Ups Test Based on Key Point Estimation of Human Body and Equipment\",\"A Temporal Convolutional Neural Network Based Activity Recognition Model using a Real-Time Two-Dimensional Single Pose Estimation Framework\",\"2D human pose estimation based on object detection using RGB-D information\",\"Real-Time Human Pose Estimation on Embedded Devices Based on Deep Learning\",\"Structured Context Enhancement Network for Mouse Pose Estimation\",\"A review of deep learning techniques for 2D and 3D human pose estimation\",\"Computer Vision & Deep Learning based Realtime and Pre-Recorded Human Pose Estimation\",\"Estimating a 2D pose from a tiny person image with super-resolution reconstruction\",\"Beginning Machine Learning in the Browser: Quick-start Guide to Gait Analysis with JavaScript and TensorFlow.js\",\"Self Adversarial Training for Human Pose Estimation\",\"Pose-attention: A novel baseline for person re-identification\",\"Human Pose Estimation in UAV-Human Workspace\",\"Coarse-to-Fine 3D Human Pose Estimation\",\"Visual-Based Positioning and Pose Estimation\",\"DeepRehab: Real Time Pose Estimation on the Edge for Knee Injury Rehabilitation\",\"Lightweight Human Pose Estimation Network Based on HRNet; [\\u57fa \\u4e8e H R N et \\u7684 \\u8f7b \\u91cf \\u5316 \\u4eba \\u4f53 \\u59ff \\u6001 \\u4f30 \\u8ba1 \\u7f51 \\u7edc]\",\"A deep learning based method for 3D human pose estimation from 2D fisheye images\",\"PoseSonic: 3D Upper Body Pose Estimation Through Egocentric Acoustic Sensing on Smartglasses\",\"Staged cascaded network for monocular 3D human pose estimation\",\"Cascade attention networks for group emotion recognition with face, body and image cues\",\"Multi-Class Human Activity Prediction using Deep Learning Algorithm\",\"Deep learning approaches for workout repetition counting and validation\",\"2D\\u20133D pose consistency-based conditional random fields for 3D human pose estimation\",\"A System for Calculating the Amount of Motion Based on 3D Pose Estimation\",\"Attention Mechanism Exploits Temporal Contexts: Real-Time 3D Human Pose Reconstruction\",\"Joint Classification and Trajectory Regression of Online Handwriting using a Multi-Task Learning Approach\",\"Bayesian capsule networks for 3D human pose estimation from single 2D images\",\"EfficientPose: Efficient human pose estimation with neural architecture search\",\"3D human pose estimation by depth map\",\"Efficient High-Resolution Human Pose Estimation\",\"A mobile application for running form analysis based on pose estimation technique\",\"CapsulePose: A variational CapsNet for real-time end-to-end 3D human pose estimation\",\"Unsupervised pose estimation via inverse graphics\",\"Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network\",\"An Application of Evaluation of Human Sketches using Deep Learning Technique\",\"Region-of-interest and channel attention-based joint optimization of image compression and computer vision\",\"Cooperative Localization for\\u00a0Human Pose Estimation\",\"A survey of technical challenges in computer vision via machine and deep learning for human pose estimation\",\"Enhancing 3D Human Pose Estimation through Adversarial Learning and Graph Convolutional Networks\",\"Video-Based 3D pose estimation for residential roofing\",\"Human centric spatial affordances for improving human activity recognition\",\"PifPaf: Composite fields for human pose estimation\",\"Researches Advanced in Human Pose Estimation Based on Deep Learning\",\"Efficient Spatial-Attention Module for\\u00a0Human Pose Estimation\",\"Feature Monitored High-Dimension Endecoder Net for End to End Markless Human Pose Estimation; [\\u5e26\\u7279\\u5f81\\u76d1\\u63a7\\u7684\\u9ad8\\u7ef4\\u4fe1\\u606f\\u7f16\\u89e3\\u7801\\u7aef\\u5230\\u7aef\\u65e0\\u6807\\u8bb0\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1\\u7f51\\u7edc]\",\"Lifting 2d Human Pose to 3d : A Weakly Supervised Approach\",\"Human Pose Estimation Based Pre-training Model and Efficient High-Resolution Representation\",\"Human Posture Estimation: In Aspect of the Agriculture Industry\",\"Towards kinematically constrained real time human pose estimation using sparse IMUs\",\"Human Pose Estimation based Speed Detection System for Running on Treadmill\",\"Learning Latent Representations of 3D Human Pose with Deep Neural Networks\",\"Measuring Gait Variables Using Computer Vision to Assess Mobility and Fall Risk in Older Adults with Dementia\",\"Lightweight human pose estimation with attention mechanism\",\"Research on Dance Movement Evaluation Method Based on Deep Learning Posture Estimation\",\"Uncertainty-Aware 3D Human Pose Estimation from Monocular Video\",\"Human 3D Pose Estimation Based on Inception Architecture\",\"Pose Estimation Based on Attention Module and CPN; [\\u57fa\\u4e8e\\u6ce8\\u610f\\u529b\\u673a\\u5236\\u548c\\u7ea7\\u8054\\u91d1\\u5b57\\u5854\\u7f51\\u7edc\\u7684\\u59ff\\u6001\\u4f30\\u8ba1]\",\"Simple Fine-Tuning Attention Modules for Human Pose Estimation\",\"Full-BAPose: Bottom Up Framework for Full Body Pose Estimation\",\"Fusion Poser: 3D Human Pose Estimation Using Sparse IMUs and Head Trackers in Real Time\",\"Weakly-supervised discovery of geometry-aware representation for 3D human pose estimation\",\"SAHF-LightPoseResNet: Spatially-Aware Attention-Based Hierarchical Features Enabled Lightweight PoseResNet for 2D Human Pose Estimation\",\"Hierarchical graphical-based human pose estimation via local multi-resolution convolutional neural network\",\"Learning human pose estimation features with convolutional networks\",\"3D human pose estimation via deep learning from 2D annotations\",\"Building semantic understanding beyond deep learning from sound and vision\",\"Deep High-Resolution Representation Learning for Visual Recognition\",\"SOCA-PRNet: Spatially Oriented Attention-Infused Structured-Feature-Enabled PoseResNet for 2D Human Pose Estimation\",\"OpenPose's Evaluation in the Video Traditional Martial Arts Presentation\",\"Using EfficientNet-B7 (CNN), Variational Auto Encoder (VAE) and Siamese Twins\\u2019 Networks to Evaluate Human Exercises as Super Objects in a TSSCI Images\",\"Deep autoencoder for combined human pose estimation and body model upscaling\",\"Center point to pose: Multiple views 3D human pose estimation for multi-person\",\"Marker-less 3D human motion capture with monocular image sequence and height-maps\",\"SPCNet: Spatial preserve and content-aware network for human pose estimation\",\"ConvPose: A modern pure ConvNet for human pose estimation\",\"Magnetometer robust deep human pose regression with uncertainty prediction using sparse body worn magnetic inertial measurement units\",\"Optimising 3D-CNN design towards human pose estimation on low power devices\",\"Patient 3D body pose estimation from pressure imaging\",\"Human Posture Estimation\",\"Refining the pose: Training and use of deep recurrent autoencoders for improving human pose estimation\",\"Adversarial Learning of Structure-Aware Fully Convolutional Networks for Landmark Localization\",\"SWBPose: A Lightweight and Efficient Method for Human Pose Estimation\",\"Effect of Face Blurring on Human Pose Estimation: Ensuring Subject Privacy for Medical and Occupational Health Applications\",\"3D human pose estimation from RGB-D images using deep learning method\",\"Improved DenseNet network for human pose estimation; [\\u57fa\\u4e8e\\u6539\\u8fdbDenseNet\\u7f51\\u7edc\\u7684\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1]\",\"Evaluation of CNN-Based Human Pose Estimation for Body Segment Lengths Assessment\",\"DEEP LEARNING BASED MULTIPLE ANIMAL POSE ESTIMATION\",\"BalanceHRNet: An effective network for bottom-up human pose estimation\",\"Research on Lightweight High-resolution Network Human Pose Estimation Based on Self-attention\",\"Lightweight and efficient human pose estimation with enhanced priori skeleton structure; [\\u5f3a \\u5316 \\u5148 \\u9a8c \\u9aa8 \\u67b6 \\u7ed3 \\u6784 \\u7684 \\u8f7b \\u91cf \\u578b \\u9ad8 \\u6548 \\u4eba \\u4f53 \\u59ff \\u6001 \\u4f30 \\u8ba1]\",\"Depth-Based vs. Color-Based Pose Estimation in\\u00a0Human Action Recognition\",\"DeepSafety: A Deep Learning Framework for Unsafe Behaviors Detection of Steel Activity in Construction Projects\",\"E-discover state-of-the-art research trends of deep learning for computer vision\",\"Person Re-Identification Based on Instance Segmentation and Pose Estimation\",\"A Deep Feedforward Neural Network Model for Image Prediction\",\"Active Non-Line-of-Sight human pose estimation based on deep learning\",\"A Joint Relationship Aware Neural Network for Single-Image 3D Human Pose Estimation\",\"A monocular 3D human pose estimation approach for virtual character skeleton retargeting\",\"Deep learning-based estimation of whole-body kinematics from multi-view images\",\"Deep full-body HPE for activity recognition from RGB frames only\",\"Human Pose Estimation Using Deep Learning: A Systematic Literature Review\",\"Tensor Computation for Data Analysis\",\"Human pose estimation via improved ResNet50\",\"Downsizing Heatmap Resolution for real-time 3D Human Pose Estimation\",\"Enhancing Localization Accuracy of Indoor Occupancy Tracking Using Optical Camera Communication and Human Pose Estimation\",\"Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation\",\"Relation-Based Associative Joint Location for Human Pose Estimation in Videos\",\"Recognition of Cheating Behavior in Examination Room Based on Deep Learning\",\"Deep body-pose estimation via synthetic depth data: A case study\",\"3D human pose estimation: Using context information in monocular video\",\"Algorithm for Predicting Pedestrian Behavior on Public Roads\",\"Human Pose Estimation Based on Light-Weight High-Resolution Network with Polarized Self-Attention\",\"A systematic review of the application of camera-based human pose estimation in the field of sport and physical exercise\",\"Uniting holistic and part-based attitudes for accurate and robust deep human pose estimation\",\"Nearby-person Occlusion Data Augmentation for Human Pose Estimation with Non-extra Annotations\",\"Teacher Classroom Behavior Detection Based on a Human Pose Estimation Algorithm\",\"Kinetic particles : from human pose estimation to an immersive and interactive piece of art questionning thought-movement relationships.\",\"DeepWindow: Sliding window based on deep learning for road extraction from remote sensing images\",\"A Study of 2D Multi-person Pose Estimation Using Distance Scaling on Images\",\"Action recognition based on human pose estimation\",\"Beyond Human Detection: A Benchmark for Detecting Common Human Posture\",\"Deep learning-based real-time 3D human pose estimation\",\"Stacked encoder-decoders for accurate semantic segmentation of very high resolution satellite datasets\",\"A multi-scale recalibrated approach for 3D human pose estimation\",\"DUA: A Domain-Unified Approach for Cross-Dataset 3D Human Pose Estimation\",\"A Deconvolutional Bottom-up Deep Network for multi-person pose estimation\",\"Improving Accuracy and Runtime of Skeletal Tracking of Lower Limbs for Athletic Jump Mechanics Assessment\",\"A Comprehensive Analysis of Deep Regression\",\"Multiview Video-Based 3-D Pose Estimation of Patients in Computer-Assisted Rehabilitation Environment (CAREN)\",\"AI and computer vision technologies for metaverse\",\"LEARNING MONOCULAR 3D HUMAN POSE ESTIMATION WITH SKELETAL INTERPOLATION\",\"UMVpose++: Unsupervised Multi-View Multi-Person 3D Pose Estimation Using Ground Point Matching\",\"A Work-Related Musculoskeletal Disorders (WMSDs) Risk-Assessment System Using a Single-View Pose Estimation Model\",\"MSRT: multi-scale representation transformer for regression-based human pose estimation\",\"Human Body Pose Estimation for Gait Identification: A Comprehensive Survey of Datasets and Models\",\"2022 8th International Conference on Virtual Reality, ICVR 2022\",\"Human pose estimation via improved resnet50\",\"Dual-Level Structural Information Learning Neural Network for Monocular 2D Pose Estimation\",\"ActiveMoCap: Optimized viewpoint selection for active human motion capture\",\"AnimePose: Multi-person 3D pose estimation and animation\",\"Active Learning for Human Pose Estimation based on Temporal Pose Continuity\",\"What face and body shapes can tell us about height\",\"Human pose search using deep networks\",\"Self-supervised Keypoint Correspondences for Multi-person Pose Estimation and Tracking in Videos\",\"Deep Learning-based Human Pose Estimation: A Survey\",\"Deep learning-enabled multitask system for exercise recognition and counting\",\"Animal pose estimation: A closer look at the state-of-the-art, existing gaps and opportunities\",\"Integral human pose regression\",\"Supervised High-Dimension Endecoder Net: 3D End to End Prediction Network for Mark-less Human Pose Estimation from Single Depth Map\",\"Human Pose Estimation: Benchmarking Deep Learning-based Methods\",\"Semi-Perspective Decoupled Heatmaps for 3D Robot Pose Estimation From Depth Maps\",\"An adaptive stacked hourglass network with Kalman filter for estimating 2D human pose in video\",\"Cross-Domain Complementary Learning Using Pose for Multi-Person Part Segmentation\",\"Object detection and analysis of human body postures based on TensorFlow\",\"Exploiting 3D Hand Pose Estimation in Deep Learning-Based Sign Language Recognition from RGB Videos\",\"Human pose estimation using deep learning: review, methodologies, progress and future research directions\",\"T-net: Parametrizing fully convolutional nets with a single high-order tensor\",\"Proceedings of the 2023 5th International Conference on Image, Video and Signal Processing, IVSP 2023\",\"Automated work efficiency analysis for smart manufacturing using human pose tracking and temporal action localization\",\"Deep kinematic pose regression\",\"Recent Advances of Monocular 2D and 3D Human Pose Estimation: A Deep Learning Perspective\",\"Monocular human pose estimation: A survey of deep learning-based methods\",\"Hybrid Approach for Orientation-Estimation of Rotating Humans in Video Frames Acquired by Stationary Monocular Camera\",\"Fast and fluid human pose tracking\",\"JointFusionNet: Parallel Learning Human Structural Local and\\u00a0Global Joint Features for\\u00a03D Human Pose Estimation\",\"Gesture recognition based on 3D human pose estimation and body part segmentation for RGB data input\",\"An HRNET-BLSTM model with two-stage training for singing melody extraction\",\"Improvements on Integrated Health and Safety Management System based on Wi-Pose to increase Productivity\",\"Real-time human pose recognition in complex environment based on the bidirectional LSTM; [\\u57fa\\u4e8e\\u53cc\\u5411LSTM\\u7684\\u590d\\u6742\\u73af\\u5883\\u4e0b\\u5b9e\\u65f6\\u4eba\\u4f53\\u59ff\\u52bf\\u8bc6\\u522b]\",\"A human pose estimation algorithm based on the integration of improved convolutional neural networks and multi-level graph structure constrained model\",\"Testing the Feasibility of a Multi-Model Fusion Method for Monitoring the Action of Rehabilitating Stroke Patients in Care Management\",\"Modelling Sign Language with Encoder-Only Transformers and Human Pose Estimation Keypoint Data\",\"LogRF: An Approach to Human Pose Estimation Using Skeleton Landmarks for Physiotherapy Fitness Exercise Correction\",\"Networked Inertial Navigation with Constraints Generated by Neural Networks\",\"Study on MindSpore Deep Learning Framework\",\"Real-Time Drone Surveillance System for Violent Crowd Behavior Unmanned Aircraft System (UAS) - Human Autonomy Teaming (HAT)\",\"Monocular 3D Human Pose Markerless Systems for Gait Assessment\",\"Computer Users Sitting Posture Classification Using Distinct Feature Points and Small Scale Convolutional Neural Network for Humana Computer Intelligent Interactive System During COVID-19\",\"Deep reinforcement learning for active human pose estimation\",\"Pose-based human activity recognition: A review\",\"DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders\",\"3D human pose estimation from depth maps using a deep combination of poses\",\"Deep learning for material synthesis and pose estimation material systems: A review\",\"Internet-of-Things-Enabled Markerless Running Gait Assessment from a Single Smartphone Camera\",\"Quantitative loosening detection of threaded fasteners using vision-based deep learning and geometric imaging theory\",\"Neural body fitting: Unifying deep learning and model based human pose and shape estimation\",\"A comprehensive survey on human pose estimation approaches\",\"SPACE: Finding Key-speaker in Complex Multi-person Scenes\",\"Gait recognition based on human walking feature vector diagram; [\\u57fa\\u4e8e\\u884c\\u8d70\\u7279\\u5f81\\u77e2\\u91cf\\u56fe\\u7684\\u6b65\\u6001\\u8bc6\\u522b\\u65b9\\u6cd5]\",\"A multi-sensor architecture combining human pose estimation and real-time location systems for workflow monitoring on hybrid operating suites\",\"Multi-source deep learning for human pose estimation\",\"Effects of Image Quality on the Accuracy Human Pose Estimation and Detection of Eye Lid Opening\\u002fClosing Using Openpose and DLib\",\"Adapting MobileNets for mobile based upper body pose estimation\",\"InfPose: Real-time Infrared Multi-human Pose Estimation for Edge devices based on Encoder-Decoder CNN Architecture\",\"Performance Evaluation of Model-Based Gait on Multi-View Very Large Population Database with Pose Sequences\",\"Human Pose Estimation based on Manifold Gaussian Process with Depth Images\",\"In-Bed Pose Estimation: Deep Learning with Shallow Dataset\",\"BowlingDL: A Deep Learning-Based Bowling Players Pose Estimation and Classification\",\"Deep neural networks for human pose estimation from a very low resolution depth image\",\"Human pose estimation based on cross-view feature fusion\",\"Three-Dimensional Action Recognition for Basketball Teaching Coupled with Deep Neural Network\",\"Traditional African Dances Preservation Using Deep Learning Techniques\",\"Exploring rare pose in human pose estimation\",\"A Vision-based Deep Learning Platform for Human Motor Activity Recognition\",\"EventHPE: Event-based 3D Human Pose and Shape Estimation\",\"Toward modeling psychomotor performance in karate combats using computer vision pose estimation\",\"Research Progress of Two-Dimensional Human Pose Estimation Based on Deep Learning\",\"Human pose estimation from depth images via inference embedded multi-task learning\",\"CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-wild 2D Annotations\",\"A Survey of\\u00a0Recent Advances on\\u00a0Two-Step 3D Human Pose Estimation\",\"Human Abnormal Behavior Recognition Algorithm Based on Pose Estimation; [\\u57fa\\u4e8e\\u59ff\\u6001\\u4f30\\u8ba1\\u7684\\u4eba\\u4f53\\u5f02\\u5e38\\u884c\\u4e3a\\u8bc6\\u522b\\u7b97\\u6cd5]\",\"A Strong Geometric Baseline for\\u00a0Cross-View Matching of\\u00a0Multi-person 3D Pose Estimation from\\u00a0Multi-view Images\",\"Application of Deep Convolution Network Algorithm in Sports Video Hot Spot Detection\",\"Multi-person Human Pose Estimation Based on Deformable Convolution; [\\u57fa\\u4e8e\\u53ef\\u53d8\\u5f62\\u5377\\u79ef\\u7684\\u591a\\u4eba\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1]\",\"Refining joint locations for human pose tracking in sports videos\",\"Fusing information from multiple 2D depth cameras for 3D human pose estimation in the operating room\",\"Real-Time Possessing Relationship Detection for Sports Analytics\",\"DeepPose: Human pose estimation via deep neural networks\",\"Residual pose: A decoupled approach for depth-based 3D human pose estimation\",\"Squirrel Search Optimization with Deep Convolutional Neural Network for Human Pose Estimation\",\"Adaptive and Robust Partition Learning for Person Retrieval with Policy Gradient\",\"Human Pose Estimation Based on SNHRNet\",\"Three stage deep network for 3D human pose reconstruction by exploiting spatial and temporal data via its 2D pose\",\"Depth-based 3D human pose refinement: Evaluating the refinet framework\",\"2D Pose Estimation Based on Deep Learning for Mobile System\",\"Study on Deep Learning Models for Human Pose Estimation and its Real Time Application\",\"Coupled multiview autoencoders with locality sensitivity for three-dimensional human pose estimation\",\"Deep Human Pose Estimation via Self-guided Learning\",\"Automatic estimation of pose and falls in videos using computer vision model\",\"3D single person pose estimation method based on deep learning\",\"Light-weighted Network based Human Pose Estimation for Mobile AR Service\",\"Group Activity Recognition via Computing Human Pose Motion History and Collective Map from Video\",\"Baseball Swing Pose Estimation Using OpenPose\",\"Machine Learning-Based Automatic Rating for Cardinal Symptoms of Parkinson Disease\",\"In the wild human pose estimation using explicit 2D features and intermediate 3D representations\",\"Fast Dataset Collection Approach for Articulated Equipment Pose Estimation\",\"ANN for human pose estimation in low resolution depth images\",\"SRPose: Low-Resolution Human Pose Estimation with\\u00a0Super-Resolution\",\"Diagnosis of Cerebellar Ataxia Based on Gait Analysis Using Human Pose Estimation: A Deep Learning Approach\",\"3D Human Body Shape and Pose Estimation from Depth Image\",\"Using DeepLabCut for 3D markerless pose estimation across species and behaviors\",\"3D human pose estimation from a single image via exemplar augmentation\",\"Deep Learning-Based 2D and 3D Human Pose Estimation: A Survey\",\"ITERATIVE SUBNETWORK WITH LINEAR HIERARCHICAL ORDERING FOR HUMAN POSE ESTIMATION\",\"LDCNet: Limb Direction Cues-aware Network for Flexible Human Pose Estimation in Industrial Behavioral Biometrics Systems\",\"Understanding holistic human pose using class-specific convolutional neural network\",\"Performing visual gait identification for re-identification without specialized labels\",\"Syllable-level Korean Fingerspelling Recognition from a Video\",\"On the impact of lossy image and video compression on the performance of deep convolutional neural network architectures\",\"Human pose completion in partial body camera shots\",\"DE1-SoC FPGA Support for Human Posture Detection System\",\"The Latest Progress in Human Pose Estimation\",\"Consensus-Based Optimization for 3D Human Pose Estimation in Camera Coordinates\",\"An efficient convolutional network for human pose estimation\",\"Multi-View 3D Human Pose Estimation with Self-Supervised Learning\",\"Caged Monkey Dataset: A New Benchmark for Caged Monkey Pose Estimation\",\"Joint Path Alignment Framework for 3D Human Pose and Shape Estimation From Video\",\"UniPose+: A Unified Framework for 2D and 3D Human Pose Estimation in Images and Videos\",\"Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation\",\"Scale-Aware Network with Attentional Selection for Human Pose Estimation\",\"3D Human Knee Flexion Angle Estimation Using Deep Convolutional Neural Networks\",\"An insight into human pose estimation and its applications\",\"Evaluation of deep learning based pose estimation for sign language recognition\",\"Vector hourglass network for human pose estimation based on deep learning\",\"Unsupervised cross-dataset adaptation via probabilistic amodal 3D human pose completion\",\"Overview on 2D Human Pose Estimation Based on Deep Learning; [\\u57fa\\u4e8e\\u6df1\\u5ea6\\u5b66\\u4e60\\u7684\\u4e8c\\u7ef4\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1\\u7efc\\u8ff0]\",\"Deep learning for visual understanding: A review\",\"Research on Pedestrian Tracking Algorithm Based on Deep Learning\",\"Propagating LSTM: 3D pose estimation based on joint interdependency\",\"EFCPose: End-to-End Multi-Person Pose Estimation with Fully Convolutional Heads\",\"Human Pose Estimation using Deep Learning Techniques\",\"Deep high-resolution representation learning for human pose estimation\",\"A literature review: Geometric methods and their applications in human-related analysis\",\"Bi-Pose: Bidirectional 2D-3D Transformation for Human Pose Estimation From a Monocular Camera\",\"Image domain adaption of simulated data for human pose estimation\",\"Automatic evaluation of atlantoaxial subluxation in rheumatoid arthritis by a deep learning model\",\"3d human pose estimation based on multi view information fusion\",\"Pictonaut: movie cartoonization using 3D human pose estimation and GANs\",\"Deep-Learning-Based Motion Capture Technology in Film and Television Animation Production\",\"Mobile-LRPose: Low-Resolution Representation Learning for\\u00a0Human Pose Estimation in\\u00a0Mobile Devices\",\"Gait Analysis and Detection of Human Pose Diseases\",\"Human Sports Action and Ideological and PoliticalEvaluation by Lightweight Deep Learning Model\",\"Multiview 3D human pose estimation using improved least-squares and LSTM networks\",\"A Neural Network-Based Lower Extremity Joint Angle Estimation from Insole Data\",\"2D Human pose estimation: a survey\",\"Classification of Human Poses Using Deep Learning Techniques\",\"Lightweight Whole-Body Human Pose Estimation With Two-Stage Refinement Training Strategy\",\"ActionFormer: Pose-based Action Recognition via Transformer and CNN\",\"Learning 3D Human Pose Estimation from Dozens of Datasets using a Geometry-Aware Autoencoder to Bridge Between Skeleton Formats\",\"Dimensional Expansion and Time-Series Data Augmentation Policy for Skeleton-Based Pose Estimation\",\"Learning loss for active learning\",\"Interacting Multiple Model-Based Human Pose Estimation Using a Distributed 3D Camera Network\",\"CROMOSim: A Deep Learning-based Cross-modality Inertial Measurement Simulator\",\"Teaching Robots to Predict Human Motion\",\"Stacked hourglass deep learning networks based on attention mechanism in multi-person pose estimation\",\"Improving Human Pose Estimation Based on Stacked Hourglass Network\",\"An Intelligent Human Activity Recognizer for\\u00a0Visually Impaired People Using VGG-SVM Model\",\"3D Human Pose Machine with a ToF Sensor using Pre-trained Convolutional Neural Networks\",\"A unified deep framework for joint 3D pose estimation and action recognition from a single RGB camera\",\"Unsupervised 3D pose estimation with geometric self-supervision\",\"MetaFi: Device-Free Pose Estimation via Commodity WiFi for Metaverse Avatar Simulation\",\"Hybrid approach for orientation-estimation of rotating humans in video frames acquired by stationary monocular camera\",\"Unbiased feature position alignment for human pose estimation\",\"Application of structured support vector machine backpropagation to a convolutional neural network for human pose estimation\",\"Multi-scale Adaptive Structure Network for Human Pose Estimation from Color Images\",\"High-Resolution Representations Network for Single Image Dehazing\",\"Simple multi-resolution representation learning for human pose estimation\",\"Concurrent validity of human pose tracking in video for measuring gait parameters in older adults: a preliminary analysis with multiple trackers, viewing angles, and walking directions\",\"A multi-task neural network for action recognition with 3D key-points\",\"Handgun Detection Using Combined Human Pose and Weapon Appearance\",\"Review of Deep Learning-Based Human Pose Estimation; [\\u57fa\\u4e8e\\u6df1\\u5ea6\\u5b66\\u4e60\\u7684\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1\\u65b9\\u6cd5\\u7efc\\u8ff0]\",\"Deep Regression Neural Networks for Proportion Judgment\",\"Weakly-supervised pre-training for 3D human pose estimation via perspective knowledge\",\"MEMe: A Mutually Enhanced Modeling Method for Efficient and Effective Human Pose Estimation\",\"Recovering 3D human poses and camera motions from deep sequence\",\"2-D Human Pose Estimation from Images Based on Deep Learning: A Review\",\"Human Pose Estimation Using Convolutional Neural Networks\",\"HuMoMM: A Multi-Modal Dataset and\\u00a0Benchmark for\\u00a0Human Motion Analysis\",\"Design and evaluation of intelligent teaching system on basic movements in PE\",\"Research on Human Pose Estimation and Object Detection in the Field of Unmanned Retail\",\"Deep probabilistic human pose estimation\",\"A Lightweight and Fast Approach for Upper Limb Range of Motion Assessment\",\"The Risk Classification of Ergonomic Musculoskeletal Disorders in Work-related Repetitive Manual Handling Operations with Deep Learning Approaches\",\"Joint usage of global and local attentions in hourglass network for human pose estimation\",\"Human Body-Aware Feature Extractor Using Attachable Feature Corrector for Human Pose Estimation\",\"Deep learning techniques for physical abuse detection\",\"HiddenPose: Non-Line-of-Sight 3D Human Pose Estimation\",\"A Review on Human Pose Estimation Using Mediapipe\",\"The IKEA ASM Dataset: Understanding people assembling furniture through actions, objects and pose\",\"Real-time weld seam feature extraction in construction sites\",\"Motion Capture for Sporting Events Based on Graph Convolutional Neural Networks and Single Target Pose Estimation Algorithms\",\"Multi-modal 3D Human Pose Estimation for Human-Robot Collaborative Applications\",\"A baseline for cross-database 3d human pose estimation\",\"REALISTIC AUGMENTATION FOR EFFECTIVE 2D HUMAN POSE ESTIMATION UNDER OCCLUSION\",\"Deep Learning Networks for View-independent Knee and Elbow Joint Angle Estimation\",\"A LIGHTWEIGHT MULTI-PERSON POSE ESTIMATION SCHEME BASED ON JETSON NANO\",\"Learning to Augment Poses for 3D Human Pose Estimation in Images and Videos\",\"Generating multiple hypotheses for 3D human pose estimation with mixture density network\",\"Localization of hard joints in human pose estimation based on residual down-sampling and attention mechanism\",\"Human Pose Estimation Using Deep Convolutional Densenet Hourglass Network with Intermediate Points Voting\",\"Deep learning methods for 3D human pose estimation under different supervision paradigms: A survey\",\"EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation Using Accelerated Neuroevolution with Weight Transfer\",\"Human pose estimation based on Improved High Resolution Network\",\"JointPose: Jointly Optimizing Evolutionary Data Augmentation and Prediction Neural Network for 3D Human Pose Estimation\",\"Advanced Human\\u2013Computer Interaction Technology in Digital Twins\",\"Supporting sports instruction with comparative display of forms\",\"V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map\",\"Home-based physical therapy with an interactive computer vision system\",\"Analyzing Data Efficiency and Performance of Machine Learning Algorithms for Assessing Low Back Pain Physical Rehabilitation Exercises\",\"Deep learning based two-dimension human pose estimation\\uff1aa critical analysis; [\\u6df1\\u5ea6\\u5b66\\u4e60\\u4e8c\\u7ef4\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1\\u65b9\\u6cd5\\u7efc\\u8ff0]\",\"Latency control for distributed machine vision at the edge through approximate computing\",\"Research Advanced in Human Pose Estimation based on Deep Learning\",\"Computer vision approaches based on deep learning and neural networks: Deep neural networks for video analysis of human pose estimation\",\"Assessing Bicep Curl Exercises by Human Pose Application: A Preliminary Study\",\"Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation\",\"Reducing the device complexity for 3D human pose estimation: A deep learning approach using monocular camera and IMUs\",\"ASPset: An outdoor sports pose video dataset with 3D keypoint annotations\",\"Virtual Character Animation based on Data-driven Motion Capture using Deep Learning Technique\",\"A Review of Human Pose Estimation Methods in Markerless Motion Capture\",\"Human Pose Estimation Based on Multi-branch Convolutional Neural Network\",\"A computer vision approach to continuously monitor fatigue during resistance training\",\"Multi-Human Pose Estimation by Deep Learning-Based Sequential Approach for Human Keypoint Position and Human Body Detection; [\\u57fa\\u4e8e\\u6df1\\u5ea6\\u5b66\\u4e60\\u5e8f\\u5217\\u65b9\\u6cd5\\u7684\\u591a\\u4eba\\u59ff\\u6001\\u4f30\\u8ba1\\u7528\\u6765\\u68c0\\u6d4b\\u4eba\\u4f53\\u4e0e\\u5173\\u952e\\u70b9\\u4f4d\\u7f6e]\",\"Real-time detection method for weld feature of construction site steel structure based on top-down paradigm; [\\u57fa\\u4e8eTop-down\\u8303\\u5f0f\\u7684\\u65bd\\u5de5\\u73b0\\u573a\\u94a2\\u7ed3\\u6784\\u710a\\u7f1d\\u7279\\u5f81\\u5b9e\\u65f6\\u68c0\\u6d4b\\u65b9\\u6cd5]\",\"Humanoid Robot Control Based on Deep Learning\",\"Video based 3D human pose estimation combining sparse representation and deep learning; [\\u7ed3\\u5408\\u7a00\\u758f\\u8868\\u793a\\u548c\\u6df1\\u5ea6\\u5b66\\u4e60\\u7684\\u89c6\\u9891\\u4e2d3D\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1]\",\"Privacy-Preserving in-Bed Human Pose Estimation: Highlights from the IEEE Video and Image Processing Cup 2021 Student Competition [SP Competitions]\",\"Video based exercise recognition and correct pose detection\",\"Enhancing Cricket Performance Analysis with Human Pose Estimation and Machine Learning\",\"Table tennis stroke recognition using two-dimensional human pose estimation\",\"A pose estimation scheme based on distance scaling algorithm in real-time environment\",\"EHPE: Skeleton Cues-based Gaussian Coordinate Encoding for Efficient Human Pose Estimation\",\"Densely connected attentional pyramid residual network for human pose estimation\",\"Robust Human Pose Estimation for Rotation via Self-Supervised Learning\",\"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era\",\"Human pose estimation using per-point body region assignment\",\"Unsupervised Multi-view Multi-person 3D Pose Estimation Using Reprojection Error\",\"AI Tool as a Fitness Trainer Using Human Pose Estimation\",\"Lightweight stacked hourglass network for human pose estimation\",\"Multi-Person Pose Estimation via Learning Feature Integration\",\"BlanketGen - A Synthetic Blanket Occlusion Augmentation Pipeline for Motion Capture Datasets\",\"Improved Human-Object Interaction Detection through On-the-Fly Stacked Generalization\",\"Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing\",\"Deep 3D Human Pose Estimation under Partial Body Presence\",\"A Systematic Review of Recent Deep Learning Approaches for 3D Human Pose Estimation\",\"Assessment of a novel deep learning-based marker-less motion capture system for gait study\",\"NAO Robot Learns to Interact with Humans through Imitation Learning from Video Observation\",\"Stacked mixed-scale networks for human pose estimation\",\"Cascaded Deep Monocular 3D Human Pose Estimation with Evolutionary Training Data\",\"Deep Mixture of MRFs for Human Pose Estimation\",\"Human attribute recognition method based on pose estimation and multiple-feature fusion\",\"A Survey on Depth Ambiguity of 3D Human Pose Estimation\",\"Viewpoint Selection for DermDrone using Deep Reinforcement Learning\",\"Human pose estimation-based real-time gait analysis using convolutional neural network\",\"PYRAMID KNOWLEDGE DISTILLATION FOR EFFICIENT HUMAN POSE ESTIMATION\",\"A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition\",\"State-consistency loss for learning spatial perception tasks from partial labels\",\"Reconsideration of multi-stage deep network for human pose estimation\",\"Hierarchical Dynamic Programming Module for Human Pose Refinement\",\"Vision-Based Human Activity Recognition Methods Using Pose Estimation\",\"Human Pose Estimation from Monocular Images: A Comprehensive Survey\",\"Exploring workout repetition counting and validation through deep learning\",\"A Review of Human Pose Estimation from Single Image\",\"Intelligent fitness trainer system based on human pose estimation\",\"Pose estimation method for construction machine based on improved AlphaPose model\",\"Human pose estimation and LSTM-based diver heading prediction for AUV navigation guidance\",\"Action Machine: Toward Person-Centric Action Recognition in Videos\",\"Precise human pose estimation based on two-dimensional images for kinematic analysis\",\"Vision-Based Human Pose Estimation via Deep Learning: A Survey\",\"3D human pose estimation from multi person stereo 360\\u25e6 scenes\",\"Automatic timed up-and-go sub-task segmentation for Parkinson's disease patients using video-based activity classification\",\"Assessment of deep learning pose estimates for sports collision tracking\",\"Deep learning-based for human segmentation and tracking, 3D human pose estimation and action recognition on monocular video of MADS dataset\",\"2D and 3D Human Pose Estimation and Analysis Using Deep Learning\",\"Optical non-line-of-sight physics-based 3d human pose estimation\",\"Shoulder and Knee Abnormality Examination Based on Artificial Landmark Estimation\",\"Anomalous Human Action Detection Using a Cascade of Deep Learning Models\",\"Human Pose Estimation Using GNN\",\"High-speed multi-person pose estimation with deep feature transfer\",\"A Practical Hybrid Active Learning Approach for Human Pose Estimation\",\"Mass displacement networks\",\"CloTH-VTON+: Clothing Three-Dimensional Reconstruction for Hybrid Image-Based Virtual Try-ON\",\"6D Object Pose Estimation Using Keypoints and Part Affinity Fields\",\"Survey on 3D Human Pose Estimation of Deep Learning\",\"Heatmap Regression via Randomized Rounding\",\"Synthetic Crowd and Pedestrian Generator for Deep Learning Problems\",\"Efficient Human Pose Estimation via Multi-Head Knowledge Distillation\",\"Pandanet: Anchor-based single-shot multi-person 3D pose estimation\",\"SYNPOSE: A LARGE-SCALE AND DENSELY ANNOTATED SYNTHETIC DATASET FOR HUMAN POSE ESTIMATION IN CLASSROOM\",\"MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision\",\"Posefix: Model-agnostic general human pose refinement network\",\"G2O-Pose: Real-Time Monocular 3D Human Pose Estimation Based on General Graph Optimization\",\"MetaFi++: WiFi-Enabled Transformer-Based Human Pose Estimation for Metaverse Avatar Simulation\",\"Human Pose Detection System Using Machine Learning\",\"Monocular 2D and 3D Human Pose Estimation Review\",\"An intelligent collaborative inference approach of service partitioning and task offloading for deep learning based service in mobile edge computing networks\",\"Deep learning for assistive computer vision\",\"Deep Learning-Based Standardized Evaluation and Human Pose Estimation: A Novel Approach to Motion Perception\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"Human Pose Estimation using Deep Learning\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"Human Pose Estimation using Deep Learning\"],\"textfont\":{\"size\":12},\"x\":[10.110274314880371,10.658209800720215,9.991835594177246,9.161947250366211,9.374369621276855,7.0029377937316895,10.138599395751953,9.503971099853516,10.173805236816406,8.37478256225586,11.259357452392578,9.840929985046387,9.460533142089844,9.771475791931152,11.849897384643555,9.764689445495605,8.195484161376953,8.708503723144531,7.834136009216309,10.428229331970215,8.25507926940918,9.918607711791992,9.849095344543457,8.718989372253418,11.634915351867676,11.247648239135742,12.367341041564941,8.679939270019531,10.053680419921875,9.528018951416016,9.153017044067383,10.132848739624023,7.640525817871094,7.538407802581787,7.2489094734191895,10.087178230285645,10.396370887756348,12.422597885131836,9.882615089416504,9.473686218261719,11.772836685180664,8.194952964782715,10.273098945617676,8.060870170593262,8.476505279541016,9.281509399414062,12.500724792480469,9.15341854095459,9.674378395080566,10.079965591430664,9.507210731506348,11.616738319396973,11.956427574157715,9.844743728637695,12.03412914276123,10.1939058303833,10.890314102172852,11.884838104248047,7.617599010467529,10.102425575256348,11.836894989013672,9.601302146911621,9.224688529968262,8.064886093139648,7.559030055999756,8.997687339782715,9.970380783081055,10.344808578491211,9.225388526916504,11.533722877502441,7.987833499908447,9.262371063232422,8.563010215759277,9.61931324005127,7.46321439743042,9.694920539855957,10.886486053466797,11.341712951660156,9.388752937316895,12.351171493530273,11.064985275268555,9.092226028442383,9.39258098602295,10.24140453338623,11.466126441955566,10.826522827148438,11.895805358886719,8.8991060256958,11.521380424499512,11.318774223327637,11.472092628479004,10.039308547973633,9.649526596069336,10.753474235534668,10.010623931884766,10.285332679748535,9.237600326538086,10.071468353271484,9.556857109069824,9.214959144592285,11.110599517822266,8.81472110748291,10.970437049865723,10.297837257385254,11.04444694519043,8.650609016418457,9.173867225646973,7.372868537902832,8.77853012084961,9.793906211853027,7.26101016998291,10.255839347839355,11.24716854095459,8.934975624084473,11.911392211914062,9.502541542053223,11.763208389282227,10.053864479064941,9.2173490524292,9.102822303771973,11.921045303344727,10.004669189453125,11.64612102508545,7.775665283203125,11.553511619567871,10.321383476257324,11.905797004699707,10.280525207519531,10.064395904541016,11.298140525817871,7.7965826988220215,11.761011123657227,10.318048477172852,10.014001846313477,11.873014450073242,12.040592193603516,11.884349822998047,7.72794246673584,8.328399658203125,9.656652450561523,10.773040771484375,7.253966331481934,9.994428634643555,10.326682090759277,10.06566047668457,10.508484840393066,11.30165958404541,7.568325519561768,11.376907348632812,11.548591613769531,12.077766418457031,11.45974349975586,10.1674165725708,9.455978393554688,8.324602127075195,8.772051811218262,9.709906578063965,10.851683616638184,6.962737560272217,12.698127746582031,9.015092849731445,8.756325721740723,9.92980670928955,9.875349044799805,10.475869178771973,11.07674503326416,7.103374004364014,9.533512115478516,12.462455749511719,9.69616985321045,8.87539291381836,10.048419952392578,11.0723295211792,9.513049125671387,10.788939476013184,9.856786727905273,12.447235107421875,11.808431625366211,7.455476760864258,10.367133140563965,7.061049938201904,11.28373908996582,11.851113319396973,11.10561466217041,8.913813591003418,11.27122974395752,7.443404197692871,7.915743827819824,10.566576957702637,10.343767166137695,10.186675071716309,9.772506713867188,8.296323776245117,11.451518058776855,7.455463409423828,7.382903575897217,10.448369026184082,9.008191108703613,10.787580490112305,12.545125007629395,11.21654224395752,10.0368013381958,12.653369903564453,7.121372222900391,10.822284698486328,9.988759994506836,7.549435615539551,9.136752128601074,7.459040641784668,8.273576736450195,12.542564392089844,9.9315185546875,11.897401809692383,9.063328742980957,10.170493125915527,10.304896354675293,8.64916706085205,6.887462615966797,11.940269470214844,10.67306900024414,11.055681228637695,9.46213436126709,10.314241409301758,10.162238121032715,8.748967170715332,7.4350080490112305,12.527766227722168,7.408657550811768,11.095730781555176,10.760537147521973,9.280706405639648,10.103090286254883,7.472356796264648,10.031669616699219,10.049034118652344,8.384178161621094,11.876246452331543,8.940010070800781,7.112857341766357,7.017149925231934,7.423766613006592,12.066205024719238,9.95499038696289,7.700482368469238,8.719111442565918,7.771733283996582,9.724489212036133,9.523234367370605,9.826805114746094,10.267863273620605,11.818092346191406,11.87055778503418,7.144198417663574,8.965269088745117,7.551263809204102,10.629145622253418,9.45384693145752,9.41700553894043,9.243555068969727,9.3604097366333,11.202139854431152,11.432197570800781,6.9744486808776855,11.868812561035156,9.26891040802002,9.071846961975098,10.798835754394531,11.091160774230957,7.268179416656494,9.537958145141602,10.176212310791016,10.818655967712402,9.363049507141113,7.295933246612549,10.203413009643555,10.360706329345703,7.365690231323242,11.692390441894531,7.799471855163574,11.330321311950684,7.964833736419678,10.01212215423584,10.519031524658203,10.125248908996582,7.949625015258789,12.055678367614746,6.72667932510376,8.957690238952637,8.46198558807373,9.908747673034668,10.796650886535645,10.193534851074219,10.268489837646484,9.47722339630127,9.350165367126465,11.613082885742188,10.846284866333008,11.34299087524414,8.436868667602539,9.802748680114746,10.978796005249023,9.366439819335938,9.434405326843262,9.598374366760254,10.79839038848877,10.178936958312988,11.841635704040527,7.154476642608643,10.3018798828125,10.132487297058105,9.8478422164917,11.608154296875,11.709774017333984,10.481931686401367,8.063603401184082,10.26575756072998,10.084259033203125,6.947946548461914,9.996193885803223,9.35910415649414,8.687362670898438,11.349459648132324,10.123119354248047,11.249056816101074,12.561720848083496,7.371851444244385,9.132916450500488,12.488737106323242,10.77443790435791,10.11110782623291,10.109940528869629,11.83227825164795,9.31789779663086,9.664435386657715,12.33881950378418,9.814912796020508,9.622137069702148,11.47856616973877,8.49293327331543,12.624269485473633,11.708776473999023,8.995152473449707,11.110512733459473,7.408825874328613,7.192348957061768,12.6094331741333,9.307923316955566,11.004836082458496,11.79637336730957,9.024264335632324,7.523192405700684,11.446160316467285,10.602035522460938,11.688496589660645,11.5201416015625,11.318521499633789,11.114335060119629,11.86338996887207,11.529932022094727,9.98013687133789,10.541024208068848,9.43803596496582,10.496850967407227,11.796879768371582,8.862361907958984,11.39628791809082,10.473170280456543,8.917330741882324,9.713284492492676,8.06602954864502,11.8045654296875,8.641378402709961,7.004244804382324,10.426970481872559,9.245537757873535,9.509790420532227,11.60362720489502,9.52078628540039,11.940766334533691,11.228205680847168,10.128878593444824,7.30462646484375,9.525748252868652,11.5210542678833,11.89813232421875,10.447723388671875,9.93256664276123,9.090568542480469,6.987131118774414,12.592795372009277,9.389566421508789,11.02739429473877,10.594486236572266,12.079924583435059,7.9052042961120605,11.960718154907227,9.194658279418945,8.726587295532227,10.10029125213623,7.419361591339111,10.301870346069336,10.707110404968262,12.057718276977539,11.590991973876953,7.705738544464111,10.38707447052002,11.054174423217773,10.518869400024414,7.528645992279053,9.66446590423584,12.066039085388184,11.827397346496582,11.515119552612305,10.225746154785156,9.335102081298828,10.055370330810547,12.053987503051758,7.62489652633667,9.046588897705078,9.694414138793945,8.003582954406738,11.592421531677246,6.976881980895996,9.630958557128906,11.007271766662598,9.60918140411377,11.488282203674316,10.694348335266113,10.83634090423584,10.88642692565918,7.508279323577881,12.734541893005371,11.369897842407227,9.329291343688965,12.53139877319336,11.40926742553711,11.213855743408203,9.665810585021973,10.073784828186035,10.669610023498535,9.694599151611328,9.975959777832031,10.3543062210083,12.537152290344238,8.595511436462402,8.712778091430664,8.549509048461914,9.9420804977417,9.259215354919434,10.095090866088867,10.264754295349121,7.931827068328857,10.472447395324707,8.164791107177734,8.688657760620117,8.51774787902832,6.9114861488342285,8.18410873413086,12.56313705444336,10.24078369140625,9.316142082214355,10.216241836547852,7.749359130859375,10.028468132019043,7.184554100036621,9.514394760131836,10.101049423217773,11.586662292480469,10.109707832336426,11.401143074035645,9.68570327758789,10.00307846069336,11.540342330932617,10.26663875579834,9.068408012390137,9.241379737854004,9.42483901977539,9.573018074035645,10.860184669494629,9.993127822875977,9.884635925292969,11.628765106201172,10.401616096496582,10.555683135986328,9.15293025970459,12.040519714355469,8.307575225830078,9.821887969970703,10.224838256835938,7.133805751800537,9.242124557495117,10.531527519226074,7.465938568115234,6.9652018547058105,9.787859916687012,11.591886520385742,11.460859298706055,10.126874923706055,11.572105407714844,11.33173942565918,12.031950950622559,7.216527462005615,11.557927131652832,7.519588470458984,11.314421653747559,8.780834197998047,12.563358306884766,9.82071304321289,10.879199981689453,11.500870704650879,10.285103797912598,11.13454818725586,8.991456031799316,9.774465560913086,10.930719375610352,10.272171020507812,11.085152626037598,10.743860244750977,11.728866577148438,11.792365074157715,9.685729026794434,11.811736106872559,9.474675178527832,10.160183906555176,7.30600643157959,9.327498435974121,10.564980506896973,11.176024436950684,8.779032707214355,9.474715232849121,11.155214309692383,9.057920455932617,12.484699249267578,9.481362342834473,10.792838096618652,12.37218952178955,8.570549964904785,8.795162200927734,10.004226684570312,8.995344161987305,10.157549858093262,10.717538833618164,9.8375244140625,12.808515548706055,7.541101932525635,12.504867553710938,8.57244873046875,9.36382007598877,8.369813919067383,11.599685668945312,11.583316802978516,11.098569869995117,9.756793975830078,11.193775177001953,9.891193389892578,7.575981616973877,10.072001457214355,11.563634872436523,11.880683898925781,11.027556419372559,11.867864608764648,12.01879596710205,11.671823501586914,10.513275146484375,9.708345413208008,11.871529579162598,9.416545867919922,11.120500564575195,7.8610920906066895,10.090686798095703,7.520668983459473,9.521385192871094,9.327044486999512,10.366032600402832,8.318497657775879,7.572169303894043,7.227772235870361,10.08541488647461,9.636529922485352,10.492273330688477,7.8703532218933105,9.98565673828125,7.924447059631348,11.78172779083252,9.682876586914062,7.230936050415039,9.691027641296387,10.908222198486328,9.95187759399414],\"y\":[17.130407333374023,15.242588996887207,18.156972885131836,13.945013999938965,18.183246612548828,16.522594451904297,13.792008399963379,17.76412582397461,13.9364595413208,16.444278717041016,17.12752914428711,12.694319725036621,18.33696746826172,18.376571655273438,13.816750526428223,15.813411712646484,17.116870880126953,15.016237258911133,16.473363876342773,17.17507553100586,15.90441608428955,14.932734489440918,18.656272888183594,16.75935173034668,16.835790634155273,12.83503532409668,13.267045021057129,16.753488540649414,16.312685012817383,16.627073287963867,15.928430557250977,17.970094680786133,15.197748184204102,16.428077697753906,16.346311569213867,17.294050216674805,17.92656135559082,13.190528869628906,18.173572540283203,17.820234298706055,14.09328842163086,16.213783264160156,16.258543014526367,17.031204223632812,15.105186462402344,17.175098419189453,14.393728256225586,15.286035537719727,16.274892807006836,14.0379056930542,17.571598052978516,16.841707229614258,15.151168823242188,17.865032196044922,17.24508285522461,15.882762908935547,16.801366806030273,12.701301574707031,16.49334716796875,16.340511322021484,12.771114349365234,16.647380828857422,17.174100875854492,16.315858840942383,15.963534355163574,14.849411964416504,13.964420318603516,18.12087631225586,13.719048500061035,15.091435432434082,16.500967025756836,16.084230422973633,16.879655838012695,18.1706600189209,16.330184936523438,16.13767433166504,16.57061004638672,12.671895027160645,13.85472583770752,13.269942283630371,12.823992729187012,15.285246849060059,17.79814338684082,13.740296363830566,13.58568000793457,15.575826644897461,17.13791275024414,16.51490020751953,12.682551383972168,12.717214584350586,16.311622619628906,15.845272064208984,13.880614280700684,17.50786018371582,18.48887062072754,17.348541259765625,17.545448303222656,18.588539123535156,17.626562118530273,15.282157897949219,15.37553882598877,15.314615249633789,13.094463348388672,16.897729873657227,16.329103469848633,16.793439865112305,13.952282905578613,16.2268009185791,15.832430839538574,14.245810508728027,16.16214370727539,17.746559143066406,13.36635684967041,16.557703018188477,13.480179786682129,18.406370162963867,14.787686347961426,13.896186828613281,15.580263137817383,15.329660415649414,15.053628921508789,16.353416442871094,16.937496185302734,15.656076431274414,14.748860359191895,18.522485733032227,14.63987922668457,18.44158363342285,16.669652938842773,12.749568939208984,16.175552368164062,14.76143741607666,18.13565444946289,18.207277297973633,12.698820114135742,17.279598236083984,12.70396614074707,16.422204971313477,17.238727569580078,15.151555061340332,14.583200454711914,16.29218292236328,16.15013313293457,15.508950233459473,18.273176193237305,15.38545036315918,12.75380802154541,16.756576538085938,14.588882446289062,16.293010711669922,17.268220901489258,13.615376472473145,13.801447868347168,14.206551551818848,15.460420608520508,15.83681583404541,15.39874267578125,14.92054271697998,16.656658172607422,13.09325885772705,15.938170433044434,15.137184143066406,17.711917877197266,17.736116409301758,15.421029090881348,16.35105323791504,16.374515533447266,17.582361221313477,14.34863567352295,18.09361457824707,16.665071487426758,13.856316566467285,13.397027969360352,18.271223068237305,13.473359107971191,17.939664840698242,14.368403434753418,15.38662052154541,16.25055694580078,17.38172149658203,16.40632438659668,13.418924331665039,15.183752059936523,15.43554973602295,16.522829055786133,16.51839828491211,15.343489646911621,16.32261848449707,16.60420036315918,17.55360984802246,12.86166000366211,13.996926307678223,15.954397201538086,16.763389587402344,15.623974800109863,15.351302146911621,18.213592529296875,15.376291275024414,14.671625137329102,14.311612129211426,13.372078895568848,18.191640853881836,13.098763465881348,16.336015701293945,14.008654594421387,17.967397689819336,16.5020751953125,15.290897369384766,15.847756385803223,15.939116477966309,14.372918128967285,18.45836067199707,15.02132797241211,16.613922119140625,16.313045501708984,18.25199317932129,17.35631561279297,16.595020294189453,15.015552520751953,13.85394287109375,13.417998313903809,18.36864471435547,17.5504207611084,17.95650863647461,15.877978324890137,15.869032859802246,14.419120788574219,15.569340705871582,15.386213302612305,14.573806762695312,17.731163024902344,15.713058471679688,16.199810028076172,15.663426399230957,13.927684783935547,15.854207992553711,14.825825691223145,15.439400672912598,16.45347023010254,16.335037231445312,16.671627044677734,17.255563735961914,12.800210952758789,14.944951057434082,15.056014060974121,15.334283828735352,16.92372703552246,17.996734619140625,18.113265991210938,12.881369590759277,14.758150100708008,14.874309539794922,15.557226181030273,16.591005325317383,16.73297691345215,15.226366996765137,16.60980796813965,16.647733688354492,13.936735153198242,17.6911563873291,16.535907745361328,14.786956787109375,16.513093948364258,14.779942512512207,16.308794021606445,15.370594024658203,14.2570161819458,14.282697677612305,14.99342155456543,15.097430229187012,16.185319900512695,14.863402366638184,17.071502685546875,15.135451316833496,18.2418212890625,16.517454147338867,16.42389678955078,13.523465156555176,15.382654190063477,12.625661849975586,14.624918937683105,18.675817489624023,17.219898223876953,12.910113334655762,16.501066207885742,17.25237464904785,11.444428443908691,16.558019638061523,16.628976821899414,17.905839920043945,18.223220825195312,16.300657272338867,18.220857620239258,16.62578582763672,15.44719409942627,17.058874130249023,13.445691108703613,16.786462783813477,16.479717254638672,17.83938217163086,14.667505264282227,18.135419845581055,15.585482597351074,16.002140045166016,14.630292892456055,16.919981002807617,14.8316068649292,15.673846244812012,13.640189170837402,13.00353717803955,18.310976028442383,16.993789672851562,17.005212783813477,16.042686462402344,16.61334228515625,18.23891258239746,14.133277893066406,16.568389892578125,12.776272773742676,17.547130584716797,16.74736213684082,12.627588272094727,16.61151123046875,13.005219459533691,14.376399040222168,15.462817192077637,13.633320808410645,13.184998512268066,14.511234283447266,16.877601623535156,13.801673889160156,15.192708969116211,18.066822052001953,15.5215425491333,13.26025390625,12.682793617248535,18.49618911743164,16.924560546875,15.643293380737305,13.050836563110352,14.780889511108398,16.520095825195312,15.321043014526367,16.118812561035156,16.20198631286621,13.038446426391602,17.976579666137695,15.318155288696289,14.265249252319336,17.878192901611328,16.812641143798828,13.803013801574707,13.867847442626953,14.803793907165527,13.053915977478027,14.665236473083496,13.716394424438477,15.321883201599121,16.29210090637207,18.668519973754883,16.675525665283203,13.881149291992188,17.268192291259766,14.316753387451172,16.06045150756836,13.581974029541016,18.183164596557617,15.340812683105469,16.60311508178711,16.993207931518555,14.86429214477539,15.05419921875,16.414274215698242,17.573627471923828,18.004648208618164,14.299723625183105,16.846023559570312,17.95948600769043,14.972681045532227,14.90243911743164,15.829007148742676,16.103151321411133,16.13363265991211,13.729108810424805,12.70407485961914,18.24927520751953,12.764877319335938,17.93182945251465,16.7111873626709,13.072709083557129,18.0262508392334,16.669553756713867,18.109731674194336,17.272668838500977,16.96578025817871,15.091486930847168,16.579105377197266,15.028735160827637,16.613508224487305,15.388728141784668,17.37525749206543,14.716521263122559,17.255643844604492,17.088924407958984,16.121219635009766,18.327919006347656,16.625816345214844,17.912996292114258,16.89813804626465,18.27943992614746,17.25901985168457,13.53744125366211,16.27654266357422,16.695228576660156,15.647428512573242,18.46503257751465,17.244609832763672,15.097312927246094,14.71088981628418,18.074811935424805,16.24222755432129,16.866554260253906,16.582786560058594,15.667393684387207,16.915922164916992,16.040864944458008,13.072964668273926,16.770421981811523,18.267242431640625,18.241579055786133,16.4874324798584,13.092131614685059,13.529618263244629,17.96342658996582,14.020862579345703,16.61765480041504,14.627154350280762,16.2896671295166,13.897690773010254,18.05272102355957,18.619001388549805,16.29261016845703,16.75236701965332,14.347753524780273,14.505550384521484,15.81753158569336,15.723769187927246,13.876482009887695,18.045761108398438,14.01656436920166,18.50287437438965,14.670431137084961,15.963247299194336,16.113203048706055,17.225170135498047,16.502979278564453,16.587352752685547,16.135793685913086,13.117042541503906,14.145987510681152,14.227272987365723,16.164173126220703,15.331672668457031,18.557605743408203,16.29280662536621,17.832168579101562,15.741816520690918,16.262434005737305,14.324318885803223,13.742738723754883,14.892614364624023,15.799468994140625,13.131207466125488,12.860279083251953,15.810323715209961,16.038902282714844,13.868094444274902,16.706378936767578,14.704998016357422,14.019475936889648,12.731528282165527,16.808595657348633,18.30414390563965,18.351057052612305,15.2905855178833,13.582043647766113,15.430403709411621,18.76708221435547,18.114511489868164,16.444433212280273,15.960952758789062,16.904386520385742,16.491411209106445,16.516324996948242,18.697111129760742,14.638666152954102,13.730981826782227,17.464950561523438,12.877582550048828,12.684647560119629,17.21722984313965,16.00387954711914,16.2678165435791,15.317845344543457,13.279927253723145,17.467607498168945,14.389406204223633,17.594623565673828,18.23767852783203,17.027210235595703,16.08915901184082,13.349869728088379,15.29914665222168,12.6449556350708,16.863969802856445,18.225830078125,15.377372741699219,13.414254188537598,13.966374397277832,14.019970893859863,15.098855018615723,14.886899948120117,18.292593002319336,15.559270858764648,15.549111366271973,16.283803939819336,17.186294555664062,13.082263946533203,15.81100082397461,16.59027671813965,15.43862533569336,15.640454292297363,14.396112442016602,18.06043815612793,17.389677047729492,13.248092651367188,14.466997146606445,15.888479232788086,18.623512268066406,16.48944854736328,15.021000862121582,17.6729736328125,17.69081687927246,13.229074478149414,16.57861328125,14.399120330810547,14.440225601196289,15.546338081359863,16.230392456054688,14.633519172668457,16.878381729125977,13.314666748046875,15.650309562683105,13.136094093322754,12.731696128845215,15.742218971252441,13.933797836303711,14.776795387268066,14.914772033691406,17.292428970336914,12.702109336853027,13.515299797058105,14.698047637939453,18.273603439331055,16.96854019165039,13.509481430053711,13.897007942199707,16.61884307861328,16.961763381958008,16.304826736450195,16.563371658325195,18.422897338867188,15.650995254516602,17.54928207397461,16.392005920410156,15.181645393371582,16.04147720336914,17.30010414123535,16.01848602294922,18.006175994873047,16.98419761657715,18.197513580322266,14.676946640014648,16.592424392700195,16.227331161499023,16.059673309326172,14.4837007522583,14.002603530883789,15.803058624267578],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"ICTC 2020 - 11th International Conference on ICT Convergence: Data, Network, and AI in the Age of Untact\",\"2nd International Conference on Soft Computing and Signal Processing, ICSCSP 2019\",\"2nd World Conference on Intelligent and 3D Technologies, WCI3DT 2023\",\"International Conference on Image, Signal Processing, and Pattern Recognition, ISPP 2023\",\"17th International Conference on Image Analysis and Recognition, ICIAR 2020\",\"27th International Workshop on Frontiers of Computer Vision, IW-FCV 2021\",\"10th International Conference on Articulated Motion and Deformable Objects, AMDO 2018\",\"International Conference on Cyberspace Data and Intelligence, CyberDI 2020 and the International Conference on Cyber-Living, Cyber-Syndrome and Cyber-Health, CyberLife 2020\",\"13th European Conference on Computer Vision, ECCV 2014\",\"12th International Conference on International Conference on Computational Collective Intelligence, ICCCI 2020\",\"Review of multimodal data processing techniques with limited data; [\\u6570\\u636e\\u53d7\\u9650\\u6761\\u4ef6\\u4e0b\\u7684\\u591a\\u6a21\\u6001\\u5904\\u7406\\u6280\\u672f\\u7efc\\u8ff0]\",\"Joint IAPR International Workshops on Structural, Syntactic and Statistical Techniques in Pattern Recognition, S+SSPR 2020\",\"14th European Conference on Computer Vision, ECCV 2016\",\"Proceedings of the 2022 10th International Conference on Information Technology: IoT and Smart City, ICIT 2022\",\"IVMSP 2022 - 2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop\",\"12th Asian Conference on Computer Vision, ACCV 2014\",\"7th Future Technologies Conference, FTC 2022\",\"7th Future Technologies Conference, FTC 2022\",\"International Conference on Image, Vision and Intelligent Systems, ICIVIS 2021\",\"9th Computer Science On-line Conference, CSOC 2020\",\"7th Future Technologies Conference, FTC 2022\",\"Graph Convolutional Neural Networks for\\u00a0Automated Echocardiography View Recognition: A Holistic Approach\",\"2nd CAAI International Conference on Artificial Intelligence, CICAI 2022\",\"Fourteenth International Conference on Digital Image Processing, ICDIP 2022\",\"ICBBT 2021 - Proceedings of 2021 13th International Conference on Bioinformatics and Biomedical Technology\",\"20th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2020\",\"Proceedings - 2022 4th International Symposium on Smart and Healthy Cities, ISHC 2022\",\"7th International Conference on Intelligence Science and Big Data Engineering, IScIDE 2017\",\"Fifth International Conference on Computer Information Science and Artificial Intelligence, CISAI 2022\",\"Proceedings - IWIS 2023: 3rd International Workshop on Intelligent Systems\",\"20th International Conference on Intelligent Data Engineering and Automated Learning, IDEAL 2019\",\"16th International Conference on Image Analysis and Recognition, ICIAR 2019\",\"Proceedings of the 5th International Conference on Medical Imaging with Deep Learning, MIDL 2022\",\"3rd International Conference on Advances in Information Communication Technology and Computing, AICTC 2021\",\"15th China Conference on Wireless Sensor Networks, CWSN 2021\",\"23rd International Conference on Human-Computer Interaction,  HCII 2021\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"Artificial Intelligence Applications\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"Artificial Intelligence Applications\"],\"textfont\":{\"size\":12},\"x\":[6.392279148101807,6.4394073486328125,6.580913543701172,6.49018669128418,6.4916911125183105,6.482137203216553,6.477964401245117,6.476766586303711,6.465845108032227,6.597690582275391,6.466947555541992,6.417304039001465,6.496275901794434,6.469002723693848,6.655515670776367,6.4872965812683105,6.635563373565674,6.631137847900391,6.552948951721191,6.443665504455566,6.637134552001953,6.431976795196533,6.403865814208984,6.754815101623535,6.447086811065674,6.569023132324219,9.701627731323242,6.589508533477783,6.380446910858154,6.63375997543335,6.487536907196045,6.511723518371582,6.515700817108154,6.43228006362915,6.422022819519043,6.650593280792236,6.603322982788086],\"y\":[11.977395057678223,11.858168601989746,11.530220985412598,11.80083179473877,11.409085273742676,11.443511009216309,11.375175476074219,11.809497833251953,11.401045799255371,11.997190475463867,11.705695152282715,11.475788116455078,11.364388465881348,11.769881248474121,12.08887004852295,11.39260196685791,12.067450523376465,12.067500114440918,11.63780403137207,11.872909545898438,12.079300880432129,11.496306419372559,11.667774200439453,12.10916805267334,11.386392593383789,11.478367805480957,12.580636024475098,11.906686782836914,11.869732856750488,11.942208290100098,11.805156707763672,11.627403259277344,11.629294395446777,11.801044464111328,11.925859451293945,11.479371070861816,11.745270729064941],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"GoPose: 3D Human pose estimation using WiFi\",\"VirTeach: mmWave Radar Point Cloud Based Pose Estimation With Virtual Data as a Teacher\",\"Radar-based 3d skeleton estimation enhanced with joint temporal-spatial constraints\",\"Seeing the unseen: Wifi-based 2D human pose estimation via an evolving attentive spatial-Frequency network\",\"Fast 3D Human Pose Estimation Using RF Signals\",\"3D Human Pose Estimation Using WiFi Signals\",\"RFPose-OT: RF-based 3D human pose estimation via optimal transport theory; [RFPose-OT: \\u57fa\\u4e8e\\u6700\\u4f18\\u4f20\\u8f93\\u7406\\u8bba\\u7684\\u65e0\\u7ebf\\u4e09\\u7ef4\\u4eba\\u4f53\\u59ff\\u6001\\u4f30\\u8ba1]\",\"RF-based Multi-view Pose Machine for Multi-Person 3D Pose Estimation\",\"Subject-adaptive skeleton tracking with RFID\",\"UNPOSED: an Ultra-wideband Network for Pose Estimation with Deep Learning\",\"RPM 2.0: RF-Based Pose Machines for Multi-Person 3D Pose Estimation\",\"mmPose-FK: A Forward Kinematics Approach to Dynamic Skeletal Pose Estimation Using mmWave Radars\",\"Accurate Human Pose Estimation using RF Signals\",\"RPM: RF-Based Pose Machines\",\"Spatial-temporal Multi-scale Constrained Learning for mmWave-based Human Pose Estimation\",\"A Joint Global-Local Network for Human Pose Estimation With Millimeter Wave Radar\",\"A Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\",\"RFID-Pose: Vision-Aided Three-Dimensional Human Pose Estimation with Radio-Frequency Identification\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"\\\"RF Human Pose Estimation Systems\\\"\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\\\"RF Human Pose Estimation Systems\\\"\"],\"textfont\":{\"size\":12},\"x\":[5.4272685050964355,5.053531646728516,5.095676422119141,7.932219505310059,5.243432521820068,5.455278396606445,5.270248889923096,5.15263032913208,5.302584648132324,5.308743000030518,5.203944206237793,5.090936183929443,5.1966681480407715,5.214097499847412,5.044144153594971,5.04633092880249,5.191851615905762,5.2845234870910645,5.361894607543945],\"y\":[13.697718620300293,13.375659942626953,13.414752960205078,14.862242698669434,13.522293090820312,13.72041130065918,13.55647087097168,13.45406723022461,13.583927154541016,13.587791442871094,13.497991561889648,13.410515785217285,13.490921020507812,13.5060396194458,13.370028495788574,13.369140625,13.487415313720703,13.569138526916504,13.582029342651367],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"Multi-camera, multi-person, and real-Time fall detection using long short term memory\",\"An Estimation of Pedestrian Action on Footbridges Using Computer Vision Approaches\",\"Fall Detection Method Based on Pose Estimation Using GRU\",\"ReferPose: Distance Optimization-Based Reference Learning for Human Pose Estimation and Monitoring\",\"Pose Estimation for Future Prediction of Falling\",\"Human Pose Estimation Using MediaPipe Pose and Optimization Method Based on a Humanoid Model\",\"Fall Recognition Based on Human Skeleton in Video\",\"Reliablization of Fall Recognition via Morphological Analysis\",\"A Study of Fall Detection System Using Context Cognition Method\",\"Exploring Human Pose Estimation and the Usage of Synthetic Data for Elderly Fall Detection in Real-World Surveillance\",\"Real-time fall detection algorithm based on pose estimation; [\\u57fa\\u4e8e\\u59ff\\u6001\\u4f30\\u8ba1\\u7684\\u5b9e\\u65f6\\u8dcc\\u5012\\u68c0\\u6d4b\\u7b97\\u6cd5]\",\"Forward dynamics computational modelling of a cyclist fall with the inclusion of protective response using deep learning-based human pose estimation\",\"Multi-Stream Deep Convolutional Network Using High-Level Features Applied to Fall Detection in Video Sequences\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"Fall Detection and Recognition in Elderly Persons\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"Fall Detection and Recognition in Elderly Persons\"],\"textfont\":{\"size\":12},\"x\":[13.816629409790039,13.069615364074707,13.814709663391113,13.559921264648438,13.840696334838867,13.658806800842285,13.835704803466797,13.824695587158203,13.83071231842041,13.81735897064209,13.82032299041748,13.84906005859375,13.81513500213623,13.734874725341797],\"y\":[13.555066108703613,13.434412002563477,13.553438186645508,13.771265983581543,13.537127494812012,13.631936073303223,13.557488441467285,13.56222152709961,13.550153732299805,13.548933982849121,13.567590713500977,13.535361289978027,13.564377784729004,13.566874504089355],\"type\":\"scattergl\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"DriPE: A Dataset for Human Pose Estimation in Real-World Driving Settings\",\"Simple pair pose - Pairwise human pose estimation in dense urban traffic scenes\",\"When Vehicles See Pedestrians with Phones: A Multicue Framework for Recognizing Phone-Based Activities of Pedestrians\",\"In-cabin vehicle synthetic data to test deep learning based human pose estimation models\",\"A Trend of 2D Human Pose Estimation Base on Deep Learning\",\"Driver activity recognition using deep learning and human pose estimation\",\"Survey on In-vehicle Datasets for Human Pose Estimation\",\"Sensing, perception and decision for deep learning based autonomous driving\",\"VehiPose: A multi-scale framework for vehicle pose estimation\",\"Pedestrians and their phones - Detecting phone-based activities of pedestrians for autonomous vehicles\",\"A Lightweight Model for Driver Distraction Classification\",\"Fast and compact driver pose estimation\",null],\"marker\":{\"opacity\":0.5,\"size\":5},\"mode\":\"markers+text\",\"name\":\"Autonomous Vehicle Monitoring using Pose Estimation\",\"text\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"Autonomous Vehicle Monitoring using Pose Estimation\"],\"textfont\":{\"size\":12},\"x\":[7.869612693786621,7.938770771026611,7.743155002593994,7.746524333953857,7.88781213760376,7.7322893142700195,7.70018196105957,7.6269612312316895,7.820427417755127,7.739297866821289,7.7396674156188965,7.749186992645264,7.7744903564453125],\"y\":[14.033392906188965,14.157629013061523,13.899438858032227,13.90884780883789,14.054794311523438,13.890986442565918,13.911568641662598,13.973089218139648,14.026897430419922,13.896292686462402,13.894865989685059,13.909424781799316,13.963101387023926],\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":10.106970798969268,\"x1\":10.106970798969268,\"y0\":6.8509444236755375,\"y1\":21.58214454650879},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":4.287522530555725,\"x1\":15.926419067382813,\"y0\":14.216544485092165,\"y1\":14.216544485092165}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":4.287522530555725,\"y\":14.216544485092165,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":10.106970798969268,\"xshift\":10,\"y\":21.58214454650879}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"width\":1920,\"height\":1080,\"xaxis\":{\"visible\":false},\"yaxis\":{\"visible\":false}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('14c2187d-6c87-4b8d-875a-3e50c7cbeb0a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics(title=\"\", custom_labels=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "g4e_tiAo5H3s",
        "outputId": "65511766-9fb3-4fbc-dfe4-832d7a5d1e40"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"25d44126-7d43-443d-9294-624c106e192d\" class=\"plotly-graph-div\" style=\"height:650px; width:650px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"25d44126-7d43-443d-9294-624c106e192d\")) {                    Plotly.newPlot(                        \"25d44126-7d43-443d-9294-624c106e192d\",                        [{\"customdata\":[[0,\"Human Pose Estimation using Deep Learning\",588],[1,\"Artificial Intelligence Applications\",36],[2,\"\\\"RF Human Pose Estimation Systems\\\"\",18],[3,\"Fall Detection and Recognition in Elderly Persons\",13],[4,\"Autonomous Vehicle Monitoring using Pose Estimation\",12]],\"hovertemplate\":\"\\u003cb\\u003eTopic %{customdata[0]}\\u003c\\u002fb\\u003e\\u003cbr\\u003e%{customdata[1]}\\u003cbr\\u003eSize: %{customdata[2]}\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#B0BEC5\",\"size\":[588,36,18,13,12],\"sizemode\":\"area\",\"sizeref\":0.3675,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":2}},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[1.084555983543396,1.2486228942871094,0.7302947640419006,1.688169002532959,1.822168231010437],\"xaxis\":\"x\",\"y\":[6.122378349304199,2.965837240219116,5.481418132781982,4.490950584411621,5.826414585113525],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"},\"visible\":false,\"range\":[0.6207505494356156,2.0954934656620026]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"},\"visible\":false,\"range\":[2.520961654186249,7.040735101699829]},\"legend\":{\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":650,\"height\":650,\"sliders\":[{\"active\":0,\"pad\":{\"t\":50},\"steps\":[{\"args\":[{\"marker.color\":[[\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 0\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 1\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 2\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\"]]}],\"label\":\"Topic 3\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\"]]}],\"label\":\"Topic 4\",\"method\":\"update\"}]}],\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":1.358122007548809,\"x1\":1.358122007548809,\"y0\":2.520961654186249,\"y1\":7.040735101699829},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":0.6207505494356156,\"x1\":2.0954934656620026,\"y0\":4.780848377943039,\"y1\":4.780848377943039}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":0.6207505494356156,\"y\":4.780848377943039,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":1.358122007548809,\"xshift\":10,\"y\":7.040735101699829}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('25d44126-7d43-443d-9294-624c106e192d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
        "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics,\n",
        "                                title = \"\",\n",
        "                                custom_labels = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "petr4b5TQe1A",
        "outputId": "5c824ca8-c8c5-4d05-8972-5b6bb904a178"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 155.49it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"9f8e8b87-79a3-4602-83d6-5c2d6cb7468b\" class=\"plotly-graph-div\" style=\"height:275px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9f8e8b87-79a3-4602-83d6-5c2d6cb7468b\")) {                    Plotly.newPlot(                        \"9f8e8b87-79a3-4602-83d6-5c2d6cb7468b\",                        [{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"Artificial Intelligence Applications\",\"\",\"\",\"Human Pose Estimation using Deep Learning\"],\"x\":[0.0,0.44349908384446235,0.44349908384446235,0.0],\"xaxis\":\"x\",\"y\":[-25.0,-25.0,-35.0,-35.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"pose_human_estimation_3d_learning\",\"\",\"\",\"\\\"RF Human Pose Estimation Systems\\\"\"],\"x\":[0.44349908384446235,0.6401462624280109,0.6401462624280109,0.0],\"xaxis\":\"x\",\"y\":[-30.0,-30.0,-45.0,-45.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"Autonomous Vehicle Monitoring using Pose Estimation\",\"\",\"\",\"pose_human_estimation_3d_learning\"],\"x\":[0.0,0.7020395203174933,0.7020395203174933,0.6401462624280109],\"xaxis\":\"x\",\"y\":[-15.0,-15.0,-37.5,-37.5],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"text\":[\"Fall Detection and Recognition in Elderly Persons\",\"\",\"\",\"pose_human_estimation_3d_learning\"],\"x\":[0.0,0.7424948017858714,0.7424948017858714,0.7020395203174933],\"xaxis\":\"x\",\"y\":[-5.0,-5.0,-26.25,-26.25],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"pose_human_estimation_3d_learning\"],\"marker\":{\"color\":\"black\"},\"mode\":\"markers\",\"showlegend\":false,\"x\":[0.44349908384446235],\"y\":[-30.0],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"pose_human_estimation_3d_learning\",\"pose_human_estimation_3d_learning\"],\"marker\":{\"color\":\"black\"},\"mode\":\"markers\",\"showlegend\":false,\"x\":[0.6401462624280109,0.7020395203174933],\"y\":[-37.5,-26.25],\"type\":\"scatter\"}],                        {\"autosize\":false,\"height\":275,\"hovermode\":\"closest\",\"showlegend\":false,\"width\":1000,\"xaxis\":{\"mirror\":\"allticks\",\"rangemode\":\"tozero\",\"showgrid\":false,\"showline\":true,\"showticklabels\":true,\"ticks\":\"outside\",\"type\":\"linear\",\"zeroline\":false},\"yaxis\":{\"mirror\":\"allticks\",\"rangemode\":\"tozero\",\"showgrid\":false,\"showline\":true,\"showticklabels\":true,\"tickmode\":\"array\",\"ticks\":\"outside\",\"ticktext\":[\"Fall Detection and Recognition in Elderly Persons\",\"Autonomous Vehicle Monitoring using Pose Estimation\",\"Artificial Intelligence Applications\",\"Human Pose Estimation using Deep Learning\",\"\\\"RF Human Pose Estimation Systems\\\"\"],\"tickvals\":[-5.0,-15.0,-25.0,-35.0,-45.0],\"type\":\"linear\",\"zeroline\":false,\"range\":[-50.0,0.0]},\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"plot_bgcolor\":\"#ECEFF1\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9f8e8b87-79a3-4602-83d6-5c2d6cb7468b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3kWzwjQQgTb",
        "outputId": "a7e95812-91fd-4b6e-8d26-82895adc4da0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "├─■──fall_falls_elderly_detection_falling ── Topic: 3\n",
            "└─pose_human_estimation_3d_learning\n",
            "     ├─■──driver_pose_estimation_driving_vehicle ── Topic: 4\n",
            "     └─pose_human_estimation_3d_learning\n",
            "          ├─pose_human_estimation_3d_learning\n",
            "          │    ├─■──learning_deep_detection_data_analysis ── Topic: 1\n",
            "          │    └─■──pose_human_estimation_3d_deep ── Topic: 0\n",
            "          └─■──signals_rf_human_radar_pose ── Topic: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart(n_words=10\n",
        "                               , title = \"\",\n",
        "                               custom_labels = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "M9_dLMZDQh6m",
        "outputId": "9539fb0c-3fb5-4d29-ec6b-9b285664c370"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"a61c01a7-ff3c-4028-955a-1342c665296e\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a61c01a7-ff3c-4028-955a-1342c665296e\")) {                    Plotly.newPlot(                        \"a61c01a7-ff3c-4028-955a-1342c665296e\",                        [{\"marker\":{\"color\":\"#D55E00\"},\"orientation\":\"h\",\"x\":[0.02003278267626544,0.020211548116757957,0.02541853009228379,0.025748074334137696,0.02617286255644072,0.02697706932358405,0.03220711042472507,0.045224783529593374,0.052702551021676196,0.05540189678420965],\"y\":[\"data  \",\"results  \",\"learning  \",\"network  \",\"model  \",\"deep  \",\"3d  \",\"estimation  \",\"human  \",\"pose  \"],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"#0072B2\"},\"orientation\":\"h\",\"x\":[0.029404543253022754,0.03001480991958692,0.030233600268430208,0.030608453662510735,0.034963112090556574,0.03636803225057846,0.041191703639380906,0.04446620711726439,0.04479686636644841,0.06736140424423143],\"y\":[\"segmentation  \",\"classification  \",\"network  \",\"neural  \",\"topics  \",\"analysis  \",\"data  \",\"detection  \",\"deep  \",\"learning  \"],\"type\":\"bar\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"#CC79A7\"},\"orientation\":\"h\",\"x\":[0.03598362110230563,0.03604509749460172,0.043029302553659485,0.04838517246060655,0.04897463827145862,0.0597557464273965,0.0628675572917759,0.07024174225325132,0.08762221062619859,0.1099193279848739],\"y\":[\"rpm  \",\"system  \",\"body  \",\"estimation  \",\"3d  \",\"pose  \",\"radar  \",\"human  \",\"rf  \",\"signals  \"],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"#E69F00\"},\"orientation\":\"h\",\"x\":[0.02896527419297046,0.029153258416095368,0.029383778491757863,0.03333921773371696,0.038279263494673234,0.03859516393165097,0.04321287039704738,0.06254561450212541,0.10141135729804328,0.14865214094261608],\"y\":[\"used  \",\"recognition  \",\"paper  \",\"activities  \",\"human  \",\"falling  \",\"detection  \",\"elderly  \",\"falls  \",\"fall  \"],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"marker\":{\"color\":\"#56B4E9\"},\"orientation\":\"h\",\"x\":[0.031600647745564696,0.03744991042171011,0.039502226025184904,0.04029007781800788,0.04393110880822073,0.04819072864725493,0.05131659043997371,0.056666919501729224,0.06153346603844324,0.18125644947178202],\"y\":[\"autonomous  \",\"deep  \",\"monitoring  \",\"human  \",\"distraction  \",\"vehicle  \",\"driving  \",\"estimation  \",\"pose  \",\"driver  \"],\"type\":\"bar\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.4],\"showgrid\":true},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,0.4],\"showgrid\":true},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.0,0.4],\"showgrid\":true},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.0,0.4],\"showgrid\":true},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 0\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 1\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 2\",\"x\":0.6375000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 3\",\"x\":0.9125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 4\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.4,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"showlegend\":false,\"width\":1000,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a61c01a7-ff3c-4028-955a-1342c665296e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestamps = papers['Year']\n",
        "topics_over_time = topic_model.topics_over_time(title_data, timestamps)\n",
        "topic_model.visualize_topics_over_time(topics_over_time,title=\"\",custom_labels = True,topics=topics,normalize_frequency=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "17rBa6Z7AqML",
        "outputId": "9342db7f-65cc-4648-9a08-c2b617d1377b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11it [00:00, 58.97it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"9a402abd-3197-4c7a-a0eb-3d4e2bd525c7\" class=\"plotly-graph-div\" style=\"height:450px; width:1250px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9a402abd-3197-4c7a-a0eb-3d4e2bd525c7\")) {                    Plotly.newPlot(                        \"9a402abd-3197-4c7a-a0eb-3d4e2bd525c7\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic -1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: guru, yoga, methods, correction, service\",\"\\u003cb\\u003eTopic -1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: yoga, yopose, grading, tool, automatic\",\"\\u003cb\\u003eTopic -1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: yoga, based, assistant, system, and\"],\"marker\":{\"color\":\"#E69F00\"},\"mode\":\"lines\",\"name\":\"Yoga Pose Detection System\",\"x\":[2021,2022,2023],\"y\":[0.42857142857142855,0.2857142857142857,0.8571428571428571],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: multi, estimation, deep, learning, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: learning, convolutional, multi, heterogeneous, structured\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, estimation, deep, learning\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: estimation, pose, networks, human, deep\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, pose, and, estimation, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, pose, estimation, human, and\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, and, pose, estimation, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: and, based, pose, human, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, and, estimation, pose, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, and, estimation, human, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, in, estimation, pose, human\"],\"marker\":{\"color\":\"#56B4E9\"},\"mode\":\"lines\",\"name\":\"Human Pose Estimation using Deep Learning\",\"x\":[2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024],\"y\":[0.021214646080581503,0.0127287876483489,0.05515807980951191,0.0509151505933956,0.1569883809963031,0.27579039904755953,0.3182196912087225,0.45823635534056045,0.5218802935823049,0.5515807980951191,0.0721297966739771],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: 2014, conference, european, vision, computer\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: european, 2016, conference, vision, computer\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: engineering, and, big, international, 2017\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: and, deformable, international, articulated, conference\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: and, international, conference, 2019, ideal\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: conference, international, and, 2020, cyber\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: international, 2021, conference, and, syntactic\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: international, 2022, conference, and, the\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: conference, international, 2022, technologies, future\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: 2nd, and, conference, world, technologies\"],\"marker\":{\"color\":\"#009E73\"},\"mode\":\"lines\",\"name\":\"Artificial Intelligence Applications\",\"x\":[2015,2016,2017,2018,2019,2020,2021,2022,2023,2024],\"y\":[0.13608276348795434,0.06804138174397717,0.06804138174397717,0.06804138174397717,0.13608276348795434,0.4762896722078402,0.34020690871988585,0.6123724356957945,0.4762896722078402,0.06804138174397717],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rfid, subject, adaptive, skeleton, tracking\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rfid, radio, wifi, frequency, dimensional\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: gopose, wifi, rf, signals, accurate\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, rf, multi, estimation, radar\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, mmwave, rpm, rf, machines\"],\"marker\":{\"color\":\"#F0E442\"},\"mode\":\"lines\",\"name\":\"\\\"RF Human Pose Estimation Systems\\\"\",\"x\":[2020,2021,2022,2023,2024],\"y\":[0.09712858623572641,0.19425717247145283,0.19425717247145283,0.8741572761215377,0.38851434494290565],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: to, in, footbridges, fall, multi\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, fall, real, algorithm, time\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, multi, detection, based, gru\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, in, based, elderly, recognition\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, and, optimization, referpose, humanoid\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, protective, inclusion, cyclist, fall\"],\"marker\":{\"color\":\"#D55E00\"},\"mode\":\"lines\",\"name\":\"Fall Detection and Recognition in Elderly Persons\",\"x\":[2019,2020,2021,2022,2023,2024],\"y\":[0.3380617018914066,0.1690308509457033,0.50709255283711,0.6761234037828132,0.3380617018914066,0.1690308509457033],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pedestrians, based, phones, phone, and\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: based, pedestrians, see, phones, phone\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: compact, and, driver, fast, estimation\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: in, vehicle, pose, estimation, driver\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: in, trend, vehicle, survey, estimation\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: distraction, driver, lightweight, classification, model\"],\"marker\":{\"color\":\"#0072B2\"},\"mode\":\"lines\",\"name\":\"Autonomous Vehicle Monitoring using Pose Estimation\",\"x\":[2016,2018,2020,2021,2022,2023],\"y\":[0.16666666666666666,0.3333333333333333,0.16666666666666666,0.8333333333333334,0.3333333333333333,0.16666666666666666],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"xaxis\":{\"showgrid\":true},\"yaxis\":{\"showgrid\":true,\"title\":{\"text\":\"Normalized Frequency\"}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\",\"y\":0.95,\"x\":0.4,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":1250,\"height\":450,\"legend\":{\"title\":{\"text\":\"\\u003cb\\u003eGlobal Topic Representation\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9a402abd-3197-4c7a-a0eb-3d4e2bd525c7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = papers['Document Type']\n",
        "topics_per_class = topic_model.topics_per_class(docs, classes)\n",
        "topic_model.visualize_topics_per_class(topics_per_class,custom_labels = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "CcVPyI48OEVB",
        "outputId": "047bf7cb-26a6-487a-d001-5e65dae21580"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8it [00:00, 46.04it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"6c0f0d16-198d-4d69-8951-fb0584007342\" class=\"plotly-graph-div\" style=\"height:900px; width:1250px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6c0f0d16-198d-4d69-8951-fb0584007342\")) {                    Plotly.newPlot(                        \"6c0f0d16-198d-4d69-8951-fb0584007342\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, 3d, model\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, learning, reconstruction, pose, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, estimation, chapter, ai\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: algorithm, detection, human, feature, filter\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: tensor, ml, analysis, data, browser\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, hpe, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, 3d, deep\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: mesh, 3d, reconstruction, meshlifter, human\"],\"marker\":{\"color\":\"#E69F00\"},\"name\":\"Human Pose Estimation using Deep Learning\",\"orientation\":\"h\",\"visible\":true,\"x\":[250,3,7,1,2,11,313,1],\"y\":[\"Article\",\"Conference review\",\"Book chapter\",\"Retracted\",\"Book\",\"Review\",\"Conference paper\",\"Letter\"],\"type\":\"bar\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: learning, deep, detection, analysis, topics\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: multimodal, data, learning, model, labeling\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: cardiac, images, meshes, synthetic, heart\"],\"marker\":{\"color\":\"#56B4E9\"},\"name\":\"Artificial Intelligence Applications\",\"orientation\":\"h\",\"visible\":\"legendonly\",\"x\":[34,1,1],\"y\":[\"Conference review\",\"Review\",\"Conference paper\"],\"type\":\"bar\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: signals, rf, radar, human, pose\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: signals, rf, human, pose, 3d\"],\"marker\":{\"color\":\"#009E73\"},\"name\":\"\\\"RF Human Pose Estimation Systems\\\"\",\"orientation\":\"h\",\"visible\":\"legendonly\",\"x\":[11,7],\"y\":[\"Article\",\"Conference paper\"],\"type\":\"bar\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, falls, elderly, falling, joint\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, falls, accuracy, detection, classification\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, falls, elderly, detection, human\"],\"marker\":{\"color\":\"#F0E442\"},\"name\":\"Fall Detection and Recognition in Elderly Persons\",\"orientation\":\"h\",\"visible\":\"legendonly\",\"x\":[6,1,6],\"y\":[\"Article\",\"Book chapter\",\"Conference paper\"],\"type\":\"bar\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driver, pose, integral, distraction, estimation\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driver, pose, driving, estimation, vehicle\"],\"marker\":{\"color\":\"#D55E00\"},\"name\":\"Autonomous Vehicle Monitoring using Pose Estimation\",\"orientation\":\"h\",\"visible\":\"legendonly\",\"x\":[2,10],\"y\":[\"Article\",\"Conference paper\"],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"xaxis\":{\"showgrid\":true,\"title\":{\"text\":\"Frequency\"}},\"yaxis\":{\"showgrid\":true,\"title\":{\"text\":\"Class\"}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eTopics per Class\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.4,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":1250,\"height\":900,\"legend\":{\"title\":{\"text\":\"\\u003cb\\u003eGlobal Topic Representation\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6c0f0d16-198d-4d69-8951-fb0584007342');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = papers['Source title']\n",
        "topics_per_class = topic_model.topics_per_class(docs, classes)\n",
        "topic_model.visualize_topics_per_class(topics_per_class,custom_labels = True)"
      ],
      "metadata": {
        "id": "HAX_JXWeRhuo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "outputId": "030446a1-34c5-46d3-b4ca-a47d280ed4f5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "323it [00:03, 104.47it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"93a41732-47eb-40fb-98f5-34db21ce48a0\" class=\"plotly-graph-div\" style=\"height:900px; width:1250px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"93a41732-47eb-40fb-98f5-34db21ce48a0\")) {                    Plotly.newPlot(                        \"93a41732-47eb-40fb-98f5-34db21ce48a0\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: load, external, internal, fatigue, exertion\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, estimation, 3d, performance\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, estimation, network, information, module\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: 3d, gcns, human, gans, discriminator\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: markerbased, realtime, software, privacy, motion\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: scenarios, clinical, dataset, 3dpw, inbed\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, inbed, estimation, gait, irs\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, features, estimation, apn\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, 3d, estimation, deep\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: bowling, players, model, training, bowlingdl\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: 3d, pose, camerapose, 2d, estimators\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: bolt, exposed, loosening, weld, detection\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: estimation, jump, human, pose, knee\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pushups, human, endurance, system, movement\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: raw, successful, estimation, pose, interest\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: distributed, imm, human, constant, motion\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pedestrian, detection, tracking, algorithm, module\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: mouse, pose, estimation, real, data\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: barbell, weightlifting, yolov7, keypointrcnn, model\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: tof, detector, stage, network, image\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: distance, resolution, body, noise, low\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: patient, physical, athome, therapy, guidance\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, 2d, pose, network, video\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, network, estimation, module, performance\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: estimation, pose, human, limbs, rehabilitation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, recent, report, models\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: tug, patients, bradykinesia, parkinsons, disease\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: poses, motions, sequential, recovering, images\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: data, imu, har, cromosim, fidelity\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: structural, information, heatmaps, bone, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: construction, machines, model, yfp, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: point, cloud, pose, input, forming\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: cheating, candidates, behavior, examination, analyzing\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: gait, analysis, clinical, conventional, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: versions, operation, structures, hand, mobile\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, approaches, estimation, research\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: detection, keypoints, network, speed, structure\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: system, markerless, parameters, joint, position\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: reconstruction, coarse, 3d, supervision, multiinitialization\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: either, locations, unimodal, joint, datadriven\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: depth, data, color, alternative, datasets\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: edge, scheme, model, multiperson, end\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: global, branches, partbased, pedestrian, rank1\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: gait, variables, calculated, older, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: body, depth, inference, parts, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rss, spacing, inspection, module, keypoints\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: 3d, humancentered, 2d, automation, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: create, human, cause, heat, unit\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: upper, limb, goniometer, rom, model\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: map, 3d, 2d, depth, markless\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: chapter, pose, estimation, human, classical\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: remote, sensing, downscaleupscale, isprs, wgiii4\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, spie, images\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, 3d, adversarial, great, rgb\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: dance, video, students, movements, evaluation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: ik, orientation, angles, sensors, real\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: flow, body, optical, frame, relative\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: hourglass, obtained, network, paf, heatmap\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: processing, resolution, rootnet, heatmap, decreased\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: feature, scfham, ifrm, effectively, aggregation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, api, pose, sign, applications\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: vrar, asymmtric, anthropomorphic, inceptionv3, egocentric\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: hrnet, complexity, ghost, sagnet, prediction\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: algorithm, schemes, teachers, states, scenario\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: hpe, task, models, used, paper\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anomalous, actions, examination, stage, behavior\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: footfall, occ, beacons, deeplearningbased, indoor\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: excavator, pose, dataset, estimation, construction\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: residential, roofing, feature, pose, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: nearbyperson, occlusion, body, human, foreground\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: cehigherhrnet, higherhrnet, scale, persons, small\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: uncertainty, depth, edl, calibrate, 2d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: dance, evaluation, quantitative, instruction, motion\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: gait, modelbased, database, oumvlppose, largescale\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: action, human, geometric, pose, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: exercise, trainers, home, model, professional\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: robot, pose, estimation, skeleton, keypoints\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, place, robot, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: poses, 3d, 2d, views, reference\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: cnn, classification, image, neural, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: crowd, 3d, pose, counting, categorized\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: keypoint, maskrcnn, features, mask, feature\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rating, icc, resting, tremor, nontrained\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, 3d, human, estimation, 2d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: medical, korea, rehabilitation, doctors, problems\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: insole, systems, optical, sensor, indoor\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: physical, therapy, models, lower, dataset\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: flow, body, optical, frame, relative\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: posture, human, vitposeb, estimation, pck\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: stereo, system, error, hpe, lengths\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: estimation, human, pose, problems, different\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: running, form, standards, application, motion\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: gait, classify, deeppose, kerastensorflow, anaconda\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: formula, mae, presented, markerbased, sporting\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: dl, visions, discover, mining, trends\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: mindspore, learning, deep, easy, execution\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: hpe, dlbased, ssdcnnhpe, human, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: behavior, temporal, abnormal, spatial, time\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pullups, test, intelligent, application, realize\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: 3d, human, pose, 2d, flow\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: frequency, tracking, motion, fast, kalman\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, estimation, hpe, caren, wholebody\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: viewpoints, learns, aaai, temporally, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, estimation, 2d, pte\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rgbd, paper, scenes, precision, calculation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, deep, learning\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: formula, presented, signs, vocabulary, 300\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: behavior, human, implementation, article, called\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: tensor, analysis, data, clustering, computation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pipelines, pose, application, deep, alpha\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: technical, estimation, human, pose, lack\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: martial, arts, traditional, cpm, joints\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: positioning, human, 3d, algorithm, phase\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: resnet50, position, joints, multistages, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: unconstrained, pose, 3d, estimation, amodal\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: minimum, human, mean, error, v2\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: paper, human, discussion, ideas, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, deep, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: model, computational, precision, average, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: ai, metaverse, ml, worlds, chapter\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: user, toolbox, animals, deeplabcut, behavioral\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: resnet50, position, joints, multistages, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: counting, multitask, exercise, reppenn, recognition\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: p2pmeshnet, pose, estimation, joint, network\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, 3d, estimation, 2d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: mobilenets, accuracy, network, mobile, achieved\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: important, human, pose, part, field\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, 3d, estimation, human, action\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: cheating, tools, games, video, defense\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: multiview, human, images, pose, dataset\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: posture, assessment, workers, dywhse, system\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: construction, workers, unsafe, behaviors, object\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, results, estimation, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: adlc, human, wgan, dts, senewhrnet\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, estimation, pose, 3d, learning\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: estimation, pose, human, representative, introduces\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, estimation, pose, motion, accuracy\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, stage, contextbased, structure\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: road, centerline, extraction, patch, cnn\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, 3d, springer\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: golf, analyses, swing, temporalbased, mpe\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: videos, sports, hpe, dcnn, classify\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: parallel, ensembles, linkages, heuristics, presented\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: le, would, aiuas, crowd, uass\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: encoded, lsaes, mfe, feature, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: probabilistic, deterministic, pose, estimation, networks\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: action, estimation, pose, semantic, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, information, cpm, pose, detected\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, estimation, human, model, module\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: regularisation, model, pose, estimation, pixels\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: multiview, pose, 3d, 2d, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: estimation, pose, development, human, multiperson\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, estimation, human, 3d, animal\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: 3d, estimation, pose, human, different\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: exercisecheck, height, pose, patient, deep\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: keyspeaker, speaker, detection, find, poseattention\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: model, action, layer, convolutional, localization\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: event, point, pointnet, cloud, rasterized\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: msrt, fam, transformers, highlevel, features\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: learns, constraints, network, pose, benchmarkdatasets\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: xhrnet, lightweight, which, decreases, parameters\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, underwater, robot, human, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: multilevel, structure, graph, model, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: attribute, divers, underwater, recognition, auv\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: direction, ldcnet, limb, cauchy, differentiated\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: hitter, baseball, coaching, rules, custom\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: multibranch, adopt, network, estimation, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: lhrnet, parameters, complexity, network, coco\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rehabilitation, gmm, physical, exercises, lbp\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: famous, human, pose, review, deep\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, estimation, accuracy, occlusion\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: geometric, evidence, pose, allowing, postprocessing\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, 3d, network, human, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: posture, human, body, development, joints\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: estimation, pose, probabilistic, transfer, loss\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: occlusion, visibility, part, human, keypointlevel\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: transformer, transnet, transformers, convolutional, vision\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: efficient, gflops, backbone, networks, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, yields, estimation, human, optimal\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: ml, browser, libraries, youll, gait\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: patient, pose, clinical, results, contact\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: poses, output, estimating, well, inaccuracy\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: viewsheds, improve, reidentification, pedestrian, public\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: athlete, athletes, system, pose, openpose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: cameras, multiagent, fixed, reinforcement, selection\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, retrieval, image, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: advantage, within, human, posture, field\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: snhrnet, singlehuman, network, nhrnet, hrnet\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: detection, speed, system, running, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: learningbased, mesh, hand, deep, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: reconstruction, disparity, hands, neural, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: inbed, behavior, monitoring, concerns, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: spectral, ir, lct, restoration, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pretraining, expensive, benchmarks, network, deep\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: articulation, research, point, summarized, bottomup\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: hpe, dataset, approaches, models, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: action, recognition, information, posebased, spatiotemporal\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: research, implementations, gpu, paradigm, deep\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: proportion, stimulus, judgment, mae, another\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, persons, pose, skeletonlike, depiction\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: gait, wfvd, carrying, covariates, recognition\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, mrcnn, human, ar, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: workflow, formula, medical, os, surgical\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: lmrcnn, parts, defined, relative, context\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, classifier, recognition, sent, nonocclusion\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, estimation, human, datasets, infant\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: model, human, pose, geometric, effectively\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: detection, postprocessing, deployed, scheme, experiment\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: abuse, convolution, reporting, human, shows\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: deepfake, creation, upper, language, world\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, vs, human, thermal, tis\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, har, human, estimation, input\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: hpe, model, pose, human, lightweight\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: time, stance, double, clinical, gait\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: keypoints, information, human, pose, priori\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: weakness, strokerelated, classify, body, model\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, estimation, model, learning\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, retargeting, estimation, partbased\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: walker, patients, pose, estimation, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, estimation, human, 3d, data\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: cnn, h150, prospected, neural, attributes\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, landmark, pose, posture, estimate\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: shoulder, knee, measurements, tilt, distance\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: activity, human, recognition, provide, discuss\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, 3d, estimation, pose, compares\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: joints, joint, pose, jre, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: virtual, reality, trainings, seated, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: research, action, recent, dominant, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, estimation, recognition, body\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: 2d, hpe, accuracy, quantization, superior\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: garments, customers, garment, obtain, fashion\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, machines, convolutional, semantic, segmentation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: sr, lr, accuracy, images, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: sequence, uplift, uplifts, ganuplift, skeleton\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: bones, loop, virtual, constraints, real\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: system, mobility, human, clean, tof\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: gait, assist, analysis, ataxic, diagnosing\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pe, movements, students, teaching, basic\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, target, committee, interdependence, tracked\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: movements, similar, cases, exercise, illustrates\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pipelines, estimation, discussed, survey, deep\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: face, subchallenge, image, cue, body\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: mts, loss, trajectory, classification, learning\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: efficientpose, cascaded, pose, singleperson, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, data, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: cnn, object, task, performance, issues\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: model, utkinect, movement, disorders, action\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: dance, dances, african, traditional, sinte\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: sensor, manipulating, jointspace, mattress, array\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: generator, discriminator, adversarial, backpropagates, organization\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, network, estimation, 3d, ctp\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, hpe, deep, estimation, recognition\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: matrix, 3d, computational, frames, trajectory\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, shots, partial, complete, images\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: welding, feature, weld, profile, points\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: problems, datasets, human, article, discussed\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: estimation, pose, twodimensional, feature, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: hrnet32, polarized, feature, estimation, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: 3d, pose, estimation, human, 2d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fingerspelling, dqn, system, viewpoint, writing\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: decoding, threedimensional, coding, realize, modules\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: key, points, occluded, human, accurately\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: memory, mbytes, deployable, required, accuracy\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: mocap, character, video, technology, 3d\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: algorithm, detection, human, feature, filter\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: matching, network, accuracy, high, estimation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: nlos, hidden, los, transients, hiddenpose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: avatar, metafi, metaverse, illumination, home\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: architecture, human, variations, activity, various\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: hpe, variety, downstream, discusses, human\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: model, sac, adi, ra, cervical\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: detection, human, pose, many, applications\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fullyconnected, appearance, granularity, body, spatial\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: mechanism, occlusionaware, pedestrians, trajectory, occluded\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, wmsd, estimation, loadbearing, dynamicity\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: author, research, several, estimation, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: data, imu, har, cromosim, fidelity\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: ici, service, computing, mobile, services\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: constraints, navigation, networked, inertial, kinematic\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: model, alstmlstm, deep, learning, brief\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: interactive, setup, human, movement, art\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: higherlevel, lowlevel, spatial, many, architecture\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: capture, motion, films, characters, animation\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: developed, lumbar, improper, points, work\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: human, pose, estimation, device, model\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: body, nbf, model, 2d, endtoend\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: robot, humanrobot, motions, interaction, skills\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, 3d, body\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: activity, recognition, model, estimation, pose\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: posesonic, semiinthewild, acoustic, body, setting\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, estimation, human, cameras, deep\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: pose, human, estimation, network, two\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: ashn, l2, focal, network, convolutional\"],\"marker\":{\"color\":\"#E69F00\"},\"name\":\"Human Pose Estimation using Deep Learning\",\"orientation\":\"h\",\"visible\":true,\"x\":[1,17,3,1,1,2,2,4,4,1,2,2,3,1,1,1,1,4,1,1,1,1,1,2,2,1,2,1,1,1,1,1,1,1,1,3,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,17,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,5,1,1,4,1,1,3,1,1,3,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,5,1,3,1,6,1,6,1,1,1,1,1,1,2,1,2,1,1,11,1,1,1,1,1,3,10,1,1,5,1,2,1,1,7,1,3,1,1,1,1,48,1,1,1,1,1,1,2,1,6,3,1,1,6,1,2,1,3,1,1,1,1,4,1,2,1,1,1,1,1,1,8,1,4,1,1,1,1,1,1,1,2,1,1,1,1,3,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,4,1,1,1,5,2,1,1,1,2,2,2,1,1,1,12,2,2,3,1,1,1,1,1,3,1,1,7,1,1,2,1,1,1,2,1,1,2,1,1,1,1,2,14,1,1,1,1,1,2,1,2,1,1,1,1,1,21,2,1,1,1,1,1,1,1,1,1,1,1,2,1,1,2,1,1,1,1,2,1,1,1,1,5,1,1,4,1,1,7,6,1],\"y\":[\"Biomedical Signal Processing and Control\",\"Sensors\",\"Visual Computer\",\"2023 14th International Conference on Computing Communication and Networking Technologies, ICCCNT 2023\",\"Computer Methods and Programs in Biomedicine\",\"2023 IEEE 7th Portuguese Meeting on Bioengineering, ENBENG 2023\",\"IEEE Journal of Translational Engineering in Health and Medicine\",\"IEEE Transactions on Multimedia\",\"Journal of Visual Communication and Image Representation\",\"1st International Conference in Advanced Innovation on Smart City, ICAISC 2023 - Proceedings\",\"Proceedings - 2023 IEEE Winter Conference on Applications of Computer Vision, WACV 2023\",\"Automation in Construction\",\"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS\",\"5th International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2023\",\"2016 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2016 - Conference Proceedings\",\"IEEE Sensors Journal\",\"Proceedings - 2021 International Conference on Computer Information Science and Artificial Intelligence, CISAI 2021\",\"IEEE Transactions on Circuits and Systems for Video Technology\",\"IEEE Region 10 Annual International Conference, Proceedings\\u002fTENCON\",\"VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications\",\"Signal Processing - Algorithms, Architectures, Arrangements, and Applications Conference Proceedings, SPA\",\"Proceedings - 2023 IEEE\\u002fACM International Conference on Connected Health: Applications, Systems and Engineering Technologies, CHASE 2023\",\"Expert Systems\",\"Frontiers in Artificial Intelligence and Applications\",\"Sensors (Basel, Switzerland)\",\"2023 6th International Conference on Information Systems and Computer Networks, ISCON 2023\",\"IEEE Transactions on Neural Systems and Rehabilitation Engineering\",\"VISIGRAPP 2018 - Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications\",\"IEEE Transactions on Mobile Computing\",\"Proceedings - 2022 6th International Conference on Imaging, Signal Processing and Communications, ICISPC 2022\",\"Engineering, Construction and Architectural Management\",\"Computing and Informatics\",\"Proceedings - 2021 International Conference on Computer Engineering and Application, ICCEA 2021\",\"Bioengineering\",\"International Conference on Human System Interaction, HSI\",\"ACM Computing Surveys\",\"Kongzhi yu Juece\\u002fControl and Decision\",\"Gait and Posture\",\"MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia\",\"Proceedings - 2019 International Conference on 3D Vision, 3DV 2019\",\"VISIGRAPP 2020 - Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications\",\"Applied Computer Science\",\"2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence, PRAI 2023\",\"Journal of NeuroEngineering and Rehabilitation\",\"MM 2016 - Proceedings of the 2016 ACM Multimedia Conference\",\"Journal of Building Engineering\",\"IEEE Transactions on Automation Science and Engineering\",\"AIP Conference Proceedings\",\"Proceedings - 21st IEEE International Conference on Machine Learning and Applications, ICMLA 2022\",\"2019 5th International Conference on Control, Automation and Robotics, ICCAR 2019\",\"Bioinformatics and Medical Applications: Big Data Using Deep Learning Algorithms\",\"International Geoscience and Remote Sensing Symposium (IGARSS)\",\"Proceedings of SPIE - The International Society for Optical Engineering\",\"Proceedings - 2019 International Conference on Virtual Reality and Visualization, ICVRV 2019\",\"Proceedings - 2022 2nd International Conference on Big Data Engineering and Education, BDEE 2022\",\"CEUR Workshop Proceedings\",\"Journal of WSCG\",\"Proceedings of 2019 IEEE 2nd International Conference on Automation, Electronics and Electrical Engineering, AUTEEE 2019\",\"2021 36th International Technical Conference on Circuits\\u002fSystems, Computers and Communications, ITC-CSCC 2021\",\"Neural Processing Letters\",\"7th IEEE International Conference on Computational Systems and Information Technology for Sustainable Solutions, CSITSS 2023 - Proceedings\",\"International Conference on Intelligent User Interfaces, Proceedings IUI\",\"2023 IEEE 8th International Conference on Big Data Analytics, ICBDA 2023\",\"Proceedings - 2019 IEEE International Conference on Smart Internet of Things, SmartIoT 2019\",\"Machine Learning and Knowledge Extraction\",\"Proceedings - European Workshop on Visual Information Processing,  EUVIP\",\"2022 IEEE 14th International Conference on Advanced Infocomm Technology, ICAIT 2022\",\"Computing in Civil Engineering 2019: Data, Sensing, and Analytics - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2019\",\"Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization\",\"2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2021 - Proceedings\",\"IAENG International Journal of Computer Science\",\"MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia\",\"Traitement du Signal\",\"IEEE Transactions on Biometrics, Behavior, and Identity Science\",\"Sensors (Switzerland)\",\"2023 International Conference on Network, Multimedia and Information Technology, NMITCON 2023\",\"2019 7th International Conference on Robot Intelligence Technology and Applications, RiTA 2019\",\"IEEE International Conference on Intelligent Robots and Systems\",\"Proceedings of the International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications\",\"EAI\\u002fSpringer Innovations in Communication and Computing\",\"Proceedings of the International Joint Conference on Neural Networks\",\"Journal of Shanghai Jiaotong University (Science)\",\"Neurology\",\"Pattern Recognition\",\"Diagnostics\",\"2023 20th International Conference on Ubiquitous Robots, UR 2023\",\"Proceedings - IEEE International Conference on Multimedia and Expo\",\"Computer Science Research Notes\",\"2022 IEEE Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation, IATMSI 2022\",\"Lecture Notes in Computational Vision and Biomechanics\",\"2023 15th International Conference on Computer Research and Development, ICCRD 2023\",\"2018 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2018\",\"2022 8th International Conference on Control, Decision and Information Technologies, CoDIT 2022\",\"Journal of Sports Sciences\",\"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics\",\"Proceedings - 2021 17th International Conference on Computational Intelligence and Security, CIS 2021\",\"Computers, Materials and Continua\",\"Beijing Gongye Daxue Xuebao\\u002fJournal of Beijing University of Technology\",\"Mobile Information Systems\",\"Proceedings of the IEEE International Conference on Computer Vision\",\"2019 IEEE International Conference on Real-Time Computing and Robotics, RCAR 2019\",\"IEEE Transactions on Human-Machine Systems\",\"AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\",\"Lecture Notes in Electrical Engineering\",\"Proceedings - 2023 2nd International Conference on Big Data, Information and Computer Network, BDICN 2023\",\"Journal of Physics: Conference Series\",\"Mathematics\",\"2023 Systems of Signals Generating and Processing in the Field of on Board Communications, SOSG 2023 - Conference Proceedings\",\"Tensor Computation for Data Analysis\",\"2021 International Conference on Circuits, Controls and Communications, CCUBE 2021\",\"IISE Annual Conference and Expo 2022\",\"Proceedings - 2019 19th International Symposium on Communications and Information Technologies, ISCIT 2019\",\"3rd International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2021\",\"4th International Conference on Smart and Sustainable City, ICSSC 2017\",\"Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020\",\"2022 28th International Conference on Mechatronics and Machine Vision in Practice, M2VIP 2022\",\"2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology, CCET 2020\",\"Neurocomputing\",\"2023 4th International Conference on Big Data and Artificial Intelligence and Software Engineering, ICBASE 2023\",\"Metaverse Communication and Computing Networks: Applications, Technologies, and Approaches\",\"Nature Protocols\",\"IET Conference Publications\",\"Multimodal Technologies and Interaction\",\"Engineering Applications of Artificial Intelligence\",\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"Proceedings of AVSS 2018 - 2018 15th IEEE International Conference on Advanced Video and Signal-Based Surveillance\",\"Materials Today: Proceedings\",\"Proceedings - International Conference on Pattern Recognition\",\"Conference Proceedings of the IEEE International Performance, Computing, and Communications Conference\",\"Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021\",\"International Journal of Environmental Research and Public Health\",\"Proceedings - 2020 International Computer Symposium, ICS 2020\",\"Multimedia Tools and Applications\",\"Studies in Computational Intelligence\",\"Multimedia Systems\",\"Jisuanji Xuebao\\u002fChinese Journal of Computers\",\"Computer-Aided Design and Applications\",\"Proceedings - 2021 International Conference on Computer Engineering and Artificial Intelligence, ICCEAI 2021\",\"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing\",\"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\",\"ICMEW 2022 - IEEE International Conference on Multimedia and Expo Workshops 2022, Proceedings\",\"Frontiers in Neurorobotics\",\"ASME 2018 Dynamic Systems and Control Conference, DSCC 2018\",\"AIAA\\u002fIEEE Digital Avionics Systems Conference - Proceedings\",\"Journal of Electronic Imaging\",\"IET Computer Vision\",\"IEEE Signal Processing Letters\",\"KSII Transactions on Internet and Information Systems\",\"Electronics (Switzerland)\",\"Journal of Imaging\",\"Symmetry\",\"Ruan Jian Xue Bao\\u002fJournal of Software\",\"Computer Vision and Image Understanding\",\"International Journal of Pattern Recognition and Artificial Intelligence\",\"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019\",\"IEEE Transactions on Emerging Topics in Computing\",\"Neural Networks\",\"Proceedings - 2022 International Conference on 3D Vision, 3DV 2022\",\"Pattern Analysis and Applications\",\"Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016\",\"Hunan Daxue Xuebao\\u002fJournal of Hunan University Natural Sciences\",\"IEEE Robotics and Automation Letters\",\"Personal and Ubiquitous Computing\",\"Signal, Image and Video Processing\",\"IEEE Transactions on Industrial Informatics\",\"2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence, RAAI 2021\",\"2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications, ICPECA 2022\",\"ICIIBMS 2023 - 8th International Conference on Intelligent Informatics and Biomedical Sciences\",\"Proceedings of the 11th European Conference on Mobile Robots, ECMR 2023\",\"ICALIP 2018 - 6th International Conference on Audio, Language and Image Processing\",\"Proceedings - International Conference on Image Processing, ICIP\",\"British Machine Vision Conference 2018, BMVC 2018\",\"International Journal of Computer Vision\",\"Proceedings - 2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence, MLBDBI 2021\",\"Evolving Systems\",\"Jisuanji Yanjiu yu Fazhan\\u002fComputer Research and Development\",\"Smart Health\",\"Computational Visual Media\",\"ACM Transactions on Multimedia Computing, Communications and Applications\",\"Beginning Machine Learning in the Browser: Quick-start Guide to Gait Analysis with JavaScript and TensorFlow.js\",\"International Journal of Computer Assisted Radiology and Surgery\",\"Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao\\u002fJournal of Computer-Aided Design and Computer Graphics\",\"Zidonghua Xuebao\\u002fActa Automatica Sinica\",\"International Conference on Electrical, Computer, Communications and Mechatronics Engineering, ICECCME 2021\",\"Remote Sensing\",\"Image and Vision Computing\",\"Proceedings - 2023 3rd International Conference on Innovative Sustainable Computational Technologies, CISCT 2023\",\"Proceedings - 2022 8th Annual International Conference on Network and Information Systems for Computers, ICNISC 2022\",\"Proceedings - 2020 International Conference on Culture-Oriented Science and Technology, ICCST 2020\",\"Journal of Supercomputing\",\"Proceedings - 2021 International Conference on 3D Vision, 3DV 2021\",\"IEEE Signal Processing Magazine\",\"Infrared Physics and Technology\",\"British Machine Vision Conference 2016, BMVC 2016\",\"Laser and Optoelectronics Progress\",\"International Journal of Computing and Digital Systems\",\"Proceedings - 2022 Chinese Automation Congress, CAC 2022\",\"2017 6th Mediterranean Conference on Embedded Computing, MECO 2017 - Including ECYPS 2017, Proceedings\",\"Future Internet\",\"ASSIC 2022 - Proceedings: International Conference on Advancements in Smart, Secure and Intelligent Computing\",\"Shenzhen Daxue Xuebao (Ligong Ban)\\u002fJournal of Shenzhen University Science and Engineering\",\"International Conference on ICT Convergence\",\"Future Generation Computer Systems\",\"AIP Advances\",\"Yi Qi Yi Biao Xue Bao\\u002fChinese Journal of Scientific Instrument\",\"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings\",\"Moshi Shibie yu Rengong Zhineng\\u002fPattern Recognition and Artificial Intelligence\",\"2023 20th International Computer Conference on Wavelet Active Media Technology and Information Processing, ICCWAMTIP 2023\",\"IAES International Journal of Artificial Intelligence\",\"Forecasting\",\"IEEE Internet of Things Journal\",\"Smart Innovation, Systems and Technologies\",\"Machine Vision and Applications\",\"Digital Biomarkers\",\"Zhejiang Daxue Xuebao (Gongxue Ban)\\u002fJournal of Zhejiang University (Engineering Science)\",\"Proceedings - 2023 IEEE Conference on Artificial Intelligence, CAI 2023\",\"ACM International Conference Proceeding Series\",\"Journal of Ambient Intelligence and Humanized Computing\",\"Expert Systems with Applications\",\"Journal of Image and Graphics\",\"Proceedings of 2018 2nd IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference, IMCEC 2018\",\"Proceedings of the 2nd International Conference on Applied Artificial Intelligence and Computing, ICAAIC 2023\",\"Proceedings - 2023 6th International Symposium on Computer, Consumer and Control, IS3C 2023\",\"2019 15th International Wireless Communications and Mobile Computing Conference, IWCMC 2019\",\"Journal of Frontiers of Computer Science and Technology\",\"IEEE Transactions on Image Processing\",\"International Conference on Virtual Rehabilitation, ICVR\",\"Proceedings - 2021 2nd International Conference on Computing and Data Science, CDS 2021\",\"Applied Sciences (Switzerland)\",\"International Conference Image and Vision Computing New Zealand\",\"Handbook of Intelligent Computing and Optimization for Sustainable Development\",\"IS and T International Symposium on Electronic Imaging Science and Technology\",\"Computers and Electrical Engineering\",\"2022 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2022\",\"IEEE Transactions on Cognitive and Developmental Systems\",\"ICTC 2019 - 10th International Conference on ICT Convergence: ICT Convergence Leading the Autonomous Future\",\"7th IEEE-EMBS Conference on Biomedical Engineering and Sciences, IECBES 2022 - Proceedings\",\"Applied Mathematics and Nonlinear Sciences\",\"Chinese Control Conference, CCC\",\"Journal of Personalized Medicine\",\"Tsinghua Science and Technology\",\"ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction\",\"Proceedings - 2022 IEEE\\u002fCVF Winter Conference on Applications of Computer Vision, WACV 2022\",\"Applied Intelligence\",\"IEEE Access\",\"2019 2nd International Conference on Communication, Computing and Digital Systems, C-CODE 2019\",\"2023 12th International Conference on Modern Circuits and Systems Technologies, MOCAST 2023 - Proceedings\",\"Proceedings of the ACM on Computer Graphics and Interactive Techniques\",\"Proceedings - 6th Annual Conference on Computational Science and Computational Intelligence, CSCI 2019\",\"2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2018 - Proceedings\",\"PLoS ONE\",\"Informatics\",\"30th British Machine Vision Conference 2019, BMVC 2019\",\"Journal of Experimental and Theoretical Artificial Intelligence\",\"Dongnan Daxue Xuebao (Ziran Kexue Ban)\\u002fJournal of Southeast University (Natural Science Edition)\",\"CMES - Computer Modeling in Engineering and Sciences\",\"Jisuanji Gongcheng\\u002fComputer Engineering\",\"ICIIBMS 2022 - 7th International Conference on Intelligent Informatics and Biomedical Sciences\",\"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition\",\"International Conference on Control, Automation and Systems\",\"Tien Tzu Hsueh Pao\\u002fActa Electronica Sinica\",\"Huadong Ligong Daxue Xuebao\\u002fJournal of East China University of Science and Technology\",\"2022 IEEE International Workshop on Metrology for Extended Reality, Artificial Intelligence and Neural Engineering, MetroXRAINE 2022 - Proceedings\",\"2020 IEEE 15th International Conference on Industrial and Information Systems, ICIIS 2020 - Proceedings\",\"Complexity\",\"IST 2019 - IEEE International Conference on Imaging Systems and Techniques, Proceedings\",\"IEEE International Conference on Computational Photography, ICCP 2022\",\"2022 IEEE 8th World Forum on Internet of Things, WF-IoT 2022\",\"18th IEEE International Conference on Emerging Technologies, ICET 2023\",\"International Journal of Multimedia Information Retrieval\",\"Arthritis Research and Therapy\",\"International Journal of Intelligent Systems and Applications in Engineering\",\"Proceedings of Machine Learning Research\",\"International Conference on Systems, Signals, and Image Processing\",\"Proceedings - 2020 International Conference on Pervasive Artificial Intelligence, ICPAI 2020\",\"Proceedings - 2019 Amity International Conference on Artificial Intelligence, AICAI 2019\",\"Proceedings - 2nd International Workshop on Cyber-Physical-Human System Design and Implementation, CPHS 2022\",\"Transactions on Emerging Telecommunications Technologies\",\"Proceedings of the International Technical Meeting of The Institute of Navigation, ITM\",\"Computational Intelligence and Neuroscience\",\"IMX 2023 - Proceedings of the 2023 ACM International Conference on Interactive Media Experiences\",\"2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings\",\"Security and Communication Networks\",\"Journal of Computer Science\",\"Lecture Notes in Networks and Systems\",\"Proceedings - 2018 International Conference on 3D Vision, 3DV 2018\",\"Journal of Intelligent and Robotic Systems: Theory and Applications\",\"Pattern Recognition Letters\",\"Proceedings of the Confluence 2022 - 12th International Conference on Cloud Computing, Data Science and Engineering\",\"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies\",\"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops\",\"Communications in Computer and Information Science\",\"Proceedings of 2022 6th Asian Conference on Artificial Intelligence Technology, ACAIT 2022\"],\"type\":\"bar\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: detection, system, learning, research, topics\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: systems, learning, data, classification, detection\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: control, drone, strategy, learning, object\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: image, algorithm, learning, network, segmentation\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: learning, deep, detection, analysis, neural\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: segmentation, surface, traffic, learning, detection\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: workpiece, detection, end, convolution, learning\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: reinforcement, energy, learning, user, mobile\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: prediction, research, technology, digital, series\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: analysis, detection, learning, network, molecular\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: multimodal, data, learning, model, labeling\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: learning, tumor, interpretable, reconstruction, classification\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: learning, intelligence, artificial, detection, deep\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: learning, data, deep, detection, conference\"],\"marker\":{\"color\":\"#56B4E9\"},\"name\":\"Artificial Intelligence Applications\",\"orientation\":\"h\",\"visible\":\"legendonly\",\"x\":[3,2,1,1,13,1,1,1,1,2,1,1,4,4],\"y\":[\"Proceedings of SPIE - The International Society for Optical Engineering\",\"Advances in Intelligent Systems and Computing\",\"Proceedings - IWIS 2023: 3rd International Workshop on Intelligent Systems\",\"Lecture Notes in Electrical Engineering\",\"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\",\"IVMSP 2022 - 2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop\",\"Proceedings - 2022 4th International Symposium on Smart and Healthy Cities, ISHC 2022\",\"International Conference on ICT Convergence\",\"Smart Innovation, Systems and Technologies\",\"ACM International Conference Proceeding Series\",\"Journal of Image and Graphics\",\"Proceedings of Machine Learning Research\",\"Lecture Notes in Networks and Systems\",\"Communications in Computer and Information Science\"],\"type\":\"bar\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rf, signals, rpm, body, human\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: mmposefk, fk, mmwave, estimation, pose\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: signals, rf, multiperson, rpm, 3d\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: uwb, human, like, pose, technology\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rfid, human, pose, rfidpose, tracking\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: signals, rf, rfmvp, multiperson, 3d\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: frames, stream, signals, estimation, among\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rf, signals, rfposeot, poses, environment\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rfpose, fast, rf, human, signals\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rf, signals, rpm, human, body\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: wifi, signals, system, gopose, reflected\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: signals, global, rpc, local, learning\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: rfid, cyclepose, data, system, subjectadaptive\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: constraint, radar, joint, relationships, spatial\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: radar, iruwb, signals, human, algorithm\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: signals, spatialfrequency, csi, evolving, wifi\",\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: aoa, wifi, signals, system, body\"],\"marker\":{\"color\":\"#009E73\"},\"name\":\"\\\"RF Human Pose Estimation Systems\\\"\",\"orientation\":\"h\",\"visible\":\"legendonly\",\"x\":[1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1],\"y\":[\"IEEE Transactions on Multimedia\",\"IEEE Sensors Journal\",\"IEEE Transactions on Circuits and Systems for Video Technology\",\"2023 IEEE International Workshop on Metrology for Industry 4.0 and IoT, MetroInd4.0 and IoT 2023 - Proceedings\",\"IEEE Transactions on Reliability\",\"Proceedings - IEEE International Conference on Multimedia and Expo\",\"Lecture Notes in Electrical Engineering\",\"Frontiers of Information Technology and Electronic Engineering\",\"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings\",\"2022 IEEE 24th International Workshop on Multimedia Signal Processing, MMSP 2022\",\"SenSys 2021 - Proceedings of the 2021 19th ACM Conference on Embedded Networked Sensor Systems\",\"IEEE Internet of Things Journal\",\"Proceedings - 2020 16th International Conference on Mobility, Sensing and Networking, MSN 2020\",\"IEEE Transactions on Cognitive and Developmental Systems\",\"IEEE Access\",\"Pattern Recognition Letters\",\"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies\"],\"type\":\"bar\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: computer, footbridges, humaninduced, vibration, vision\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, falls, detection, lstm, persons\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, point, algorithm, falls, joint\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: falls, medical, retirement, senior, falling\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, video, falls, elderly, set\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, falls, accuracy, detection, classification\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: reference, guidance, fall, referpose, representation\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, activities, segment, falls, head\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: joint, falling, angle, fall, center\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, recognition, morphological, calculates, reliability\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: elderly, fall, falls, recognition, population\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: falls, fall, handcrafted, highlevel, input\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: fall, falls, cyclist, injury, kinematics\"],\"marker\":{\"color\":\"#F0E442\"},\"name\":\"Fall Detection and Recognition in Elderly Persons\",\"orientation\":\"h\",\"visible\":\"legendonly\",\"x\":[1,1,1,1,1,1,1,1,1,1,1,1,1],\"y\":[\"Frontiers in Built Environment\",\"Progress in Biomedical Optics and Imaging - Proceedings of SPIE\",\"Kongzhi yu Juece\\u002fControl and Decision\",\"Proceedings - 21st IEEE International Conference on Machine Learning and Applications, ICMLA 2022\",\"Lecture Notes in Electrical Engineering\",\"Studies in Computational Intelligence\",\"IEEE Transactions on Industrial Informatics\",\"Proceedings - 2021 21st ACIS International Semi-Virtual Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel\\u002fDistributed Computing, SNPD-Winter 2021\",\"Applied Sciences (Switzerland)\",\"International Conference on Signal Processing Proceedings, ICSP\",\"IEEE Access\",\"International Conference on Systems, Signals, and Image Processing\",\"Journal of Biomechanics\"],\"type\":\"bar\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driver, accidents, pedestrians, particular, device\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driver, activity, classifier, reaction, forest\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: vehicle, multiscale, architecture, driver, vehipose\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driver, new, general, drivers, estimation\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: invehicle, driving, driver, autonomous, essential\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driving, autonomous, tasks, driver, realization\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driver, pose, vehicle, estimation, human\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driving, driver, deep, estimation, human\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driver, distraction, pedestrian, secondary, pose\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driver, traffic, distracted, distraction, accidents\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: driver, shufflenet, v2, integral, estimation\"],\"marker\":{\"color\":\"#D55E00\"},\"name\":\"Autonomous Vehicle Monitoring using Pose Estimation\",\"orientation\":\"h\",\"visible\":\"legendonly\",\"x\":[1,1,1,1,1,1,2,1,1,1,1],\"y\":[\"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC\",\"2021 International Conference on INnovations in Intelligent SysTems and Applications, INISTA 2021 - Proceedings\",\"Proceedings of SPIE - The International Society for Optical Engineering\",\"Proceedings of the IEEE International Conference on Computer Vision\",\"2022 International Conference on Electronics, Information, and Communication, ICEIC 2022\",\"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\",\"IEEE Intelligent Vehicles Symposium, Proceedings\",\"Proceedings - 2022 4th International Symposium on Smart and Healthy Cities, ISHC 2022\",\"IEEE Transactions on Intelligent Vehicles\",\"ACM International Conference Proceeding Series\",\"Transactions of the Japanese Society for Artificial Intelligence\"],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"xaxis\":{\"showgrid\":true,\"title\":{\"text\":\"Frequency\"}},\"yaxis\":{\"showgrid\":true,\"title\":{\"text\":\"Class\"}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eTopics per Class\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.4,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":1250,\"height\":900,\"legend\":{\"title\":{\"text\":\"\\u003cb\\u003eGlobal Topic Representation\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('93a41732-47eb-40fb-98f5-34db21ce48a0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "import string\n",
        "from keybert import KeyBERT\n",
        "\n",
        "papers = pd.read_excel(\"/content/topic_modeling_summary.xlsx\",sheet_name=['-1']) # Run from -1 to 4\n",
        "data_minus_1 = papers['-1']['Abstract']\n",
        "\n",
        "list_abstract = []\n",
        "for row in data_minus_1:\n",
        "    list_abstract.append(row)\n",
        "doc = \" \".join(list_abstract)\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "#keywords_highlighted = kw_model.extract_keywords(doc, highlight=True)\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 1)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 2)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 3)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1,1),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(2,2),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(3,3),use_mmr=True, diversity=0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ghfw4czs_NR",
        "outputId": "b96fc379-1c0b-48b7-a01a-a1d8f4fc2d62"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('yogapose', 0.5431), ('yoga', 0.5368), ('postures', 0.4337), ('posture', 0.432), ('poses', 0.4145)]\n",
            "[('automation yoga', 0.6373), ('yoga classifier', 0.6268), ('yoga tracker', 0.6221), ('yoga poses', 0.6132), ('persons yoga', 0.612)]\n",
            "[('yoga pose dataset', 0.6684), ('yoga pose recognition', 0.6618), ('tracking correcting yoga', 0.6611), ('pose dataset yoga', 0.6562), ('detect correct yoga', 0.6507)]\n",
            "[('yogapose', 0.5431), ('yoga', 0.5368), ('postures', 0.4337), ('poses', 0.4145), ('svm', 0.3976)]\n",
            "[('automation yoga', 0.6373), ('yoga classifier', 0.6268), ('yoga tracker', 0.6221), ('yoga poses', 0.6132), ('dataset yoga', 0.6093)]\n",
            "[('yoga pose dataset', 0.6684), ('yoga pose recognition', 0.6618), ('tracking correcting yoga', 0.6611), ('detect correct yoga', 0.6507), ('specifically yoga human', 0.6362)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "papers = pd.read_excel(\"/content/topic_modeling_summary.xlsx\",sheet_name=['0']) # Run from -1 to 4\n",
        "data_minus_1 = papers['0']['Abstract']\n",
        "\n",
        "list_abstract = []\n",
        "for row in data_minus_1:\n",
        "    list_abstract.append(row)\n",
        "doc = \" \".join(list_abstract)\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "#keywords_highlighted = kw_model.extract_keywords(doc, highlight=True)\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 1)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 2)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 3)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1,1),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(2,2),use_mmr=True, diversity=0.1))\n",
        "#print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(3,3),use_mmr=True, diversity=0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MpDUDbpPaSj",
        "outputId": "1161f6a1-38a9-43e6-8d84-3ed6b621c3da"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('poseconfiguration', 0.4429), ('poseresnet', 0.4177), ('postures', 0.4125), ('posetracking', 0.4002), ('humancomputer', 0.3971)]\n",
            "[('pose manual', 0.5642), ('posture dataset', 0.5572), ('human poseaware', 0.5545), ('pose information', 0.5514), ('pose trackers', 0.5403)]\n",
            "[('pose datasets human36', 0.6064), ('human poseaware features', 0.6018), ('pose information provides', 0.6015), ('human pose estimationï¼œtwodimensional', 0.5868), ('pose humancentered automation', 0.5838)]\n",
            "[('poseconfiguration', 0.4429), ('poseresnet', 0.4177), ('postures', 0.4125), ('humancomputer', 0.3971), ('accuracyresource', 0.3849)]\n",
            "[('pose manual', 0.5642), ('posture dataset', 0.5572), ('human poseaware', 0.5545), ('pose trackers', 0.5403), ('models human36m', 0.5273)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "papers = pd.read_excel(\"/content/topic_modeling_summary.xlsx\",sheet_name=['1']) # Run from -1 to 4\n",
        "data_minus_1 = papers['1']['Abstract']\n",
        "\n",
        "list_abstract = []\n",
        "for row in data_minus_1:\n",
        "    list_abstract.append(row)\n",
        "doc = \" \".join(list_abstract)\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "#keywords_highlighted = kw_model.extract_keywords(doc, highlight=True)\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 1)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 2)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 3)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1,1),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(2,2),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(3,3),use_mmr=True, diversity=0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ODmr5Qt2ip",
        "outputId": "d6517be3-f377-471b-962b-837b078fbee0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('features', 0.3571), ('datasets', 0.3308), ('humancomputer', 0.3282), ('3d', 0.3198), ('ai', 0.3154)]\n",
            "[('multimodal datasets', 0.3812), ('vision topics', 0.3747), ('machines multimodal', 0.373), ('architecture video', 0.3716), ('development multimodality', 0.3697)]\n",
            "[('classification incorporates 3d', 0.4153), ('multimodal datasets equipmentlimited', 0.4144), ('development intelligent vr', 0.3964), ('computer vision topics', 0.3946), ('skills immersive vr', 0.3915)]\n",
            "[('features', 0.3571), ('datasets', 0.3308), ('humancomputer', 0.3282), ('3d', 0.3198), ('ai', 0.3154)]\n",
            "[('multimodal datasets', 0.3812), ('vision topics', 0.3747), ('architecture video', 0.3716), ('classification drone', 0.3667), ('conference humancomputer', 0.3637)]\n",
            "[('classification incorporates 3d', 0.4153), ('multimodal datasets equipmentlimited', 0.4144), ('development intelligent vr', 0.3964), ('vision topics challenges', 0.3889), ('motion videos research', 0.3829)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "papers = pd.read_excel(\"/content/topic_modeling_summary.xlsx\",sheet_name=['2']) # Run from -1 to 4\n",
        "data_minus_1 = papers['2']['Abstract']\n",
        "\n",
        "list_abstract = []\n",
        "for row in data_minus_1:\n",
        "    list_abstract.append(row)\n",
        "doc = \" \".join(list_abstract)\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "#keywords_highlighted = kw_model.extract_keywords(doc, highlight=True)\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 1)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 2)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 3)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1,1),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(2,2),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(3,3),use_mmr=True, diversity=0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QLqZ5XDQt1c",
        "outputId": "313d8782-356d-44cc-ffa0-26b1259f4c50"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('postures', 0.4358), ('pose', 0.3691), ('kinect', 0.3466), ('rfmvp', 0.3371), ('rfpose', 0.3269)]\n",
            "[('rf pose', 0.5199), ('performing postures', 0.5039), ('radarbased human', 0.4948), ('pose machine', 0.4794), ('radarbased pose', 0.4782)]\n",
            "[('radarbased human pose', 0.5584), ('signals human poses', 0.541), ('rfbased human pose', 0.5379), ('process human pose', 0.5335), ('pose machine rfmvp', 0.5331)]\n",
            "[('postures', 0.4358), ('pose', 0.3691), ('kinect', 0.3466), ('rfmvp', 0.3371), ('rfidpose', 0.3265)]\n",
            "[('rf pose', 0.5199), ('performing postures', 0.5039), ('radarbased human', 0.4948), ('pose machine', 0.4794), ('radarbased pose', 0.4782)]\n",
            "[('radarbased human pose', 0.5584), ('signals human poses', 0.541), ('rfbased human pose', 0.5379), ('process human pose', 0.5335), ('pose machine rfmvp', 0.5331)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "papers = pd.read_excel(\"/content/topic_modeling_summary.xlsx\",sheet_name=['3']) # Run from -1 to 4\n",
        "data_minus_1 = papers['3']['Abstract']\n",
        "\n",
        "list_abstract = []\n",
        "for row in data_minus_1:\n",
        "    list_abstract.append(row)\n",
        "doc = \" \".join(list_abstract)\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "#keywords_highlighted = kw_model.extract_keywords(doc, highlight=True)\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 1)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 2)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 3)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1,1),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(2,2),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(3,3),use_mmr=True, diversity=0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8SPpFvjQx1x",
        "outputId": "b0deaefa-8c43-447d-8ea4-0add9d70dfb9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('kinematic', 0.3732), ('footage', 0.3488), ('biomechanics', 0.3481), ('bicycle', 0.3398), ('videos', 0.3387)]\n",
            "[('analysis cyclist', 0.4347), ('fall kinematic', 0.4342), ('estimation cyclist', 0.4307), ('pose knowledge', 0.427), ('physical activities', 0.4177)]\n",
            "[('video analysis cyclist', 0.529), ('injury estimation cyclist', 0.4982), ('cyclist fall kinematics', 0.4722), ('cyclist fall kinematic', 0.4599), ('practicality videos exercise', 0.4489)]\n",
            "[('kinematic', 0.3732), ('footage', 0.3488), ('biomechanics', 0.3481), ('cyclist', 0.3368), ('datasets', 0.3334)]\n",
            "[('analysis cyclist', 0.4347), ('fall kinematic', 0.4342), ('pose knowledge', 0.427), ('precision pedestrian', 0.4119), ('practicality videos', 0.4085)]\n",
            "[('video analysis cyclist', 0.529), ('injury estimation cyclist', 0.4982), ('cyclist fall kinematics', 0.4722), ('human motion realworld', 0.4387), ('guidance pose knowledge', 0.4371)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "papers = pd.read_excel(\"/content/topic_modeling_summary.xlsx\",sheet_name=['4']) # Run from -1 to 4\n",
        "data_minus_1 = papers['4']['Abstract']\n",
        "\n",
        "list_abstract = []\n",
        "for row in data_minus_1:\n",
        "    list_abstract.append(row)\n",
        "doc = \" \".join(list_abstract)\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "#keywords_highlighted = kw_model.extract_keywords(doc, highlight=True)\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 1)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 2)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1, 3)))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(1,1),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(2,2),use_mmr=True, diversity=0.1))\n",
        "print(kw_model.extract_keywords(doc,keyphrase_ngram_range=(3,3),use_mmr=True, diversity=0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKVt3b5HQzng",
        "outputId": "4a583bfd-ab74-4a9c-d620-8a0580777228"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('pose', 0.357), ('dataset', 0.3463), ('datasets', 0.3407), ('poses', 0.3353), ('driver', 0.3326)]\n",
            "[('driver pose', 0.5094), ('driving tasks', 0.4695), ('learning driver', 0.4653), ('persons dataset', 0.4638), ('vehicle pose', 0.4614)]\n",
            "[('driver activity deep', 0.5785), ('driver pose information', 0.5574), ('driver distraction classification', 0.5413), ('classification driver pose', 0.5302), ('driver activity recognition', 0.5133)]\n",
            "[('pose', 0.357), ('dataset', 0.3463), ('driver', 0.3326), ('recognition', 0.3238), ('driving', 0.3233)]\n",
            "[('driver pose', 0.5094), ('driving tasks', 0.4695), ('learning driver', 0.4653), ('persons dataset', 0.4638), ('keypoints human', 0.4468)]\n",
            "[('driver activity deep', 0.5785), ('driver pose information', 0.5574), ('driver distraction classification', 0.5413), ('classification driver pose', 0.5302), ('datasets providing poses', 0.493)]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOg9N5E5rkQOXm5n6OZfHc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f294bd738484453fb537cb00ac510e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67dd25cc034a4975879149028193e11c",
              "IPY_MODEL_465633bbf2324a5d970f2c7fa940bee2",
              "IPY_MODEL_91d738bd27a84610bedcb4220cfc2c39"
            ],
            "layout": "IPY_MODEL_74bcf17f48a94254b30f8aa972a6e6ec"
          }
        },
        "67dd25cc034a4975879149028193e11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2e931ce885d4315ba256a59cdf0f464",
            "placeholder": "​",
            "style": "IPY_MODEL_015cd096bca946a3b792a096add1f03d",
            "value": "config.json: 100%"
          }
        },
        "465633bbf2324a5d970f2c7fa940bee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d15bcde485a2455881c48877432fc76a",
            "max": 587,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a7231d89e574202a6df6a7f3f4291ac",
            "value": 587
          }
        },
        "91d738bd27a84610bedcb4220cfc2c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_489356534b53493db10a318df050b53c",
            "placeholder": "​",
            "style": "IPY_MODEL_e86234a823e840e798010fe469576df6",
            "value": " 587/587 [00:00&lt;00:00, 47.5kB/s]"
          }
        },
        "74bcf17f48a94254b30f8aa972a6e6ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2e931ce885d4315ba256a59cdf0f464": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "015cd096bca946a3b792a096add1f03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d15bcde485a2455881c48877432fc76a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a7231d89e574202a6df6a7f3f4291ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "489356534b53493db10a318df050b53c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e86234a823e840e798010fe469576df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a715ffeb585a44c6864fdc4fec0045b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5188f3241ec469081f2012c7cf9b582",
              "IPY_MODEL_55d1d2abab48430da3c4f82210018efa",
              "IPY_MODEL_c5bf9dbe2b8c456d906e6f41a8586f7a"
            ],
            "layout": "IPY_MODEL_16e449cc53274f71a5d7a000f4c91251"
          }
        },
        "e5188f3241ec469081f2012c7cf9b582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87c212cab8c34fe6907a3769b5dd4145",
            "placeholder": "​",
            "style": "IPY_MODEL_4b2aa9827ed54589a260b7b5b0168c9c",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "55d1d2abab48430da3c4f82210018efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ec14a953ec34890a358daf0ed7ade16",
            "max": 33444,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_118afbc89a2e412aa3d5fc284f3d5d1a",
            "value": 33444
          }
        },
        "c5bf9dbe2b8c456d906e6f41a8586f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79127b5da06f4bbca0d2a8073e6e9adb",
            "placeholder": "​",
            "style": "IPY_MODEL_88196f6fc13645e3b07c7098fc761913",
            "value": " 33.4k/33.4k [00:00&lt;00:00, 2.54MB/s]"
          }
        },
        "16e449cc53274f71a5d7a000f4c91251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87c212cab8c34fe6907a3769b5dd4145": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b2aa9827ed54589a260b7b5b0168c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ec14a953ec34890a358daf0ed7ade16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118afbc89a2e412aa3d5fc284f3d5d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79127b5da06f4bbca0d2a8073e6e9adb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88196f6fc13645e3b07c7098fc761913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a416ef6255f64c948bf457b8e60ac08a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4073980fbd64aca8e9250efc272a7f3",
              "IPY_MODEL_61d12479688447ebb4d83707b0a894d7",
              "IPY_MODEL_bcd8bf7bf1d54d46a77302fd422672a7"
            ],
            "layout": "IPY_MODEL_43ad86cf5de24a0bb028ee96afdd736f"
          }
        },
        "e4073980fbd64aca8e9250efc272a7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48a8f522febb4b08acac1bcfa6935d46",
            "placeholder": "​",
            "style": "IPY_MODEL_35ea83e4466849809651a71d1b80abc6",
            "value": "Downloading shards: 100%"
          }
        },
        "61d12479688447ebb4d83707b0a894d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5204ff0c68d44f649e283f5656f001ec",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc9e66bc999e4a5ea9182997320bdca9",
            "value": 3
          }
        },
        "bcd8bf7bf1d54d46a77302fd422672a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0a6b140741b46de853aa6a54e1c9c5b",
            "placeholder": "​",
            "style": "IPY_MODEL_152193c644df48558270388a0e62f43a",
            "value": " 3/3 [06:10&lt;00:00, 114.92s/it]"
          }
        },
        "43ad86cf5de24a0bb028ee96afdd736f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a8f522febb4b08acac1bcfa6935d46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35ea83e4466849809651a71d1b80abc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5204ff0c68d44f649e283f5656f001ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9e66bc999e4a5ea9182997320bdca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0a6b140741b46de853aa6a54e1c9c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152193c644df48558270388a0e62f43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab7347627882458eb785c025a7388aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bd2060b47154e81a101aaaac3b061fc",
              "IPY_MODEL_e4f04be7f659491687469d88b7cd914c",
              "IPY_MODEL_0015e36e3dac4e7b86c0bb247ccaf8b2"
            ],
            "layout": "IPY_MODEL_fbcc7aa912684e79861404190b117b95"
          }
        },
        "8bd2060b47154e81a101aaaac3b061fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddd33afbe72d49e5a12e9d32b1d96e10",
            "placeholder": "​",
            "style": "IPY_MODEL_f246d28f78d34b25989f97ac393b2b96",
            "value": "model-00001-of-00003.safetensors: 100%"
          }
        },
        "e4f04be7f659491687469d88b7cd914c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6791e4f6a3e4d28830ad20ab3d5c471",
            "max": 9948693272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9aac0c18000c43f4a0ccdcc80e7d2135",
            "value": 9948693272
          }
        },
        "0015e36e3dac4e7b86c0bb247ccaf8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6176f8cad894310b4274cf244903b48",
            "placeholder": "​",
            "style": "IPY_MODEL_055d2091df0c4182a70a96ee955b2fae",
            "value": " 9.95G/9.95G [02:33&lt;00:00, 67.8MB/s]"
          }
        },
        "fbcc7aa912684e79861404190b117b95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddd33afbe72d49e5a12e9d32b1d96e10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f246d28f78d34b25989f97ac393b2b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6791e4f6a3e4d28830ad20ab3d5c471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aac0c18000c43f4a0ccdcc80e7d2135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6176f8cad894310b4274cf244903b48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "055d2091df0c4182a70a96ee955b2fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ac99a9964e74811b292f9bc178e7a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bd453d1e49344ca8adae2a8cabece50",
              "IPY_MODEL_13f9846948d442379d1e44bbf7029cc6",
              "IPY_MODEL_6b780c039b7e418a8b921aaf5b83e9a8"
            ],
            "layout": "IPY_MODEL_28bce5e80f8443da8f5b098cecedfb42"
          }
        },
        "0bd453d1e49344ca8adae2a8cabece50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52082f29352d480eae7b94572c240f5f",
            "placeholder": "​",
            "style": "IPY_MODEL_6528fcb559a34061aec6ef297bdd5553",
            "value": "model-00002-of-00003.safetensors: 100%"
          }
        },
        "13f9846948d442379d1e44bbf7029cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29f2805c247a47229d986cb8083da39b",
            "max": 9904129368,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b1d68f3cfd8422e83164e11861fca5d",
            "value": 9904129368
          }
        },
        "6b780c039b7e418a8b921aaf5b83e9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f50a7d7f2d384d559330aa3d7c9e2828",
            "placeholder": "​",
            "style": "IPY_MODEL_fe3eec1e9d304f0ca2d9abefe6caf817",
            "value": " 9.90G/9.90G [02:13&lt;00:00, 61.9MB/s]"
          }
        },
        "28bce5e80f8443da8f5b098cecedfb42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52082f29352d480eae7b94572c240f5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6528fcb559a34061aec6ef297bdd5553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29f2805c247a47229d986cb8083da39b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1d68f3cfd8422e83164e11861fca5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f50a7d7f2d384d559330aa3d7c9e2828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe3eec1e9d304f0ca2d9abefe6caf817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3da364cb4c574d2dbba73d764a42ca4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a509e88b975642be867db9760840ffcb",
              "IPY_MODEL_fed94cab700945919839540e90328c29",
              "IPY_MODEL_034bd0dfbf064638baf302d59a32273b"
            ],
            "layout": "IPY_MODEL_e2dadcc35e2d4f38b6302e74544f0f2e"
          }
        },
        "a509e88b975642be867db9760840ffcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ca7a994680d4a4a9f248659ca847c9b",
            "placeholder": "​",
            "style": "IPY_MODEL_6e353ea809bf4152bb8596997fde9498",
            "value": "model-00003-of-00003.safetensors: 100%"
          }
        },
        "fed94cab700945919839540e90328c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b31f6e912553474497153c485ab5dfae",
            "max": 6178962272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23712799dd3347e3992f59be05f1017f",
            "value": 6178962272
          }
        },
        "034bd0dfbf064638baf302d59a32273b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a1db28658e149f38242f06fcfb88055",
            "placeholder": "​",
            "style": "IPY_MODEL_9434e6af13f44407ba94c4ad23061016",
            "value": " 6.18G/6.18G [01:21&lt;00:00, 69.8MB/s]"
          }
        },
        "e2dadcc35e2d4f38b6302e74544f0f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ca7a994680d4a4a9f248659ca847c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e353ea809bf4152bb8596997fde9498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b31f6e912553474497153c485ab5dfae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23712799dd3347e3992f59be05f1017f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a1db28658e149f38242f06fcfb88055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9434e6af13f44407ba94c4ad23061016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20a646fa32794cc5b30343c2224c5755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99d1cc97d2b241ee8b1650543823d685",
              "IPY_MODEL_722f89946e5d4d3ebc92ca74cdbc79b8",
              "IPY_MODEL_f54fac74c56f420a97c51fcd05d3ee75"
            ],
            "layout": "IPY_MODEL_f34c1ecf4c3b484fbd84bbbdf6f20407"
          }
        },
        "99d1cc97d2b241ee8b1650543823d685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faec76a91f5f49c1bea16094c6b6f405",
            "placeholder": "​",
            "style": "IPY_MODEL_8e1d2e7f07684426af9bf6f7230de1c3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "722f89946e5d4d3ebc92ca74cdbc79b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a9727c452184400b66552a9eed5fefc",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3cc29a367fe4ef5bc7c1fda01437763",
            "value": 3
          }
        },
        "f54fac74c56f420a97c51fcd05d3ee75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cce9b1eb54d84f6289e29414bbf0df5b",
            "placeholder": "​",
            "style": "IPY_MODEL_0a1ce4f54f9c4f44875d2c1af043b0da",
            "value": " 3/3 [01:52&lt;00:00, 35.75s/it]"
          }
        },
        "f34c1ecf4c3b484fbd84bbbdf6f20407": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faec76a91f5f49c1bea16094c6b6f405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e1d2e7f07684426af9bf6f7230de1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a9727c452184400b66552a9eed5fefc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3cc29a367fe4ef5bc7c1fda01437763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cce9b1eb54d84f6289e29414bbf0df5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a1ce4f54f9c4f44875d2c1af043b0da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3486f7a17a3463a9d422715f8f92781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d02bb307d22745fb9d0f98a4abdf57c0",
              "IPY_MODEL_c87d68446e8b41538e29dcd62cbd2d1e",
              "IPY_MODEL_ed9b5cb688754f33b6c245e966aea3b5"
            ],
            "layout": "IPY_MODEL_a13bb33011434dd0aeba17fe429b3e45"
          }
        },
        "d02bb307d22745fb9d0f98a4abdf57c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74d8bcd1f9e241f3b43e798b0ebaa3c9",
            "placeholder": "​",
            "style": "IPY_MODEL_c1c0dadaaaf24749abc56ea3fb511bc3",
            "value": "generation_config.json: 100%"
          }
        },
        "c87d68446e8b41538e29dcd62cbd2d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59f1820055ab4062be0f90e7da3688d9",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb9856fcc8a24c83908f5721460ab04f",
            "value": 188
          }
        },
        "ed9b5cb688754f33b6c245e966aea3b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f05b530228b14252a1eb25a70193afec",
            "placeholder": "​",
            "style": "IPY_MODEL_83fa95e8d5b04271a71e3b8d80873066",
            "value": " 188/188 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "a13bb33011434dd0aeba17fe429b3e45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74d8bcd1f9e241f3b43e798b0ebaa3c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c0dadaaaf24749abc56ea3fb511bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59f1820055ab4062be0f90e7da3688d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9856fcc8a24c83908f5721460ab04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f05b530228b14252a1eb25a70193afec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83fa95e8d5b04271a71e3b8d80873066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "339aac4288da47159dbbcf312b03357d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55ca4d8a3774468098c501881ea07914",
              "IPY_MODEL_acdd156eebb44a6d968b16a7f64518ea",
              "IPY_MODEL_6c9f1d6dc318405c80a94c0c1b2bf683"
            ],
            "layout": "IPY_MODEL_57ca34c742fb4d0c88342e922a526b91"
          }
        },
        "55ca4d8a3774468098c501881ea07914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bfa86bc06c0403b90efb81766567ce0",
            "placeholder": "​",
            "style": "IPY_MODEL_78250eb9e2bd45e9984ca122805bf4d5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "acdd156eebb44a6d968b16a7f64518ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b53f0b36baa243f1adf8bb16ee68ff67",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc7e639d24464a7f9d9ee831cbc25fc9",
            "value": 1618
          }
        },
        "6c9f1d6dc318405c80a94c0c1b2bf683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92399b2945a649aeaf7ae6a6b590abc9",
            "placeholder": "​",
            "style": "IPY_MODEL_1bb0bd874a454f7fab8cc334d2d807d9",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 141kB/s]"
          }
        },
        "57ca34c742fb4d0c88342e922a526b91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bfa86bc06c0403b90efb81766567ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78250eb9e2bd45e9984ca122805bf4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b53f0b36baa243f1adf8bb16ee68ff67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc7e639d24464a7f9d9ee831cbc25fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92399b2945a649aeaf7ae6a6b590abc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bb0bd874a454f7fab8cc334d2d807d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c013e2a33cc94e1894715fef05a953ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62e00e64c614409ab449003251a0904c",
              "IPY_MODEL_7a01ea2e450246ee973b097eefd8546b",
              "IPY_MODEL_6414beac3aaf421f81a8be47b2a6c608"
            ],
            "layout": "IPY_MODEL_7e63a3eba14f47d6bb119f4ead53c7c7"
          }
        },
        "62e00e64c614409ab449003251a0904c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2593e1cdae84d6c9bf83ddaeed77d8d",
            "placeholder": "​",
            "style": "IPY_MODEL_898f1025a4ab4b93a1ea6dab5532b02a",
            "value": "tokenizer.model: 100%"
          }
        },
        "7a01ea2e450246ee973b097eefd8546b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c8695540dd24d66b3af6ecff41477d3",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3140ef25eda04d788ced30cf851d011c",
            "value": 499723
          }
        },
        "6414beac3aaf421f81a8be47b2a6c608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9e2c1345751463a82b53728121f01bf",
            "placeholder": "​",
            "style": "IPY_MODEL_521fdd2ee7e74e27a297d847c37f1769",
            "value": " 500k/500k [00:00&lt;00:00, 17.4MB/s]"
          }
        },
        "7e63a3eba14f47d6bb119f4ead53c7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2593e1cdae84d6c9bf83ddaeed77d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "898f1025a4ab4b93a1ea6dab5532b02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c8695540dd24d66b3af6ecff41477d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3140ef25eda04d788ced30cf851d011c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9e2c1345751463a82b53728121f01bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "521fdd2ee7e74e27a297d847c37f1769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fa56f932dc94bcc81967caa8d0ec40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3daf19111ec46e7a8af9b660fdffa21",
              "IPY_MODEL_f5ae5a7e806c499194e479eb09455ec8",
              "IPY_MODEL_15b61c2d40e848f8b47539065b954cf5"
            ],
            "layout": "IPY_MODEL_1c1d71af9a054f83855a91dc101ca29a"
          }
        },
        "b3daf19111ec46e7a8af9b660fdffa21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56515666a041471bb52f3a4eb1602e10",
            "placeholder": "​",
            "style": "IPY_MODEL_d455aba3a3ef442d9192bcda9cca3b34",
            "value": "tokenizer.json: 100%"
          }
        },
        "f5ae5a7e806c499194e479eb09455ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58ff0a319b6b4666b59b7795ba859557",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be83b9be1ea84671a8fe24b9f37278dd",
            "value": 1842767
          }
        },
        "15b61c2d40e848f8b47539065b954cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec7cd655b16543498a7571b3aa6f2977",
            "placeholder": "​",
            "style": "IPY_MODEL_80d9eef06fd2462c995c218fa8147476",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 1.86MB/s]"
          }
        },
        "1c1d71af9a054f83855a91dc101ca29a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56515666a041471bb52f3a4eb1602e10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d455aba3a3ef442d9192bcda9cca3b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58ff0a319b6b4666b59b7795ba859557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be83b9be1ea84671a8fe24b9f37278dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec7cd655b16543498a7571b3aa6f2977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d9eef06fd2462c995c218fa8147476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfe998a36a8e43a09f67e75ce838dd22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03f3482789b54b2989ab6904ed8b9c81",
              "IPY_MODEL_51cf6ca9a64047629c4fbae6906ac69a",
              "IPY_MODEL_c382cf0dde3e4678a3ce2fc88656f2e4"
            ],
            "layout": "IPY_MODEL_937e64a9473d4db7b821c3a60d1f31c9"
          }
        },
        "03f3482789b54b2989ab6904ed8b9c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce4e0b8499b4a5bb9cfe3497c987ead",
            "placeholder": "​",
            "style": "IPY_MODEL_d9e134f2968b4ff8b329e3a1d2948637",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "51cf6ca9a64047629c4fbae6906ac69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e48ff321c5b41509efd14c74a83e542",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_566d83e3218543e8884667ed5f6eca0e",
            "value": 414
          }
        },
        "c382cf0dde3e4678a3ce2fc88656f2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_478781c13876476b991e86e41c349ac1",
            "placeholder": "​",
            "style": "IPY_MODEL_e6b071f179ac431aa8fbbdb7fb19af22",
            "value": " 414/414 [00:00&lt;00:00, 39.5kB/s]"
          }
        },
        "937e64a9473d4db7b821c3a60d1f31c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce4e0b8499b4a5bb9cfe3497c987ead": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9e134f2968b4ff8b329e3a1d2948637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e48ff321c5b41509efd14c74a83e542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "566d83e3218543e8884667ed5f6eca0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "478781c13876476b991e86e41c349ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b071f179ac431aa8fbbdb7fb19af22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1c2ce4d5d0841378bb41ab111aa4432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7436e360b59418fae8906ce24296494",
              "IPY_MODEL_6ebe4a24a7a24dfda044691eb5c7a2b0",
              "IPY_MODEL_10ecbea4515248b6a0bb9ee249a39378"
            ],
            "layout": "IPY_MODEL_f1417bc4a2a64dec991fbec280e9f6c7"
          }
        },
        "d7436e360b59418fae8906ce24296494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0df6f288aae4bd8aa09626a3aab6f9e",
            "placeholder": "​",
            "style": "IPY_MODEL_3c5c41fd77ac42d687ddd99fdeb05712",
            "value": "Batches: 100%"
          }
        },
        "6ebe4a24a7a24dfda044691eb5c7a2b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_151816d6dc9a4829bbd9d99227c45a89",
            "max": 22,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4f93717b4fe459e874b4c915470a22c",
            "value": 22
          }
        },
        "10ecbea4515248b6a0bb9ee249a39378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6efbf3267b7f46cebf92a14d95de951c",
            "placeholder": "​",
            "style": "IPY_MODEL_15f22f8fdb8046839d8a06cc4c0baa85",
            "value": " 22/22 [00:21&lt;00:00,  1.61it/s]"
          }
        },
        "f1417bc4a2a64dec991fbec280e9f6c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0df6f288aae4bd8aa09626a3aab6f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c5c41fd77ac42d687ddd99fdeb05712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "151816d6dc9a4829bbd9d99227c45a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f93717b4fe459e874b4c915470a22c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6efbf3267b7f46cebf92a14d95de951c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15f22f8fdb8046839d8a06cc4c0baa85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}